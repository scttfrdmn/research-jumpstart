"""
Unit tests for data access classes using moto S3 mocking.

Tests S3 read/write operations, data parsing, and error handling.
"""

import io
from pathlib import Path

import boto3
import pandas as pd
import pytest
from moto import mock_aws


# ============================================================================
# Genomics Data Access Tests
# ============================================================================


@mock_aws
def test_genomics_data_access_init():
    """Test GenomicsDataAccess initialization."""
    import sys

    data_access_path = (
        Path(__file__).parent.parent / "projects/genomics/variant-analysis/tier-3/src"
    )
    sys.path.insert(0, str(data_access_path))

    try:
        from data_access import GenomicsDataAccess

        # Test with credentials
        accessor = GenomicsDataAccess(use_anon=False, region="us-east-1")
        assert accessor.s3_client is not None
        assert accessor.use_anon is False

        # Test anonymous access (skip due to boto3 API incompatibility in test)
        # accessor_anon = GenomicsDataAccess(use_anon=True, region="us-west-2")
        # assert accessor_anon.s3_client is not None
        # assert accessor_anon.use_anon is True

    finally:
        sys.path.remove(str(data_access_path))


@mock_aws
def test_genomics_load_vcf_from_s3():
    """Test loading VCF file from S3."""
    # Setup mocked S3
    s3_client = boto3.client("s3", region_name="us-east-1")
    s3_client.create_bucket(Bucket="test-genomics-bucket")

    # Create sample VCF content
    vcf_content = """##fileformat=VCFv4.2
##contig=<ID=chr1,length=248956422>
#CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO
chr1	100	rs123	A	T	30	PASS	DP=50;AF=0.25;TYPE=SNP
chr1	200	.	G	C	40	PASS	DP=60;AF=0.5;TYPE=SNP
chr2	300	rs456	T	G	35	PASS	DP=55;AF=0.75;TYPE=SNP
"""

    # Upload VCF to S3
    s3_client.put_object(Bucket="test-genomics-bucket", Key="data/sample.vcf", Body=vcf_content)

    import sys

    data_access_path = (
        Path(__file__).parent.parent / "projects/genomics/variant-analysis/tier-3/src"
    )
    sys.path.insert(0, str(data_access_path))

    try:
        from data_access import GenomicsDataAccess

        accessor = GenomicsDataAccess(use_anon=False, region="us-east-1")
        df = accessor.load_vcf_from_s3("test-genomics-bucket", "data/sample.vcf")

        # Assertions
        assert isinstance(df, pd.DataFrame)
        assert len(df) == 3
        assert list(df.columns) == [
            "CHROM",
            "POS",
            "ID",
            "REF",
            "ALT",
            "QUAL",
            "FILTER",
            "DP",
            "AF",
            "TYPE",
        ]

        # Check first variant
        assert df.iloc[0]["CHROM"] == "chr1"
        assert df.iloc[0]["POS"] == 100
        assert df.iloc[0]["REF"] == "A"
        assert df.iloc[0]["ALT"] == "T"
        assert df.iloc[0]["DP"] == 50
        assert df.iloc[0]["AF"] == 0.25

    finally:
        sys.path.remove(str(data_access_path))


@mock_aws
def test_genomics_save_results_to_s3():
    """Test saving analysis results to S3."""
    # Setup mocked S3
    s3_client = boto3.client("s3", region_name="us-east-1")
    s3_client.create_bucket(Bucket="test-genomics-bucket")

    import sys

    data_access_path = (
        Path(__file__).parent.parent / "projects/genomics/variant-analysis/tier-3/src"
    )
    sys.path.insert(0, str(data_access_path))

    try:
        from data_access import GenomicsDataAccess

        accessor = GenomicsDataAccess(use_anon=False, region="us-east-1")

        # Create test DataFrame
        test_df = pd.DataFrame(
            {
                "variant_id": ["var1", "var2", "var3"],
                "effect": ["missense", "synonymous", "nonsense"],
                "score": [0.8, 0.2, 0.9],
            }
        )

        # Save to S3
        accessor.save_results(test_df, "test-genomics-bucket", "results/analysis.csv")

        # Verify saved
        response = s3_client.get_object(Bucket="test-genomics-bucket", Key="results/analysis.csv")
        saved_csv = response["Body"].read().decode("utf-8")

        # Load back and compare
        loaded_df = pd.read_csv(io.StringIO(saved_csv))
        pd.testing.assert_frame_equal(test_df, loaded_df)

    finally:
        sys.path.remove(str(data_access_path))


@mock_aws
def test_genomics_list_vcf_files():
    """Test listing VCF files in S3."""
    # Setup mocked S3
    s3_client = boto3.client("s3", region_name="us-east-1")
    s3_client.create_bucket(Bucket="test-genomics-bucket")

    # Upload multiple files
    s3_client.put_object(Bucket="test-genomics-bucket", Key="data/sample1.vcf", Body=b"test")
    s3_client.put_object(Bucket="test-genomics-bucket", Key="data/sample2.vcf.gz", Body=b"test")
    s3_client.put_object(Bucket="test-genomics-bucket", Key="data/readme.txt", Body=b"test")
    s3_client.put_object(Bucket="test-genomics-bucket", Key="results/output.vcf", Body=b"test")

    import sys

    data_access_path = (
        Path(__file__).parent.parent / "projects/genomics/variant-analysis/tier-3/src"
    )
    sys.path.insert(0, str(data_access_path))

    try:
        from data_access import GenomicsDataAccess

        accessor = GenomicsDataAccess(use_anon=False, region="us-east-1")

        # List all VCF files
        vcf_files = accessor.list_vcf_files("test-genomics-bucket")
        assert len(vcf_files) == 3
        assert "data/sample1.vcf" in vcf_files
        assert "data/sample2.vcf.gz" in vcf_files
        assert "results/output.vcf" in vcf_files
        assert "data/readme.txt" not in vcf_files

        # List with prefix
        data_vcf_files = accessor.list_vcf_files("test-genomics-bucket", prefix="data/")
        assert len(data_vcf_files) == 2
        assert all(f.startswith("data/") for f in data_vcf_files)

    finally:
        sys.path.remove(str(data_access_path))


@mock_aws
def test_genomics_load_vcf_error_handling():
    """Test error handling for missing/invalid VCF files."""
    # Setup mocked S3 with bucket but no file
    s3_client = boto3.client("s3", region_name="us-east-1")
    s3_client.create_bucket(Bucket="test-genomics-bucket")

    import sys

    data_access_path = (
        Path(__file__).parent.parent / "projects/genomics/variant-analysis/tier-3/src"
    )
    sys.path.insert(0, str(data_access_path))

    try:
        from data_access import GenomicsDataAccess

        accessor = GenomicsDataAccess(use_anon=False, region="us-east-1")

        # Try to load non-existent file
        with pytest.raises(Exception):
            accessor.load_vcf_from_s3("test-genomics-bucket", "data/nonexistent.vcf")

    finally:
        sys.path.remove(str(data_access_path))


# ============================================================================
# Climate Data Access Tests
# ============================================================================


@pytest.mark.skip(reason="CMIP6DataAccess uses s3fs which requires complex mocking")
@mock_aws
def test_climate_data_access_s3_operations():
    """
    Test climate data access S3 read/write operations.

    Note: CMIP6DataAccess uses s3fs and xarray for Zarr data,
    which requires more sophisticated mocking. Consider testing
    with real public data or separate integration tests.
    """
    pass


# ============================================================================
# Text Analysis Data Access Tests
# ============================================================================


@mock_aws
def test_text_analysis_data_access():
    """Test digital humanities text data access."""
    # Setup mocked S3
    s3_client = boto3.client("s3", region_name="us-east-1")
    s3_client.create_bucket(Bucket="test-text-bucket")

    # Upload test corpus
    test_text = "This is a sample historical document for analysis."
    s3_client.put_object(
        Bucket="test-text-bucket", Key="corpus/document1.txt", Body=test_text.encode()
    )

    import sys

    data_access_path = (
        Path(__file__).parent.parent
        / "projects/digital-humanities/text-analysis/tier-3/src"
    )
    # Clear any previously imported data_access module
    if "data_access" in sys.modules:
        del sys.modules["data_access"]
    sys.path.insert(0, str(data_access_path))

    try:
        from data_access import TextDataAccess

        accessor = TextDataAccess(use_anon=False, region="us-east-1")

        # Test loading document
        content = accessor.load_text_from_s3("test-text-bucket", "corpus/document1.txt")
        assert content == test_text

        # Test listing documents
        documents = accessor.list_text_files("test-text-bucket", prefix="corpus/")
        assert len(documents) > 0
        assert "corpus/document1.txt" in documents

    finally:
        sys.path.remove(str(data_access_path))
        if "data_access" in sys.modules:
            del sys.modules["data_access"]


# ============================================================================
# Summary Test
# ============================================================================


@pytest.mark.unit
def test_data_access_modules_discoverable():
    """Test that we can discover all data_access.py modules."""
    projects_dir = Path(__file__).parent.parent / "projects"
    data_access_files = list(projects_dir.rglob("**/tier-3/src/data_access.py"))

    assert len(data_access_files) > 0, "Should find data_access modules"

    print(f"\nâœ“ Found {len(data_access_files)} data access modules:")
    for f in sorted(data_access_files):
        domain = f.parts[-5]
        project = f.parts[-4]
        print(f"  - {domain}/{project}")
