{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misinformation Detection and Risk Scoring\n",
    "\n",
    "This notebook demonstrates pattern-based misinformation detection using a multi-factor risk scoring system.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load social media data with sentiment analysis\n",
    "2. Detect misinformation patterns (excessive caps, urgency language, vague sources)\n",
    "3. Calculate risk scores (0-10 scale)\n",
    "4. Identify high-risk posts\n",
    "5. Analyze characteristics of potentially misleading content\n",
    "6. Visualize risk distribution\n",
    "\n",
    "## Misinformation Indicators\n",
    "\n",
    "Our risk scoring system evaluates:\n",
    "\n",
    "1. **Excessive Capitalization**: ALL CAPS words indicate emotional manipulation\n",
    "2. **Excessive Punctuation**: Multiple !!! or ??? suggest sensationalism\n",
    "3. **Urgency Language**: \"BREAKING\", \"URGENT\", \"ACT NOW\" create false urgency\n",
    "4. **Vague Sources**: \"Sources say\", \"experts claim\" without specifics\n",
    "5. **Call-to-Action**: \"SHARE THIS\", \"SPREAD THE WORD\" for virality\n",
    "6. **Conspiracy Language**: \"They don't want you to know\", \"wake up\", \"cover-up\"\n",
    "\n",
    "**Risk Score**: Weighted sum (0-10 scale) where 0 = low risk, 10 = high risk\n",
    "\n",
    "## Limitations\n",
    "\n",
    "⚠️ This is a **pattern-based heuristic**, not a fact-checker:\n",
    "- Does not verify factual accuracy\n",
    "- May flag legitimate urgent news\n",
    "- Cannot detect sophisticated misinformation\n",
    "- Use as screening tool, not definitive classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from social_media_analysis import detect_misinformation_patterns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"rocket\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "DATA_BUCKET = os.getenv(\"DATA_BUCKET\")\n",
    "RESULTS_BUCKET = os.getenv(\"RESULTS_BUCKET\")\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data with Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "df = pd.read_csv(\"../../studio-lab/sample_data.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "print(f\"Loaded {len(df)} posts\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detect Misinformation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply misinformation detection\n",
    "print(\"Analyzing misinformation patterns...\")\n",
    "misinfo_results = df[\"text\"].apply(detect_misinformation_patterns)\n",
    "\n",
    "# Convert results to DataFrame columns\n",
    "misinfo_df = pd.DataFrame(misinfo_results.tolist())\n",
    "df = pd.concat([df, misinfo_df], axis=1)\n",
    "\n",
    "print(\"✓ Pattern detection complete\")\n",
    "print(\"\\nRisk Score Statistics:\")\n",
    "print(df[\"risk_score\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example detections\n",
    "print(\"Example Pattern Detections:\\n\")\n",
    "for idx in range(min(5, len(df))):\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"{idx + 1}. Text: {row['text'][:80]}...\")\n",
    "    print(f\"   Risk Score: {row['risk_score']}\")\n",
    "    print(\"   Indicators:\")\n",
    "    for col in [\"excessive_caps\", \"excessive_punctuation\"]:\n",
    "        if row[col]:\n",
    "            print(f\"     - {col.replace('_', ' ').title()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Risk Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df[\"risk_score\"], bins=range(0, 12), edgecolor=\"black\", color=\"crimson\", alpha=0.7)\n",
    "axes[0].set_title(\"Risk Score Distribution\")\n",
    "axes[0].set_xlabel(\"Risk Score\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].axvline(\n",
    "    df[\"risk_score\"].mean(),\n",
    "    color=\"yellow\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {df['risk_score'].mean():.2f}\",\n",
    ")\n",
    "axes[0].axvline(7, color=\"red\", linestyle=\"--\", linewidth=2, label=\"High Risk Threshold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df[\"risk_score\"], vert=True)\n",
    "axes[1].set_title(\"Risk Score Box Plot\")\n",
    "axes[1].set_ylabel(\"Risk Score\")\n",
    "axes[1].axhline(7, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"High Risk Threshold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identify High-Risk Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define risk categories\n",
    "def categorize_risk(score):\n",
    "    if score >= 7:\n",
    "        return \"High Risk\"\n",
    "    elif score >= 4:\n",
    "        return \"Medium Risk\"\n",
    "    else:\n",
    "        return \"Low Risk\"\n",
    "\n",
    "\n",
    "df[\"risk_category\"] = df[\"risk_score\"].apply(categorize_risk)\n",
    "\n",
    "print(\"Risk Category Distribution:\")\n",
    "print(df[\"risk_category\"].value_counts())\n",
    "print(\"\\nPercentages:\")\n",
    "print((df[\"risk_category\"].value_counts() / len(df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk categories\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "risk_counts = df[\"risk_category\"].value_counts()\n",
    "colors = [\"green\", \"orange\", \"red\"]\n",
    "ax.pie(\n",
    "    risk_counts,\n",
    "    labels=risk_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    explode=[0.05, 0.05, 0.1],\n",
    ")\n",
    "ax.set_title(\"Risk Category Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-risk posts\n",
    "high_risk = df[df[\"risk_category\"] == \"High Risk\"].sort_values(\"risk_score\", ascending=False)\n",
    "\n",
    "print(f\"High-Risk Posts: {len(high_risk)}\")\n",
    "print(\"\\nTop 5 Highest Risk Posts:\\n\")\n",
    "\n",
    "for _, row in high_risk.head(5).iterrows():\n",
    "    print(f\"Risk Score: {row['risk_score']}\")\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    print(f\"Platform: {row['platform']}\")\n",
    "    print(f\"Engagement: {row['retweets'] + row['likes'] + row['replies']}\")\n",
    "    print(\"Indicators detected: \", end=\"\")\n",
    "    indicators = [\n",
    "        col.replace(\"_\", \" \") for col in misinfo_df.columns if col != \"risk_score\" and row[col]\n",
    "    ]\n",
    "    print(\", \".join(indicators))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of each indicator\n",
    "indicator_cols = [col for col in misinfo_df.columns if col != \"risk_score\"]\n",
    "indicator_freq = df[indicator_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"Indicator Frequency:\")\n",
    "print(indicator_freq)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "indicator_freq.plot(kind=\"barh\", ax=ax, color=\"darkred\")\n",
    "ax.set_title(\"Misinformation Indicator Frequency\")\n",
    "ax.set_xlabel(\"Number of Posts\")\n",
    "ax.set_ylabel(\"Indicator\")\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence of indicators\n",
    "print(\"Indicator Co-occurrence Analysis:\\n\")\n",
    "\n",
    "for indicator in indicator_cols:\n",
    "    posts_with_indicator = df[df[indicator]]\n",
    "    if len(posts_with_indicator) > 0:\n",
    "        avg_risk = posts_with_indicator[\"risk_score\"].mean()\n",
    "        print(f\"{indicator.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Posts: {len(posts_with_indicator)}\")\n",
    "        print(f\"  Average Risk Score: {avg_risk:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Risk by Platform and Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment analysis (simple VADER)\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_compound\"] = df[\"text\"].apply(lambda x: vader.polarity_scores(x)[\"compound\"])\n",
    "df[\"sentiment\"] = df[\"sentiment_compound\"].apply(\n",
    "    lambda x: \"positive\" if x >= 0.05 else (\"negative\" if x <= -0.05 else \"neutral\")\n",
    ")\n",
    "\n",
    "print(\"✓ Sentiment added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk by platform\n",
    "platform_risk = df.groupby(\"platform\")[\"risk_score\"].agg([\"mean\", \"median\", \"max\"])\n",
    "print(\"Risk Score by Platform:\")\n",
    "print(platform_risk)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "platform_risk[\"mean\"].plot(kind=\"bar\", ax=ax, color=\"coral\", alpha=0.7, label=\"Mean\")\n",
    "platform_risk[\"median\"].plot(kind=\"bar\", ax=ax, color=\"steelblue\", alpha=0.7, label=\"Median\")\n",
    "ax.set_title(\"Average Risk Score by Platform\")\n",
    "ax.set_xlabel(\"Platform\")\n",
    "ax.set_ylabel(\"Risk Score\")\n",
    "ax.legend()\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk by sentiment\n",
    "sentiment_risk = df.groupby(\"sentiment\")[\"risk_score\"].agg([\"mean\", \"median\", \"max\"])\n",
    "print(\"Risk Score by Sentiment:\")\n",
    "print(sentiment_risk)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.boxplot(column=\"risk_score\", by=\"sentiment\", ax=ax)\n",
    "ax.set_title(\"Risk Score Distribution by Sentiment\")\n",
    "ax.set_xlabel(\"Sentiment\")\n",
    "ax.set_ylabel(\"Risk Score\")\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Engagement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total engagement\n",
    "df[\"total_engagement\"] = df[\"retweets\"] + df[\"likes\"] + df[\"replies\"]\n",
    "\n",
    "# Correlation between risk and engagement\n",
    "correlation = df[[\"risk_score\", \"total_engagement\"]].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation)\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "scatter = ax.scatter(\n",
    "    df[\"risk_score\"], df[\"total_engagement\"], c=df[\"risk_score\"], cmap=\"Reds\", alpha=0.6, s=100\n",
    ")\n",
    "ax.set_title(\"Risk Score vs Total Engagement\")\n",
    "ax.set_xlabel(\"Risk Score\")\n",
    "ax.set_ylabel(\"Total Engagement\")\n",
    "plt.colorbar(scatter, label=\"Risk Score\", ax=ax)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement by risk category\n",
    "engagement_by_risk = df.groupby(\"risk_category\")[\"total_engagement\"].agg([\"mean\", \"median\", \"sum\"])\n",
    "print(\"Engagement by Risk Category:\")\n",
    "print(engagement_by_risk)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average engagement\n",
    "engagement_by_risk[\"mean\"].plot(kind=\"bar\", ax=axes[0], color=[\"green\", \"orange\", \"red\"])\n",
    "axes[0].set_title(\"Average Engagement by Risk Category\")\n",
    "axes[0].set_xlabel(\"Risk Category\")\n",
    "axes[0].set_ylabel(\"Average Engagement\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Total engagement\n",
    "engagement_by_risk[\"sum\"].plot(kind=\"bar\", ax=axes[1], color=[\"green\", \"orange\", \"red\"])\n",
    "axes[1].set_title(\"Total Engagement by Risk Category\")\n",
    "axes[1].set_xlabel(\"Risk Category\")\n",
    "axes[1].set_ylabel(\"Total Engagement\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk over time\n",
    "df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "daily_risk = df.groupby(\"date\")[\"risk_score\"].agg([\"mean\", \"max\", \"count\"])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Average risk over time\n",
    "axes[0].plot(daily_risk.index, daily_risk[\"mean\"], marker=\"o\", linewidth=2, color=\"crimson\")\n",
    "axes[0].fill_between(daily_risk.index, daily_risk[\"mean\"], alpha=0.3, color=\"crimson\")\n",
    "axes[0].axhline(7, color=\"red\", linestyle=\"--\", label=\"High Risk Threshold\")\n",
    "axes[0].set_title(\"Average Risk Score Over Time\")\n",
    "axes[0].set_ylabel(\"Average Risk Score\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Post volume by risk category\n",
    "daily_risk_cat = df.groupby([\"date\", \"risk_category\"]).size().unstack(fill_value=0)\n",
    "daily_risk_cat.plot(kind=\"bar\", stacked=True, ax=axes[1], color=[\"green\", \"orange\", \"red\"])\n",
    "axes[1].set_title(\"Post Volume by Risk Category Over Time\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].set_ylabel(\"Number of Posts\")\n",
    "axes[1].legend(title=\"Risk Category\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Word Cloud of High-Risk Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud from high-risk posts\n",
    "if len(high_risk) > 0:\n",
    "    high_risk_text = \" \".join(high_risk[\"text\"].values)\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, background_color=\"white\", colormap=\"Reds\", max_words=100\n",
    "    ).generate(high_risk_text)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Word Cloud: High-Risk Posts\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No high-risk posts to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full results\n",
    "output_file = \"../../results/misinformation_analysis_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"✓ Full results saved to {output_file}\")\n",
    "\n",
    "# Save high-risk posts separately\n",
    "if len(high_risk) > 0:\n",
    "    high_risk_file = \"../../results/high_risk_posts.csv\"\n",
    "    high_risk.to_csv(high_risk_file, index=False)\n",
    "    print(f\"✓ High-risk posts saved to {high_risk_file}\")\n",
    "\n",
    "# Uncomment to save to S3\n",
    "# data_client = SocialMediaDataAccess()\n",
    "# data_client.save_results(df, 'misinformation_analysis_results.csv')\n",
    "# data_client.save_results(high_risk, 'high_risk_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Risk Distribution\n",
    "- **High Risk**: X% of posts (score ≥ 7)\n",
    "- **Medium Risk**: Y% of posts (score 4-6)\n",
    "- **Low Risk**: Z% of posts (score < 4)\n",
    "- **Average Risk Score**: [value]\n",
    "\n",
    "### Most Common Indicators\n",
    "1. [Indicator 1]: X posts\n",
    "2. [Indicator 2]: Y posts\n",
    "3. [Indicator 3]: Z posts\n",
    "\n",
    "### Platform Analysis\n",
    "- [Platform] has highest average risk score: [value]\n",
    "- [Platform] has most high-risk posts: [count]\n",
    "\n",
    "### Sentiment Relationship\n",
    "- [Sentiment] posts have highest risk scores\n",
    "- Correlation with sentiment: [value]\n",
    "\n",
    "### Engagement Patterns\n",
    "- High-risk posts receive [more/less/similar] engagement than low-risk\n",
    "- Correlation between risk and engagement: [value]\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Manual Review**: All posts with risk score ≥ 7 should be manually reviewed\n",
    "2. **Pattern Monitoring**: Track indicator trends over time to detect campaigns\n",
    "3. **Enhanced Detection**: Consider ML-based fact-checking for high-risk posts\n",
    "4. **User Education**: Flag high-risk content with educational context\n",
    "5. **Platform Action**: Share findings with platform moderators\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Network Analysis**: Run `04-network-analysis.ipynb` to understand information spread\n",
    "2. **Fact-Checking**: Integrate with fact-checking APIs for verification\n",
    "3. **ML Model**: Train supervised model with labeled misinformation data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}