{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analysis & Misinformation Detection\n",
    "\n",
    "**Studio Lab Quickstart** | Research Jumpstart\n",
    "\n",
    "This notebook demonstrates social media analysis techniques including:\n",
    "- Sentiment analysis\n",
    "- Topic modeling\n",
    "- Misinformation pattern detection\n",
    "- Engagement analysis\n",
    "- Network visualization\n",
    "\n",
    "**Time to complete**: 3-4 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "Import required libraries and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# NLP and text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Load the sample social media dataset and examine basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"\\nPlatforms: {df['platform'].value_counts().to_dict()}\")\n",
    "print(f\"\\nTotal engagement: {(df['retweets'] + df['likes'] + df['replies']).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample posts\n",
    "print(\"Sample posts:\\n\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"Post {idx + 1}:\")\n",
    "    print(f\"  Platform: {row['platform']}\")\n",
    "    print(f\"  Text: {row['text']}\")\n",
    "    print(f\"  Engagement: {row['likes']} likes, {row['retweets']} retweets, {row['replies']} replies\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "Clean and prepare text data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: Input text string\n",
    "    - remove_stopwords: Whether to remove stop words\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned text string\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags (but keep the text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Remove short words\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(df['text'].iloc[0])\n",
    "print(\"\\nCleaned text:\")\n",
    "print(df['text_clean'].iloc[0])\n",
    "print(\"\\nâœ“ Text preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "\n",
    "Analyze sentiment using VADER (Valence Aware Dictionary and sEntiment Reasoner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment scores using VADER.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with neg, neu, pos, compound scores\n",
    "    \"\"\"\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "def classify_sentiment(compound_score):\n",
    "    \"\"\"\n",
    "    Classify sentiment based on compound score.\n",
    "    \n",
    "    Returns:\n",
    "    - 'positive', 'negative', or 'neutral'\n",
    "    \"\"\"\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Calculate sentiment scores\n",
    "sentiment_scores = df['text'].apply(get_sentiment)\n",
    "df['sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "df['sentiment_pos'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "df['sentiment_neg'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "df['sentiment_neu'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "df['sentiment_label'] = df['sentiment_compound'].apply(classify_sentiment)\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"\\nAverage compound score: {df['sentiment_compound'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment counts\n",
    "sentiment_counts = df['sentiment_label'].value_counts()\n",
    "axes[0].bar(sentiment_counts.index, sentiment_counts.values, color=['green', 'gray', 'red'])\n",
    "axes[0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Compound score distribution\n",
    "axes[1].hist(df['sentiment_compound'], bins=20, color='skyblue', edgecolor='black')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', label='Neutral threshold')\n",
    "axes[1].set_title('Compound Sentiment Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Compound Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Sentiment analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling\n",
    "\n",
    "Discover main topics using Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for topic modeling\n",
    "n_topics = 3\n",
    "n_top_words = 10\n",
    "\n",
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer(max_features=100, min_df=1, max_df=0.8)\n",
    "doc_term_matrix = vectorizer.fit_transform(df['text_clean'])\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Assign dominant topic to each document\n",
    "df['dominant_topic'] = lda_output.argmax(axis=1)\n",
    "\n",
    "print(f\"Trained LDA model with {n_topics} topics\\n\")\n",
    "print(\"Topic distribution:\")\n",
    "print(df['dominant_topic'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top words per topic:\\n\")\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ“ Topic modeling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Engagement Analysis\n",
    "\n",
    "Analyze engagement patterns and identify viral content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total engagement\n",
    "df['total_engagement'] = df['retweets'] + df['likes'] + df['replies']\n",
    "\n",
    "# Identify viral content (top 20%)\n",
    "engagement_threshold = df['total_engagement'].quantile(0.80)\n",
    "df['is_viral'] = df['total_engagement'] >= engagement_threshold\n",
    "\n",
    "print(f\"Engagement threshold for viral content: {engagement_threshold}\")\n",
    "print(f\"\\nViral posts: {df['is_viral'].sum()} ({df['is_viral'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nTop 3 most engaging posts:\")\n",
    "print(df.nlargest(3, 'total_engagement')[['text', 'total_engagement', 'sentiment_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engagement by platform\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "platform_engagement = df.groupby('platform')['total_engagement'].sum().sort_values(ascending=False)\n",
    "platform_engagement.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Total Engagement by Platform', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Platform')\n",
    "ax.set_ylabel('Total Engagement')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Engagement vs sentiment correlation\n",
    "correlation = df['total_engagement'].corr(df['sentiment_compound'])\n",
    "print(f\"\\nCorrelation between engagement and sentiment: {correlation:.3f}\")\n",
    "print(\"âœ“ Engagement analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Misinformation Pattern Detection\n",
    "\n",
    "Detect potential misinformation indicators using pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_misinformation_patterns(text):\n",
    "    \"\"\"\n",
    "    Detect misinformation indicators in text.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with indicator flags and risk score\n",
    "    \"\"\"\n",
    "    indicators = {}\n",
    "    \n",
    "    # 1. ALL CAPS words\n",
    "    caps_words = re.findall(r'\\b[A-Z]{3,}\\b', text)\n",
    "    indicators['excessive_caps'] = len(caps_words) > 2\n",
    "    indicators['caps_count'] = len(caps_words)\n",
    "    \n",
    "    # 2. Excessive punctuation\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    indicators['excessive_punctuation'] = (exclamation_count > 2) or (question_count > 2)\n",
    "    \n",
    "    # 3. Urgency language\n",
    "    urgency_keywords = ['breaking', 'urgent', 'immediately', 'right now', 'before its too late']\n",
    "    indicators['urgency_language'] = any(keyword in text.lower() for keyword in urgency_keywords)\n",
    "    \n",
    "    # 4. Vague sources\n",
    "    vague_sources = ['they', 'mainstream media', 'they dont want', 'theyre hiding']\n",
    "    indicators['vague_sources'] = any(source in text.lower() for source in vague_sources)\n",
    "    \n",
    "    # 5. Call to action\n",
    "    cta_keywords = ['share', 'retweet', 'wake up', 'open your eyes', 'do your own research']\n",
    "    indicators['call_to_action'] = any(keyword in text.lower() for keyword in cta_keywords)\n",
    "    \n",
    "    # 6. Conspiracy keywords\n",
    "    conspiracy_keywords = ['truth', 'coverup', 'cover-up', 'conspiracy', 'deep state', 'globalist']\n",
    "    indicators['conspiracy_language'] = any(keyword in text.lower() for keyword in conspiracy_keywords)\n",
    "    \n",
    "    # Calculate risk score (0-10)\n",
    "    risk_score = sum([\n",
    "        indicators['excessive_caps'] * 2,\n",
    "        indicators['excessive_punctuation'] * 1,\n",
    "        indicators['urgency_language'] * 2,\n",
    "        indicators['vague_sources'] * 2,\n",
    "        indicators['call_to_action'] * 2,\n",
    "        indicators['conspiracy_language'] * 1\n",
    "    ])\n",
    "    \n",
    "    indicators['risk_score'] = min(risk_score, 10)  # Cap at 10\n",
    "    \n",
    "    return indicators\n",
    "\n",
    "# Apply misinformation detection\n",
    "misinfo_results = df['text'].apply(detect_misinformation_patterns)\n",
    "df['risk_score'] = misinfo_results.apply(lambda x: x['risk_score'])\n",
    "df['has_caps'] = misinfo_results.apply(lambda x: x['excessive_caps'])\n",
    "df['has_urgency'] = misinfo_results.apply(lambda x: x['urgency_language'])\n",
    "df['has_vague_sources'] = misinfo_results.apply(lambda x: x['vague_sources'])\n",
    "\n",
    "# Classify risk level\n",
    "def classify_risk(score):\n",
    "    if score <= 2:\n",
    "        return 'low'\n",
    "    elif score <= 5:\n",
    "        return 'medium'\n",
    "    elif score <= 8:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'critical'\n",
    "\n",
    "df['risk_level'] = df['risk_score'].apply(classify_risk)\n",
    "\n",
    "print(\"Risk Level Distribution:\")\n",
    "print(df['risk_level'].value_counts())\n",
    "print(f\"\\nAverage risk score: {df['risk_score'].mean():.2f}\")\n",
    "print(f\"\\nHigh-risk posts (score > 5): {(df['risk_score'] > 5).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Risk level distribution\n",
    "risk_order = ['low', 'medium', 'high', 'critical']\n",
    "risk_counts = df['risk_level'].value_counts().reindex(risk_order, fill_value=0)\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "axes[0].bar(risk_counts.index, risk_counts.values, color=colors)\n",
    "axes[0].set_title('Misinformation Risk Level Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Risk Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Risk score histogram\n",
    "axes[1].hist(df['risk_score'], bins=11, range=(0, 10), color='salmon', edgecolor='black')\n",
    "axes[1].axvline(x=5, color='red', linestyle='--', label='High risk threshold')\n",
    "axes[1].set_title('Risk Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Risk Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show high-risk posts\n",
    "high_risk = df[df['risk_score'] > 5].sort_values('risk_score', ascending=False)\n",
    "if len(high_risk) > 0:\n",
    "    print(\"\\nHigh-risk posts:\")\n",
    "    for idx, row in high_risk.iterrows():\n",
    "        print(f\"\\nRisk Score: {row['risk_score']}\")\n",
    "        print(f\"Text: {row['text']}\")\n",
    "\n",
    "print(\"\\nâœ“ Misinformation pattern detection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics\n",
    "\n",
    "Generate comprehensive summary of findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SOCIAL MEDIA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET OVERVIEW\")\n",
    "print(f\"  Total posts analyzed: {len(df)}\")\n",
    "print(f\"  Date range: {df['timestamp'].min().date()} to {df['timestamp'].max().date()}\")\n",
    "print(f\"  Platforms: {', '.join(df['platform'].unique())}\")\n",
    "\n",
    "print(f\"\\nðŸ˜Š SENTIMENT ANALYSIS\")\n",
    "sentiment_pct = df['sentiment_label'].value_counts(normalize=True) * 100\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    pct = sentiment_pct.get(sentiment, 0)\n",
    "    print(f\"  {sentiment.capitalize()}: {pct:.1f}%\")\n",
    "print(f\"  Average sentiment: {df['sentiment_compound'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ TOPIC MODELING\")\n",
    "print(f\"  Number of topics identified: {n_topics}\")\n",
    "topic_dist = df['dominant_topic'].value_counts().sort_index()\n",
    "for topic_id, count in topic_dist.items():\n",
    "    print(f\"  Topic {topic_id}: {count} posts ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸš€ ENGAGEMENT METRICS\")\n",
    "print(f\"  Total engagement: {df['total_engagement'].sum():,}\")\n",
    "print(f\"  Average engagement per post: {df['total_engagement'].mean():.1f}\")\n",
    "print(f\"  Viral posts (top 20%): {df['is_viral'].sum()}\")\n",
    "print(f\"  Most engaged platform: {df.groupby('platform')['total_engagement'].sum().idxmax()}\")\n",
    "\n",
    "print(f\"\\nâš ï¸ MISINFORMATION RISK\")\n",
    "risk_dist = df['risk_level'].value_counts()\n",
    "for level in ['low', 'medium', 'high', 'critical']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {level.capitalize()} risk: {count} posts ({pct:.1f}%)\")\n",
    "print(f\"  Average risk score: {df['risk_score'].mean():.2f} / 10\")\n",
    "\n",
    "print(f\"\\nðŸ” KEY FINDINGS\")\n",
    "print(f\"  â€¢ Most common sentiment: {df['sentiment_label'].mode()[0]}\")\n",
    "print(f\"  â€¢ Highest engagement post had {df['total_engagement'].max():,} interactions\")\n",
    "print(f\"  â€¢ {(df['risk_score'] > 5).sum()} posts flagged as high-risk\")\n",
    "print(f\"  â€¢ Sentiment-engagement correlation: {df['total_engagement'].corr(df['sentiment_compound']):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis complete! Review visualizations above for details.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Experiment with This Notebook\n",
    "\n",
    "1. **Modify parameters**:\n",
    "   - Adjust number of topics in LDA\n",
    "   - Change risk score thresholds\n",
    "   - Modify sentiment classification ranges\n",
    "\n",
    "2. **Add your own data**:\n",
    "   - Replace `sample_data.csv` with your dataset\n",
    "   - Ensure same column structure\n",
    "   - Increase dataset size\n",
    "\n",
    "3. **Extend the analysis**:\n",
    "   - Add temporal analysis (trends over time)\n",
    "   - Create word clouds for different topics\n",
    "   - Analyze user networks\n",
    "   - Compare platforms\n",
    "\n",
    "### Scale to Production\n",
    "\n",
    "When ready for real social media data:\n",
    "\n",
    "1. Review the [Unified Studio README](../unified-studio/README.md)\n",
    "2. Set up AWS account\n",
    "3. Access real Twitter/Reddit datasets\n",
    "4. Use Amazon Comprehend for advanced NLP\n",
    "5. Deploy with Amazon Bedrock for AI content analysis\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [VADER Sentiment Analysis Paper](https://ojs.aaai.org/index.php/ICWSM/article/view/14550)\n",
    "- [LDA Topic Modeling Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html)\n",
    "- [First Draft Misinformation Research](https://firstdraftnews.org/)\n",
    "- [Computational Social Science](https://compsocialscience.github.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Open an issue at https://github.com/research-jumpstart/research-jumpstart/issues\n",
    "\n",
    "*Last updated: 2025-11-09*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
