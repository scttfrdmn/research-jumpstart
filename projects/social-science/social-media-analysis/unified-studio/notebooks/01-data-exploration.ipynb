{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Data Exploration\n",
    "\n",
    "This notebook demonstrates how to load and explore social media datasets using the unified-studio package.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Configure AWS credentials and environment\n",
    "2. Initialize data access client\n",
    "3. Load data from multiple sources (Twitter, Reddit, CSV)\n",
    "4. Perform data quality validation\n",
    "5. Explore dataset characteristics\n",
    "6. Generate summary statistics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- CloudFormation stack deployed\n",
    "- `.env` file configured with bucket names and role ARN\n",
    "- Python package installed: `pip install -e ..`\n",
    "- Sample data uploaded to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for local development\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "# Import our package\n",
    "from social_media_analysis import SocialMediaDataAccess\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(Path('..') / '.env')\n",
    "\n",
    "# Get configuration\n",
    "DATA_BUCKET = os.getenv('DATA_BUCKET')\n",
    "RESULTS_BUCKET = os.getenv('RESULTS_BUCKET')\n",
    "AWS_REGION = os.getenv('AWS_REGION', 'us-east-1')\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data Bucket: {DATA_BUCKET}\")\n",
    "print(f\"  Results Bucket: {RESULTS_BUCKET}\")\n",
    "print(f\"  Region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Data Access Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data access client\n",
    "data_client = SocialMediaDataAccess(\n",
    "    use_anon=False,  # Use configured AWS credentials\n",
    "    region=AWS_REGION\n",
    ")\n",
    "\n",
    "print(\"✓ Data access client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data\n",
    "\n",
    "We'll start by loading the sample CSV dataset from Studio Lab as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll use a local CSV file first\n",
    "# In production, you would load from S3\n",
    "\n",
    "# Load sample data from studio-lab\n",
    "sample_df = pd.read_csv('../../studio-lab/sample_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(sample_df)} posts\")\n",
    "print(f\"\\nDataset shape: {sample_df.shape}\")\n",
    "print(f\"\\nColumns: {list(sample_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic validation\n",
    "validation_results = data_client.validate_dataset(sample_df)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "for key, value in validation_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(sample_df.isnull().sum())\n",
    "print(f\"\\nMissing percentage:\")\n",
    "print((sample_df.isnull().sum() / len(sample_df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"Data Types:\")\n",
    "print(sample_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "sample_df['timestamp'] = pd.to_datetime(sample_df['timestamp'])\n",
    "\n",
    "print(\"✓ Timestamp converted to datetime\")\n",
    "print(f\"Date range: {sample_df['timestamp'].min()} to {sample_df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for engagement metrics\n",
    "engagement_cols = ['retweets', 'likes', 'replies']\n",
    "print(\"Engagement Metrics Summary:\")\n",
    "sample_df[engagement_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform distribution\n",
    "print(\"Platform Distribution:\")\n",
    "print(sample_df['platform'].value_counts())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sample_df['platform'].value_counts().plot(kind='bar', ax=ax)\n",
    "ax.set_title('Posts by Platform')\n",
    "ax.set_xlabel('Platform')\n",
    "ax.set_ylabel('Number of Posts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "sample_df['text_length'] = sample_df['text'].str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(sample_df['text_length'].describe())\n",
    "\n",
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sample_df['text_length'].hist(bins=20, ax=ax, edgecolor='black')\n",
    "ax.set_title('Distribution of Post Length')\n",
    "ax.set_xlabel('Text Length (characters)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.axvline(sample_df['text_length'].mean(), color='red', \n",
    "           linestyle='--', label=f'Mean: {sample_df[\"text_length\"].mean():.0f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement analysis\n",
    "sample_df['total_engagement'] = (\n",
    "    sample_df['retweets'] + \n",
    "    sample_df['likes'] + \n",
    "    sample_df['replies']\n",
    ")\n",
    "\n",
    "print(\"Total Engagement Statistics:\")\n",
    "print(sample_df['total_engagement'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement by platform\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, metric in enumerate(engagement_cols):\n",
    "    sample_df.groupby('platform')[metric].mean().plot(\n",
    "        kind='bar', ax=axes[idx], color='skyblue'\n",
    "    )\n",
    "    axes[idx].set_title(f'Average {metric.capitalize()} by Platform')\n",
    "    axes[idx].set_xlabel('Platform')\n",
    "    axes[idx].set_ylabel(f'Average {metric.capitalize()}')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis\n",
    "sample_df['hour'] = sample_df['timestamp'].dt.hour\n",
    "sample_df['day_of_week'] = sample_df['timestamp'].dt.day_name()\n",
    "\n",
    "print(\"Posts by Hour:\")\n",
    "print(sample_df['hour'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for engagement metrics\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "correlation = sample_df[engagement_cols + ['text_length']].corr()\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Correlation Matrix: Engagement Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading Data from S3 (Production)\n",
    "\n",
    "In production, you would load data directly from S3. Here are examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load Twitter data from S3\n",
    "# Uncomment and modify for your data\n",
    "\n",
    "# twitter_df = data_client.load_twitter_dataset(\n",
    "#     bucket=DATA_BUCKET,\n",
    "#     prefix='twitter/2025/11/',\n",
    "#     date_range=('2025-11-01', '2025-11-07'),\n",
    "#     sample_size=10000  # Load 10K posts for testing\n",
    "# )\n",
    "# \n",
    "# print(f\"Loaded {len(twitter_df)} tweets\")\n",
    "# twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load Reddit data from S3\n",
    "# Uncomment and modify for your data\n",
    "\n",
    "# reddit_df = data_client.load_reddit_dataset(\n",
    "#     bucket=DATA_BUCKET,\n",
    "#     prefix='reddit/2025/11/',\n",
    "#     subreddits=['politics', 'news', 'worldnews'],\n",
    "#     sample_size=10000\n",
    "# )\n",
    "# \n",
    "# print(f\"Loaded {len(reddit_df)} Reddit posts\")\n",
    "# reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load CSV from S3\n",
    "# Uncomment and modify for your data\n",
    "\n",
    "# csv_df = data_client.load_csv_dataset(\n",
    "#     bucket=DATA_BUCKET,\n",
    "#     key='datasets/social_media_sample.csv'\n",
    "# )\n",
    "# \n",
    "# print(f\"Loaded {len(csv_df)} posts from CSV\")\n",
    "# csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save exploration results\n",
    "exploration_summary = pd.DataFrame({\n",
    "    'metric': ['total_posts', 'avg_text_length', 'avg_retweets', \n",
    "               'avg_likes', 'avg_replies', 'avg_total_engagement'],\n",
    "    'value': [\n",
    "        len(sample_df),\n",
    "        sample_df['text_length'].mean(),\n",
    "        sample_df['retweets'].mean(),\n",
    "        sample_df['likes'].mean(),\n",
    "        sample_df['replies'].mean(),\n",
    "        sample_df['total_engagement'].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Exploration Summary:\")\n",
    "print(exploration_summary)\n",
    "\n",
    "# Uncomment to save to S3\n",
    "# data_client.save_results(exploration_summary, 'exploration_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings\n",
    "\n",
    "Summary of insights from data exploration:\n",
    "\n",
    "1. **Data Quality**: Dataset contains X posts with Y% missing values\n",
    "2. **Engagement Patterns**: Average engagement is Z, with [platform] showing highest activity\n",
    "3. **Text Characteristics**: Posts average [X] characters, ranging from [min] to [max]\n",
    "4. **Temporal Patterns**: Peak posting times are [hours], most active day is [day]\n",
    "5. **Platform Distribution**: [platform] comprises [X]% of dataset\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Sentiment Analysis**: Run notebook `02-sentiment-analysis.ipynb`\n",
    "2. **Misinformation Detection**: Run notebook `03-misinformation-detection.ipynb`\n",
    "3. **Network Analysis**: Run notebook `04-network-analysis.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
