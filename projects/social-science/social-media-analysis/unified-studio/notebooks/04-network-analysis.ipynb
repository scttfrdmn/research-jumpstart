{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis: Information Spread and Influence\n",
    "\n",
    "This notebook demonstrates social network analysis techniques to understand how information spreads through social media platforms.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Build interaction networks from social media data\n",
    "2. Calculate centrality metrics (degree, betweenness, eigenvector)\n",
    "3. Identify influential users and information hubs\n",
    "4. Detect communities and echo chambers\n",
    "5. Analyze network structure and properties\n",
    "6. Visualize information flow patterns\n",
    "\n",
    "## Network Concepts\n",
    "\n",
    "### Node Centrality Metrics\n",
    "\n",
    "**Degree Centrality**: Number of direct connections\n",
    "- High degree = many interactions\n",
    "- Indicates popular/active users\n",
    "\n",
    "**Betweenness Centrality**: Number of shortest paths passing through node\n",
    "- High betweenness = information bridge\n",
    "- Indicates users connecting different communities\n",
    "\n",
    "**Eigenvector Centrality**: Connections to well-connected nodes\n",
    "- High eigenvector = influence\n",
    "- Indicates users connected to other influential users\n",
    "\n",
    "### Community Detection\n",
    "\n",
    "Identifies clusters of densely connected users:\n",
    "- Echo chambers\n",
    "- Interest groups\n",
    "- Coordinated campaigns\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Misinformation Spread**: Track how false information propagates\n",
    "- **Influence Mapping**: Identify key opinion leaders\n",
    "- **Echo Chambers**: Detect polarized communities\n",
    "- **Coordination Detection**: Find coordinated inauthentic behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "from social_media_analysis import SocialMediaDataAccess\n",
    "from social_media_analysis.network_analysis import (\n",
    "    build_interaction_network,\n",
    "    calculate_centrality,\n",
    "    detect_communities\n",
    ")\n",
    "from social_media_analysis.visualization import plot_network_graph\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('tab10')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "load_dotenv(Path('..') / '.env')\n",
    "\n",
    "DATA_BUCKET = os.getenv('DATA_BUCKET')\n",
    "RESULTS_BUCKET = os.getenv('RESULTS_BUCKET')\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "df = pd.read_csv('../../studio-lab/sample_data.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(df)} posts\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For network analysis, we need interaction data\n",
    "# Since sample data doesn't have retweet/reply relationships,\n",
    "# we'll simulate a simple network based on user interactions\n",
    "\n",
    "# Create synthetic interaction edges\n",
    "# In production, you would extract actual retweets, mentions, and replies\n",
    "\n",
    "# Simulate user interaction network\n",
    "np.random.seed(42)\n",
    "users = df['user_id'].unique()\n",
    "n_edges = min(50, len(df) * 2)  # 2 interactions per post on average\n",
    "\n",
    "edges = []\n",
    "for _ in range(n_edges):\n",
    "    source = np.random.choice(users)\n",
    "    target = np.random.choice(users)\n",
    "    if source != target:\n",
    "        edges.append((source, target))\n",
    "\n",
    "# Create edge DataFrame\n",
    "edges_df = pd.DataFrame(edges, columns=['source', 'target'])\n",
    "edges_df['weight'] = 1  # Each interaction has weight 1\n",
    "\n",
    "# Aggregate to count multiple interactions\n",
    "edges_df = edges_df.groupby(['source', 'target']).sum().reset_index()\n",
    "\n",
    "print(f\"Created {len(edges_df)} interaction edges between {len(users)} users\")\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directed graph\n",
    "G = nx.from_pandas_edgelist(\n",
    "    edges_df,\n",
    "    source='source',\n",
    "    target='target',\n",
    "    edge_attr='weight',\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"Network Statistics:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Density: {nx.density(G):.4f}\")\n",
    "print(f\"  Is Connected: {nx.is_weakly_connected(G)}\")\n",
    "\n",
    "if nx.is_weakly_connected(G):\n",
    "    print(f\"  Average Shortest Path: {nx.average_shortest_path_length(G):.2f}\")\n",
    "    print(f\"  Diameter: {nx.diameter(G)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Centrality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality metrics\n",
    "print(\"Calculating centrality metrics...\")\n",
    "\n",
    "# Degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "out_degree_centrality = nx.out_degree_centrality(G)\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Eigenvector centrality (may not converge for all graphs)\n",
    "try:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "except:\n",
    "    print(\"  ⚠️ Eigenvector centrality failed to converge, using PageRank instead\")\n",
    "    eigenvector_centrality = nx.pagerank(G)\n",
    "\n",
    "# PageRank (Google's algorithm)\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "print(\"✓ Centrality metrics calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centrality DataFrame\n",
    "centrality_df = pd.DataFrame({\n",
    "    'user_id': list(G.nodes()),\n",
    "    'degree': [degree_centrality[node] for node in G.nodes()],\n",
    "    'in_degree': [in_degree_centrality[node] for node in G.nodes()],\n",
    "    'out_degree': [out_degree_centrality[node] for node in G.nodes()],\n",
    "    'betweenness': [betweenness_centrality[node] for node in G.nodes()],\n",
    "    'eigenvector': [eigenvector_centrality[node] for node in G.nodes()],\n",
    "    'pagerank': [pagerank[node] for node in G.nodes()]\n",
    "})\n",
    "\n",
    "centrality_df = centrality_df.sort_values('pagerank', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Influential Users (by PageRank):\")\n",
    "print(centrality_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Centrality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['degree', 'in_degree', 'out_degree', 'betweenness', 'eigenvector', 'pagerank']\n",
    "colors = ['skyblue', 'lightgreen', 'coral', 'violet', 'gold', 'crimson']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].hist(centrality_df[metric], bins=20, color=colors[idx], \n",
    "                  edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()} Distribution')\n",
    "    axes[idx].set_xlabel(metric.replace('_', ' ').title())\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].axvline(centrality_df[metric].mean(), color='red', \n",
    "                     linestyle='--', label=f'Mean: {centrality_df[metric].mean():.4f}')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between centrality metrics\n",
    "correlation = centrality_df[metrics].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            square=True, ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Correlation Matrix: Centrality Metrics', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identify Key Influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top influencers by different metrics\n",
    "top_n = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY INFLUENCERS BY METRIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric in ['degree', 'betweenness', 'pagerank']:\n",
    "    top_users = centrality_df.nlargest(top_n, metric)[['user_id', metric]]\n",
    "    print(f\"\\nTop {top_n} by {metric.replace('_', ' ').title()}:\")\n",
    "    for idx, row in top_users.iterrows():\n",
    "        print(f\"  {row['user_id']}: {row[metric]:.4f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize users by role\n",
    "def categorize_user(row):\n",
    "    if row['pagerank'] > centrality_df['pagerank'].quantile(0.9):\n",
    "        return 'Influencer'\n",
    "    elif row['betweenness'] > centrality_df['betweenness'].quantile(0.9):\n",
    "        return 'Bridge'\n",
    "    elif row['in_degree'] > centrality_df['in_degree'].quantile(0.9):\n",
    "        return 'Popular'\n",
    "    elif row['out_degree'] > centrality_df['out_degree'].quantile(0.9):\n",
    "        return 'Active'\n",
    "    else:\n",
    "        return 'Regular'\n",
    "\n",
    "centrality_df['role'] = centrality_df.apply(categorize_user, axis=1)\n",
    "\n",
    "print(\"User Role Distribution:\")\n",
    "print(centrality_df['role'].value_counts())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "role_counts = centrality_df['role'].value_counts()\n",
    "role_counts.plot(kind='bar', ax=ax, color='teal', alpha=0.7)\n",
    "ax.set_title('User Role Distribution in Network')\n",
    "ax.set_xlabel('Role')\n",
    "ax.set_ylabel('Number of Users')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to undirected for community detection\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# Detect communities using Louvain method (better than greedy modularity)\n",
    "try:\n",
    "    import community as community_louvain\n",
    "    partition = community_louvain.best_partition(G_undirected)\n",
    "    print(\"Using Louvain method\")\n",
    "except ImportError:\n",
    "    # Fallback to greedy modularity\n",
    "    print(\"Using greedy modularity method (install python-louvain for better results)\")\n",
    "    communities = list(nx.community.greedy_modularity_communities(G_undirected))\n",
    "    partition = {}\n",
    "    for idx, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            partition[node] = idx\n",
    "\n",
    "# Add community to centrality DataFrame\n",
    "centrality_df['community'] = centrality_df['user_id'].map(partition)\n",
    "\n",
    "n_communities = len(set(partition.values()))\n",
    "print(f\"\\n✓ Detected {n_communities} communities\")\n",
    "print(f\"\\nCommunity Size Distribution:\")\n",
    "print(centrality_df['community'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize community sizes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "community_sizes = centrality_df['community'].value_counts().sort_index()\n",
    "community_sizes.plot(kind='bar', ax=axes[0], color='steelblue', alpha=0.7)\n",
    "axes[0].set_title('Community Size Distribution')\n",
    "axes[0].set_xlabel('Community ID')\n",
    "axes[0].set_ylabel('Number of Users')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(community_sizes, labels=[f'Community {i}' for i in community_sizes.index],\n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Community Composition')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each community\n",
    "print(\"Community Analysis:\\n\")\n",
    "\n",
    "for community_id in sorted(centrality_df['community'].unique()):\n",
    "    community_users = centrality_df[centrality_df['community'] == community_id]\n",
    "    \n",
    "    print(f\"Community {community_id}:\")\n",
    "    print(f\"  Size: {len(community_users)} users\")\n",
    "    print(f\"  Average PageRank: {community_users['pagerank'].mean():.4f}\")\n",
    "    print(f\"  Average Betweenness: {community_users['betweenness'].mean():.4f}\")\n",
    "    \n",
    "    # Top influencer in community\n",
    "    top_user = community_users.nlargest(1, 'pagerank').iloc[0]\n",
    "    print(f\"  Top Influencer: {top_user['user_id']} (PageRank: {top_user['pagerank']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualization with communities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G_undirected, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Left: Color by community\n",
    "node_colors_community = [partition[node] for node in G_undirected.nodes()]\n",
    "node_sizes = [centrality_df[centrality_df['user_id'] == node]['pagerank'].values[0] * 3000 \n",
    "             for node in G_undirected.nodes()]\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G_undirected, pos, ax=axes[0],\n",
    "    node_color=node_colors_community,\n",
    "    node_size=node_sizes,\n",
    "    cmap='tab10',\n",
    "    with_labels=False,\n",
    "    edge_color='gray',\n",
    "    alpha=0.7,\n",
    "    width=0.5\n",
    ")\n",
    "axes[0].set_title('Network Colored by Community\\n(Node size = PageRank)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Right: Color by PageRank\n",
    "node_colors_pagerank = [centrality_df[centrality_df['user_id'] == node]['pagerank'].values[0] \n",
    "                       for node in G_undirected.nodes()]\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G_undirected, pos, ax=axes[1],\n",
    "    node_color=node_colors_pagerank,\n",
    "    node_size=node_sizes,\n",
    "    cmap='Reds',\n",
    "    with_labels=False,\n",
    "    edge_color='gray',\n",
    "    alpha=0.7,\n",
    "    width=0.5\n",
    ")\n",
    "axes[1].set_title('Network Colored by PageRank\\n(Redder = More Influential)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Degree Distribution (Power Law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution analysis\n",
    "degrees = [G.degree(node) for node in G.nodes()]\n",
    "degree_counts = Counter(degrees)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].bar(degree_counts.keys(), degree_counts.values(), color='steelblue', alpha=0.7)\n",
    "axes[0].set_title('Degree Distribution (Linear Scale)')\n",
    "axes[0].set_xlabel('Degree')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Log-log scale (to see power law)\n",
    "axes[1].scatter(list(degree_counts.keys()), list(degree_counts.values()), \n",
    "               color='crimson', alpha=0.7, s=50)\n",
    "axes[1].set_title('Degree Distribution (Log-Log Scale)')\n",
    "axes[1].set_xlabel('Degree (log scale)')\n",
    "axes[1].set_ylabel('Frequency (log scale)')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Degree: {np.mean(degrees):.2f}\")\n",
    "print(f\"Max Degree: {max(degrees)}\")\n",
    "print(f\"Min Degree: {min(degrees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Information Diffusion Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate information spread from top influencer\n",
    "def simulate_diffusion(G, source, steps=5, transmission_prob=0.3):\n",
    "    \"\"\"Simulate information diffusion through network.\"\"\"\n",
    "    infected = {source}\n",
    "    newly_infected = {source}\n",
    "    infection_time = {source: 0}\n",
    "    \n",
    "    for step in range(1, steps + 1):\n",
    "        next_infected = set()\n",
    "        for node in newly_infected:\n",
    "            # Try to infect neighbors\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if neighbor not in infected and np.random.random() < transmission_prob:\n",
    "                    next_infected.add(neighbor)\n",
    "                    infection_time[neighbor] = step\n",
    "        \n",
    "        infected.update(next_infected)\n",
    "        newly_infected = next_infected\n",
    "        \n",
    "        if not newly_infected:\n",
    "            break\n",
    "    \n",
    "    return infected, infection_time\n",
    "\n",
    "# Run simulation from top influencer\n",
    "top_influencer = centrality_df.nlargest(1, 'pagerank').iloc[0]['user_id']\n",
    "infected, infection_time = simulate_diffusion(G, top_influencer, steps=10, transmission_prob=0.4)\n",
    "\n",
    "print(f\"Information Diffusion Simulation:\")\n",
    "print(f\"  Source: {top_influencer}\")\n",
    "print(f\"  Total Reached: {len(infected)} users ({len(infected)/G.number_of_nodes()*100:.1f}%)\")\n",
    "print(f\"  Number of Steps: {max(infection_time.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize diffusion over time\n",
    "time_steps = sorted(set(infection_time.values()))\n",
    "cumulative_infected = [sum(1 for t in infection_time.values() if t <= step) for step in time_steps]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(time_steps, cumulative_infected, marker='o', linewidth=2, markersize=8, color='darkred')\n",
    "ax.fill_between(time_steps, cumulative_infected, alpha=0.3, color='red')\n",
    "ax.set_title(f'Information Diffusion from Top Influencer ({top_influencer})', fontsize=14)\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Cumulative Users Reached')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save centrality analysis\n",
    "output_file = '../../results/network_centrality.csv'\n",
    "centrality_df.to_csv(output_file, index=False)\n",
    "print(f\"✓ Centrality analysis saved to {output_file}\")\n",
    "\n",
    "# Save community assignments\n",
    "community_file = '../../results/community_assignments.csv'\n",
    "centrality_df[['user_id', 'community', 'role']].to_csv(community_file, index=False)\n",
    "print(f\"✓ Community assignments saved to {community_file}\")\n",
    "\n",
    "# Save network summary\n",
    "network_summary = pd.DataFrame({\n",
    "    'metric': [\n",
    "        'nodes', 'edges', 'density', 'communities', \n",
    "        'avg_degree', 'avg_pagerank', 'avg_betweenness'\n",
    "    ],\n",
    "    'value': [\n",
    "        G.number_of_nodes(),\n",
    "        G.number_of_edges(),\n",
    "        nx.density(G),\n",
    "        n_communities,\n",
    "        np.mean(degrees),\n",
    "        centrality_df['pagerank'].mean(),\n",
    "        centrality_df['betweenness'].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary_file = '../../results/network_summary.csv'\n",
    "network_summary.to_csv(summary_file, index=False)\n",
    "print(f\"✓ Network summary saved to {summary_file}\")\n",
    "\n",
    "# Uncomment to save to S3\n",
    "# data_client = SocialMediaDataAccess()\n",
    "# data_client.save_results(centrality_df, 'network_centrality.csv')\n",
    "# data_client.save_results(network_summary, 'network_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Network Structure\n",
    "- **Nodes**: [X] users\n",
    "- **Edges**: [Y] interactions\n",
    "- **Density**: [Z] (0 = sparse, 1 = complete)\n",
    "- **Communities**: [N] detected\n",
    "\n",
    "### Top Influencers\n",
    "1. **User [ID]**: PageRank = [value], Role = [Influencer/Bridge/etc.]\n",
    "2. **User [ID]**: PageRank = [value], Role = [Influencer/Bridge/etc.]\n",
    "3. **User [ID]**: PageRank = [value], Role = [Influencer/Bridge/etc.]\n",
    "\n",
    "### Community Insights\n",
    "- Largest community: [size] users\n",
    "- Smallest community: [size] users\n",
    "- Communities show [high/medium/low] modularity\n",
    "\n",
    "### Information Spread\n",
    "- From top influencer, reached [X]% of network in [Y] steps\n",
    "- Average path length: [Z] hops\n",
    "- Network shows [small-world/scale-free] properties\n",
    "\n",
    "## Research Applications\n",
    "\n",
    "### Misinformation Campaigns\n",
    "- Identify coordinated amplification\n",
    "- Track narrative spread patterns\n",
    "- Find inauthentic coordination\n",
    "\n",
    "### Influence Operations\n",
    "- Map state-sponsored accounts\n",
    "- Detect bot networks\n",
    "- Analyze propaganda diffusion\n",
    "\n",
    "### Public Health\n",
    "- Track health misinformation\n",
    "- Identify trusted health communicators\n",
    "- Target intervention strategies\n",
    "\n",
    "### Political Polarization\n",
    "- Detect echo chambers\n",
    "- Measure cross-community interaction\n",
    "- Identify bridging accounts\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Monitor Influencers**: Track top 10 users for narrative changes\n",
    "2. **Watch Bridge Accounts**: High betweenness users may be coordinating\n",
    "3. **Community Engagement**: Tailor messaging to community characteristics\n",
    "4. **Early Detection**: Monitor influencers for misinformation first\n",
    "5. **Counter-Messaging**: Target bridge users to reach multiple communities\n",
    "\n",
    "## Further Analysis\n",
    "\n",
    "1. **Temporal Networks**: Track network evolution over time\n",
    "2. **Bot Detection**: Identify automated accounts by network behavior\n",
    "3. **Sentiment Analysis**: Overlay sentiment on network structure\n",
    "4. **Cross-Platform**: Analyze networks across Twitter, Reddit, Facebook\n",
    "5. **Predictive Modeling**: Forecast information cascade size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
