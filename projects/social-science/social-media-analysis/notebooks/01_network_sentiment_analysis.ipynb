{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": [
    "# Social Media Network and Sentiment Analysis\n",
    "\n",
    "## Research Overview\n",
    "\n",
    "This notebook demonstrates comprehensive social media analysis techniques combining network science and sentiment analysis. We explore:\n",
    "\n",
    "- **Network Structure**: Understanding social connections and community formation\n",
    "- **Sentiment Dynamics**: Analyzing emotional content in social media posts\n",
    "- **Influence Patterns**: Identifying key actors and information spreaders\n",
    "- **Information Diffusion**: Tracking how content spreads through networks\n",
    "- **Community Detection**: Discovering natural groupings in social networks\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "1. How does network structure influence information spread?\n",
    "2. What sentiment patterns emerge in different communities?\n",
    "3. Who are the most influential users and why?\n",
    "4. How do retweet cascades propagate through the network?\n",
    "5. What topics generate the most engagement?\n",
    "\n",
    "### Methodology\n",
    "\n",
    "We use synthetic data that mimics real social media platforms (similar to Twitter/X) to demonstrate analysis techniques that can be applied to real datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-cdef-012345678902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Network analysis\n",
    "# Utility\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Natural language processing\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from networkx.algorithms import community\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure visualization defaults\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-b8c9-0123-def0-123456789012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "print(\"NLTK resources ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-c9d0-1234-ef01-234567890123",
   "metadata": {},
   "source": [
    "## 2. Synthetic Social Media Data Generation\n",
    "\n",
    "We generate realistic synthetic social media data including:\n",
    "- User profiles with varying activity levels\n",
    "- Follower network with preferential attachment (power law distribution)\n",
    "- Posts with timestamps, content, and engagement metrics\n",
    "- Retweet cascades and information diffusion patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-d0e1-2345-f012-345678901234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "NUM_USERS = 1000\n",
    "NUM_POSTS = 5000\n",
    "NUM_EDGES = 8000  # Follower connections\n",
    "START_DATE = datetime(2024, 1, 1)\n",
    "END_DATE = datetime(2024, 6, 30)\n",
    "\n",
    "print(\"Generating synthetic social media dataset:\")\n",
    "print(f\"  - Users: {NUM_USERS:,}\")\n",
    "print(f\"  - Posts: {NUM_POSTS:,}\")\n",
    "print(f\"  - Follower connections: {NUM_EDGES:,}\")\n",
    "print(f\"  - Time period: {START_DATE.date()} to {END_DATE.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0-e1f2-3456-0123-456789012345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample content templates for realistic posts\n",
    "POST_TEMPLATES = [\n",
    "    # Positive sentiment\n",
    "    \"Just launched our new {topic}! So excited about this amazing opportunity! #{hashtag} #{sentiment}\",\n",
    "    \"Loving the {topic} community! You all are absolutely fantastic! #{hashtag}\",\n",
    "    \"Best day ever! Finally achieved {topic}! Thank you everyone! #{hashtag}\",\n",
    "    \"This {topic} is absolutely incredible! Highly recommend to everyone! #{hashtag}\",\n",
    "    \"Celebrating {topic} with the team! What a wonderful journey! #{hashtag} #{sentiment}\",\n",
    "    # Negative sentiment\n",
    "    \"Really disappointed with {topic}. This is terrible and frustrating. #{hashtag}\",\n",
    "    \"Can't believe how bad {topic} has become. Very unhappy about this. #{hashtag}\",\n",
    "    \"Worst experience with {topic}. Completely unacceptable! #{hashtag} #{sentiment}\",\n",
    "    \"Frustrated with {topic}. This needs to change immediately! #{hashtag}\",\n",
    "    \"Awful {topic} situation. Really upset about this whole thing. #{hashtag}\",\n",
    "    # Neutral/Informational\n",
    "    \"New research on {topic} published today. Check it out. #{hashtag}\",\n",
    "    \"Meeting about {topic} scheduled for next week. #{hashtag}\",\n",
    "    \"Sharing some thoughts on {topic}. Let me know what you think. #{hashtag}\",\n",
    "    \"Analyzing data from {topic}. Interesting patterns emerging. #{hashtag} #{sentiment}\",\n",
    "    \"Update on {topic}: progress continues as planned. #{hashtag}\",\n",
    "    # Questions/Engagement\n",
    "    \"What do you all think about {topic}? Would love to hear opinions! #{hashtag}\",\n",
    "    \"Anyone else working on {topic}? Let's connect! #{hashtag}\",\n",
    "    \"How do you approach {topic}? Looking for advice. #{hashtag} #{sentiment}\",\n",
    "]\n",
    "\n",
    "TOPICS = [\n",
    "    \"AI research\",\n",
    "    \"climate change\",\n",
    "    \"technology\",\n",
    "    \"innovation\",\n",
    "    \"data science\",\n",
    "    \"machine learning\",\n",
    "    \"social media\",\n",
    "    \"education\",\n",
    "    \"healthcare\",\n",
    "    \"sustainability\",\n",
    "    \"entrepreneurship\",\n",
    "    \"leadership\",\n",
    "    \"digital transformation\",\n",
    "    \"remote work\",\n",
    "    \"mental health\",\n",
    "    \"diversity\",\n",
    "    \"politics\",\n",
    "    \"economy\",\n",
    "    \"sports\",\n",
    "    \"entertainment\",\n",
    "]\n",
    "\n",
    "HASHTAGS = [\n",
    "    \"tech\",\n",
    "    \"innovation\",\n",
    "    \"research\",\n",
    "    \"science\",\n",
    "    \"data\",\n",
    "    \"AI\",\n",
    "    \"ML\",\n",
    "    \"future\",\n",
    "    \"digital\",\n",
    "    \"community\",\n",
    "    \"impact\",\n",
    "    \"change\",\n",
    "    \"growth\",\n",
    "    \"learning\",\n",
    "    \"news\",\n",
    "]\n",
    "\n",
    "SENTIMENT_TAGS = [\"happy\", \"excited\", \"grateful\", \"concerned\", \"thoughtful\", \"motivated\"]\n",
    "\n",
    "print(f\"Content generation configured with {len(POST_TEMPLATES)} templates\")\n",
    "print(f\"Topics: {len(TOPICS)} categories\")\n",
    "print(f\"Hashtags: {len(HASHTAGS)} options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1-f2a3-4567-1234-567890123456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_users(n_users: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic user profiles with realistic attributes.\n",
    "\n",
    "    Args:\n",
    "        n_users: Number of users to generate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with user profiles\n",
    "    \"\"\"\n",
    "    users = []\n",
    "\n",
    "    for i in range(n_users):\n",
    "        # User activity follows power law (most users post rarely, few post frequently)\n",
    "        activity_level = np.random.power(0.5)  # Skewed toward lower values\n",
    "\n",
    "        user = {\n",
    "            \"user_id\": f\"user_{i:04d}\",\n",
    "            \"username\": f\"@user{i:04d}\",\n",
    "            \"activity_level\": activity_level,\n",
    "            \"influence_score\": 0,  # Will be calculated later\n",
    "            \"account_created\": START_DATE - timedelta(days=np.random.randint(30, 365)),\n",
    "        }\n",
    "        users.append(user)\n",
    "\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "\n",
    "# Generate users\n",
    "users_df = generate_users(NUM_USERS)\n",
    "\n",
    "print(f\"Generated {len(users_df):,} users\")\n",
    "print(\"\\nSample users:\")\n",
    "print(users_df.head())\n",
    "print(\"\\nActivity level distribution:\")\n",
    "print(users_df[\"activity_level\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-2345-678901234567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_social_network(users_df: pd.DataFrame, n_edges: int) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Generate a social network with preferential attachment (Barabasi-Albert model).\n",
    "    Higher activity users are more likely to have followers.\n",
    "\n",
    "    Args:\n",
    "        users_df: DataFrame of users\n",
    "        n_edges: Target number of follower connections\n",
    "\n",
    "    Returns:\n",
    "        Directed graph where edges represent follower relationships\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add all users as nodes\n",
    "    for _, user in users_df.iterrows():\n",
    "        G.add_node(user[\"user_id\"], username=user[\"username\"], activity=user[\"activity_level\"])\n",
    "\n",
    "    # Generate edges with preferential attachment\n",
    "    # Users with higher activity are more likely to be followed\n",
    "    user_ids = users_df[\"user_id\"].tolist()\n",
    "    activity_weights = users_df[\"activity_level\"].values\n",
    "    activity_weights = activity_weights / activity_weights.sum()  # Normalize\n",
    "\n",
    "    edges_added = 0\n",
    "    max_attempts = n_edges * 3  # Prevent infinite loop\n",
    "    attempts = 0\n",
    "\n",
    "    while edges_added < n_edges and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # Select follower (random)\n",
    "        follower = np.random.choice(user_ids)\n",
    "\n",
    "        # Select target to follow (weighted by activity)\n",
    "        followee = np.random.choice(user_ids, p=activity_weights)\n",
    "\n",
    "        # Avoid self-loops and duplicate edges\n",
    "        if follower != followee and not G.has_edge(follower, followee):\n",
    "            G.add_edge(follower, followee, weight=1.0)\n",
    "            edges_added += 1\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# Generate social network\n",
    "print(\"Generating social network with preferential attachment...\")\n",
    "social_network = generate_social_network(users_df, NUM_EDGES)\n",
    "\n",
    "print(\"\\nNetwork statistics:\")\n",
    "print(f\"  Nodes (users): {social_network.number_of_nodes():,}\")\n",
    "print(f\"  Edges (follows): {social_network.number_of_edges():,}\")\n",
    "print(f\"  Density: {nx.density(social_network):.4f}\")\n",
    "print(\n",
    "    f\"  Average degree: {sum(dict(social_network.degree()).values()) / social_network.number_of_nodes():.2f}\"\n",
    ")\n",
    "\n",
    "# Check if network is connected\n",
    "if nx.is_weakly_connected(social_network):\n",
    "    print(\"  Network is weakly connected\")\n",
    "else:\n",
    "    n_components = nx.number_weakly_connected_components(social_network)\n",
    "    print(f\"  Network has {n_components} weakly connected components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-b4c5-6789-3456-789012345678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(\n",
    "    users_df: pd.DataFrame, n_posts: int, start_date: datetime, end_date: datetime\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic social media posts with realistic content and engagement.\n",
    "\n",
    "    Args:\n",
    "        users_df: DataFrame of users\n",
    "        n_posts: Number of posts to generate\n",
    "        start_date: Start of time period\n",
    "        end_date: End of time period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of posts\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    time_delta = (end_date - start_date).total_seconds()\n",
    "\n",
    "    # Sample users weighted by activity level for post creation\n",
    "    user_weights = users_df[\"activity_level\"].values\n",
    "    user_weights = user_weights / user_weights.sum()\n",
    "\n",
    "    for i in range(n_posts):\n",
    "        # Select user weighted by activity\n",
    "        user = users_df.sample(n=1, weights=user_weights).iloc[0]\n",
    "\n",
    "        # Generate timestamp\n",
    "        timestamp = start_date + timedelta(seconds=np.random.uniform(0, time_delta))\n",
    "\n",
    "        # Generate content\n",
    "        template = np.random.choice(POST_TEMPLATES)\n",
    "        topic = np.random.choice(TOPICS)\n",
    "        hashtag = np.random.choice(HASHTAGS)\n",
    "        sentiment_tag = np.random.choice(SENTIMENT_TAGS)\n",
    "\n",
    "        content = template.format(topic=topic, hashtag=hashtag, sentiment=sentiment_tag)\n",
    "\n",
    "        # Generate engagement metrics (influenced by user activity)\n",
    "        base_engagement = user[\"activity_level\"] * 100\n",
    "        likes = int(np.random.poisson(base_engagement * 0.5))\n",
    "        retweets = int(np.random.poisson(base_engagement * 0.1))\n",
    "        replies = int(np.random.poisson(base_engagement * 0.05))\n",
    "\n",
    "        # Some posts are retweets\n",
    "        is_retweet = np.random.random() < 0.3\n",
    "        original_post_id = (\n",
    "            f\"post_{np.random.randint(0, max(i, 1)):04d}\" if is_retweet and i > 0 else None\n",
    "        )\n",
    "\n",
    "        post = {\n",
    "            \"post_id\": f\"post_{i:04d}\",\n",
    "            \"user_id\": user[\"user_id\"],\n",
    "            \"username\": user[\"username\"],\n",
    "            \"timestamp\": timestamp,\n",
    "            \"content\": content,\n",
    "            \"likes\": likes,\n",
    "            \"retweets\": retweets,\n",
    "            \"replies\": replies,\n",
    "            \"is_retweet\": is_retweet,\n",
    "            \"original_post_id\": original_post_id,\n",
    "            \"hashtags\": [tag for tag in content.split() if tag.startswith(\"#\")],\n",
    "        }\n",
    "        posts.append(post)\n",
    "\n",
    "    posts_df = pd.DataFrame(posts)\n",
    "    posts_df = posts_df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    return posts_df\n",
    "\n",
    "\n",
    "# Generate posts\n",
    "print(\"Generating social media posts...\")\n",
    "posts_df = generate_posts(users_df, NUM_POSTS, START_DATE, END_DATE)\n",
    "\n",
    "print(f\"\\nGenerated {len(posts_df):,} posts\")\n",
    "print(f\"Time range: {posts_df['timestamp'].min()} to {posts_df['timestamp'].max()}\")\n",
    "print(\n",
    "    f\"Retweets: {posts_df['is_retweet'].sum():,} ({posts_df['is_retweet'].sum() / len(posts_df) * 100:.1f}%)\"\n",
    ")\n",
    "print(\"\\nSample posts:\")\n",
    "print(posts_df[[\"post_id\", \"username\", \"content\", \"likes\", \"retweets\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4-c5d6-7890-4567-890123456789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total engagement score for each post\n",
    "posts_df[\"engagement_score\"] = (\n",
    "    posts_df[\"likes\"] * 1.0 + posts_df[\"retweets\"] * 2.0 + posts_df[\"replies\"] * 1.5\n",
    ")\n",
    "\n",
    "# Add date column for temporal analysis\n",
    "posts_df[\"date\"] = posts_df[\"timestamp\"].dt.date\n",
    "\n",
    "print(\"Enhanced posts DataFrame with engagement metrics\")\n",
    "print(\"\\nEngagement statistics:\")\n",
    "print(posts_df[[\"likes\", \"retweets\", \"replies\", \"engagement_score\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-d6e7-8901-5678-901234567890",
   "metadata": {},
   "source": [
    "## 3. Network Analysis\n",
    "\n",
    "We analyze the social network structure to understand:\n",
    "- Centrality measures (who is most connected/influential)\n",
    "- Network topology and patterns\n",
    "- Key players in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6-e7f8-9012-6789-012345678901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centrality_measures(G: nx.DiGraph) -> dict[str, dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculate various centrality measures for network nodes.\n",
    "\n",
    "    Args:\n",
    "        G: Social network graph\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of centrality measures\n",
    "    \"\"\"\n",
    "    print(\"Calculating centrality measures...\")\n",
    "\n",
    "    centrality = {}\n",
    "\n",
    "    # Degree centrality (in-degree = followers, out-degree = following)\n",
    "    print(\"  - Degree centrality\")\n",
    "    centrality[\"in_degree\"] = dict(G.in_degree())\n",
    "    centrality[\"out_degree\"] = dict(G.out_degree())\n",
    "\n",
    "    # Betweenness centrality (bridging different parts of network)\n",
    "    print(\"  - Betweenness centrality\")\n",
    "    centrality[\"betweenness\"] = nx.betweenness_centrality(G, k=min(100, G.number_of_nodes()))\n",
    "\n",
    "    # Eigenvector centrality (connected to well-connected nodes)\n",
    "    print(\"  - Eigenvector centrality\")\n",
    "    try:\n",
    "        centrality[\"eigenvector\"] = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    except:\n",
    "        # If eigenvector doesn't converge, use PageRank as alternative\n",
    "        print(\"  - Using PageRank instead (eigenvector did not converge)\")\n",
    "        centrality[\"eigenvector\"] = nx.pagerank(G)\n",
    "\n",
    "    # PageRank (Google's algorithm for importance)\n",
    "    print(\"  - PageRank\")\n",
    "    centrality[\"pagerank\"] = nx.pagerank(G)\n",
    "\n",
    "    return centrality\n",
    "\n",
    "\n",
    "# Calculate centrality measures\n",
    "centrality_measures = calculate_centrality_measures(social_network)\n",
    "\n",
    "# Add centrality to users dataframe\n",
    "users_df[\"in_degree\"] = users_df[\"user_id\"].map(centrality_measures[\"in_degree\"])\n",
    "users_df[\"out_degree\"] = users_df[\"user_id\"].map(centrality_measures[\"out_degree\"])\n",
    "users_df[\"betweenness\"] = users_df[\"user_id\"].map(centrality_measures[\"betweenness\"])\n",
    "users_df[\"eigenvector\"] = users_df[\"user_id\"].map(centrality_measures[\"eigenvector\"])\n",
    "users_df[\"pagerank\"] = users_df[\"user_id\"].map(centrality_measures[\"pagerank\"])\n",
    "\n",
    "# Calculate composite influence score\n",
    "users_df[\"influence_score\"] = (\n",
    "    users_df[\"in_degree\"] * 0.3\n",
    "    + users_df[\"pagerank\"] * 100 * 0.3\n",
    "    + users_df[\"betweenness\"] * 100 * 0.2\n",
    "    + users_df[\"eigenvector\"] * 100 * 0.2\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 users by influence score:\")\n",
    "print(\n",
    "    users_df.nlargest(10, \"influence_score\")[\n",
    "        [\"username\", \"in_degree\", \"out_degree\", \"pagerank\", \"influence_score\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7-f8a9-0123-7890-123456789012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degree distribution (power law)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# In-degree distribution (followers)\n",
    "in_degrees = [d for n, d in social_network.in_degree()]\n",
    "axes[0, 0].hist(in_degrees, bins=50, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0, 0].set_xlabel(\"In-Degree (Followers)\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "axes[0, 0].set_title(\"In-Degree Distribution\")\n",
    "axes[0, 0].axvline(\n",
    "    np.mean(in_degrees), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(in_degrees):.1f}\"\n",
    ")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Out-degree distribution (following)\n",
    "out_degrees = [d for n, d in social_network.out_degree()]\n",
    "axes[0, 1].hist(out_degrees, bins=50, color=\"lightcoral\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0, 1].set_xlabel(\"Out-Degree (Following)\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "axes[0, 1].set_title(\"Out-Degree Distribution\")\n",
    "axes[0, 1].axvline(\n",
    "    np.mean(out_degrees), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(out_degrees):.1f}\"\n",
    ")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# PageRank distribution\n",
    "pageranks = list(centrality_measures[\"pagerank\"].values())\n",
    "axes[1, 0].hist(pageranks, bins=50, color=\"lightgreen\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1, 0].set_xlabel(\"PageRank Score\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].set_title(\"PageRank Distribution\")\n",
    "axes[1, 0].axvline(\n",
    "    np.mean(pageranks), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(pageranks):.6f}\"\n",
    ")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness = list(centrality_measures[\"betweenness\"].values())\n",
    "axes[1, 1].hist(betweenness, bins=50, color=\"plum\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1, 1].set_xlabel(\"Betweenness Centrality\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "axes[1, 1].set_title(\"Betweenness Centrality Distribution\")\n",
    "axes[1, 1].axvline(\n",
    "    np.mean(betweenness), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(betweenness):.6f}\"\n",
    ")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"network_centrality_distributions.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCentrality measures follow power law distribution\")\n",
    "print(\"Most users have low centrality, few users have very high centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8-a9b0-1234-8901-234567890123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network structure (sample for performance)\n",
    "# For large networks, we visualize a subgraph of most influential nodes\n",
    "\n",
    "# Get top 100 most influential nodes\n",
    "top_users = users_df.nlargest(100, \"influence_score\")[\"user_id\"].tolist()\n",
    "subgraph = social_network.subgraph(top_users).copy()\n",
    "\n",
    "print(\"Visualizing subgraph of top 100 influential users\")\n",
    "print(f\"Subgraph: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "\n",
    "# Calculate layout\n",
    "pos = nx.spring_layout(subgraph, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Node sizes based on in-degree (followers)\n",
    "node_sizes = [subgraph.in_degree(node) * 20 + 50 for node in subgraph.nodes()]\n",
    "\n",
    "# Node colors based on PageRank\n",
    "node_colors = [centrality_measures[\"pagerank\"][node] for node in subgraph.nodes()]\n",
    "\n",
    "# Draw network\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph, pos, alpha=0.2, edge_color=\"gray\", arrows=True, arrowsize=10, width=0.5, ax=ax\n",
    ")\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    subgraph, pos, node_size=node_sizes, node_color=node_colors, cmap=\"YlOrRd\", alpha=0.8, ax=ax\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(nodes, label=\"PageRank\", ax=ax)\n",
    "\n",
    "# Add labels for top 10 nodes\n",
    "top_10_users = users_df.nlargest(10, \"influence_score\")[\"user_id\"].tolist()\n",
    "labels = {\n",
    "    node: social_network.nodes[node][\"username\"]\n",
    "    for node in subgraph.nodes()\n",
    "    if node in top_10_users\n",
    "}\n",
    "nx.draw_networkx_labels(subgraph, pos, labels, font_size=8, ax=ax)\n",
    "\n",
    "ax.set_title(\"Social Network Structure (Top 100 Influential Users)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"social_network_visualization.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNetwork visualization complete\")\n",
    "print(\"Node size = number of followers\")\n",
    "print(\"Node color = PageRank score (darker = more influential)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2345-9012-345678901234",
   "metadata": {},
   "source": [
    "## 4. Community Detection\n",
    "\n",
    "We use the Louvain algorithm to detect communities (clusters) in the social network. Communities represent groups of users who are more densely connected to each other than to the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0-c1d2-3456-0123-456789012345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_communities(G: nx.DiGraph) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Detect communities using Louvain algorithm.\n",
    "\n",
    "    Args:\n",
    "        G: Social network graph\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping user_id to community_id\n",
    "    \"\"\"\n",
    "    print(\"Detecting communities using Louvain algorithm...\")\n",
    "\n",
    "    # Convert to undirected for community detection\n",
    "    G_undirected = G.to_undirected()\n",
    "\n",
    "    # Apply Louvain community detection\n",
    "    communities_generator = community.louvain_communities(G_undirected, seed=42)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    user_to_community = {}\n",
    "    for community_id, community_nodes in enumerate(communities_generator):\n",
    "        for node in community_nodes:\n",
    "            user_to_community[node] = community_id\n",
    "\n",
    "    return user_to_community\n",
    "\n",
    "\n",
    "# Detect communities\n",
    "user_communities = detect_communities(social_network)\n",
    "\n",
    "# Add to users dataframe\n",
    "users_df[\"community\"] = users_df[\"user_id\"].map(user_communities)\n",
    "\n",
    "# Community statistics\n",
    "community_sizes = users_df[\"community\"].value_counts().sort_index()\n",
    "print(f\"\\nDetected {len(community_sizes)} communities\")\n",
    "print(\"\\nCommunity sizes:\")\n",
    "print(community_sizes.describe())\n",
    "print(\"\\nTop 10 largest communities:\")\n",
    "print(community_sizes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1-d2e3-4567-1234-567890123456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize communities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Community size distribution\n",
    "axes[0].bar(\n",
    "    range(len(community_sizes)),\n",
    "    community_sizes.values,\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Community ID\")\n",
    "axes[0].set_ylabel(\"Number of Users\")\n",
    "axes[0].set_title(\"Community Size Distribution\")\n",
    "axes[0].axhline(\n",
    "    community_sizes.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {community_sizes.mean():.1f}\"\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "# Visualize network colored by community (top 100 users)\n",
    "top_users = users_df.nlargest(100, \"influence_score\")[\"user_id\"].tolist()\n",
    "subgraph = social_network.subgraph(top_users).copy()\n",
    "\n",
    "pos = nx.spring_layout(subgraph, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Color by community\n",
    "node_colors = [user_communities[node] for node in subgraph.nodes()]\n",
    "node_sizes = [subgraph.in_degree(node) * 20 + 50 for node in subgraph.nodes()]\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph, pos, alpha=0.2, edge_color=\"gray\", arrows=True, arrowsize=10, width=0.5, ax=axes[1]\n",
    ")\n",
    "nx.draw_networkx_nodes(\n",
    "    subgraph, pos, node_size=node_sizes, node_color=node_colors, cmap=\"tab20\", alpha=0.8, ax=axes[1]\n",
    ")\n",
    "\n",
    "axes[1].set_title(\"Network Communities (Top 100 Users)\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"community_detection.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Community detection complete\")\n",
    "print(\"Each color represents a different community in the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2-e3f4-5678-2345-678901234567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate modularity (quality of community detection)\n",
    "G_undirected = social_network.to_undirected()\n",
    "communities_list = []\n",
    "for comm_id in sorted(set(user_communities.values())):\n",
    "    comm_nodes = [node for node, c_id in user_communities.items() if c_id == comm_id]\n",
    "    communities_list.append(set(comm_nodes))\n",
    "\n",
    "modularity_score = community.modularity(G_undirected, communities_list)\n",
    "print(f\"\\nModularity score: {modularity_score:.4f}\")\n",
    "print(\"(Values closer to 1 indicate stronger community structure)\")\n",
    "\n",
    "if modularity_score > 0.3:\n",
    "    print(\"Strong community structure detected\")\n",
    "elif modularity_score > 0.1:\n",
    "    print(\"Moderate community structure detected\")\n",
    "else:\n",
    "    print(\"Weak community structure detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3-f4a5-6789-3456-789012345678",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis\n",
    "\n",
    "We analyze the sentiment of posts using VADER (Valence Aware Dictionary and sEntiment Reasoner), which is specifically designed for social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4-a5b6-7890-4567-890123456789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(texts: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze sentiment of texts using VADER.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text strings to analyze\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with sentiment scores\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_scores = []\n",
    "    for text in texts:\n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        sentiment_scores.append(scores)\n",
    "\n",
    "    return pd.DataFrame(sentiment_scores)\n",
    "\n",
    "\n",
    "# Analyze sentiment of all posts\n",
    "print(\"Analyzing sentiment of all posts using VADER...\")\n",
    "sentiment_df = analyze_sentiment(posts_df[\"content\"].tolist())\n",
    "\n",
    "# Add sentiment scores to posts\n",
    "posts_df[\"sentiment_neg\"] = sentiment_df[\"neg\"]\n",
    "posts_df[\"sentiment_neu\"] = sentiment_df[\"neu\"]\n",
    "posts_df[\"sentiment_pos\"] = sentiment_df[\"pos\"]\n",
    "posts_df[\"sentiment_compound\"] = sentiment_df[\"compound\"]\n",
    "\n",
    "\n",
    "# Categorize sentiment\n",
    "def categorize_sentiment(compound: float) -> str:\n",
    "    if compound >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "posts_df[\"sentiment_category\"] = posts_df[\"sentiment_compound\"].apply(categorize_sentiment)\n",
    "\n",
    "print(f\"\\nSentiment analysis complete for {len(posts_df):,} posts\")\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(posts_df[\"sentiment_category\"].value_counts())\n",
    "print(\"\\nSentiment score statistics:\")\n",
    "print(posts_df[\"sentiment_compound\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5-b6c7-8901-5678-901234567890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sentiment category distribution\n",
    "sentiment_counts = posts_df[\"sentiment_category\"].value_counts()\n",
    "colors = {\"positive\": \"lightgreen\", \"neutral\": \"lightblue\", \"negative\": \"lightcoral\"}\n",
    "sentiment_colors = [colors[cat] for cat in sentiment_counts.index]\n",
    "\n",
    "axes[0, 0].bar(\n",
    "    sentiment_counts.index,\n",
    "    sentiment_counts.values,\n",
    "    color=sentiment_colors,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Sentiment Category\")\n",
    "axes[0, 0].set_ylabel(\"Number of Posts\")\n",
    "axes[0, 0].set_title(\"Sentiment Category Distribution\")\n",
    "for i, (_cat, count) in enumerate(sentiment_counts.items()):\n",
    "    axes[0, 0].text(\n",
    "        i, count, f\"{count}\\n({count / len(posts_df) * 100:.1f}%)\", ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "# Compound sentiment score distribution\n",
    "axes[0, 1].hist(\n",
    "    posts_df[\"sentiment_compound\"], bins=50, color=\"mediumpurple\", edgecolor=\"black\", alpha=0.7\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Compound Sentiment Score\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "axes[0, 1].set_title(\"Sentiment Score Distribution\")\n",
    "axes[0, 1].axvline(\n",
    "    posts_df[\"sentiment_compound\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {posts_df['sentiment_compound'].mean():.3f}\",\n",
    ")\n",
    "axes[0, 1].axvline(0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Sentiment over time\n",
    "daily_sentiment = posts_df.groupby(\"date\")[\"sentiment_compound\"].mean()\n",
    "axes[1, 0].plot(\n",
    "    daily_sentiment.index, daily_sentiment.values, color=\"steelblue\", linewidth=2, alpha=0.7\n",
    ")\n",
    "axes[1, 0].axhline(0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[1, 0].set_xlabel(\"Date\")\n",
    "axes[1, 0].set_ylabel(\"Average Sentiment Score\")\n",
    "axes[1, 0].set_title(\"Sentiment Trends Over Time\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Sentiment by engagement\n",
    "bins = pd.qcut(\n",
    "    posts_df[\"engagement_score\"], q=5, labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    ")\n",
    "sentiment_by_engagement = posts_df.groupby(bins)[\"sentiment_compound\"].mean()\n",
    "\n",
    "axes[1, 1].bar(\n",
    "    range(len(sentiment_by_engagement)),\n",
    "    sentiment_by_engagement.values,\n",
    "    color=\"coral\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[1, 1].set_xticks(range(len(sentiment_by_engagement)))\n",
    "axes[1, 1].set_xticklabels(sentiment_by_engagement.index, rotation=45)\n",
    "axes[1, 1].set_xlabel(\"Engagement Level\")\n",
    "axes[1, 1].set_ylabel(\"Average Sentiment Score\")\n",
    "axes[1, 1].set_title(\"Sentiment by Engagement Level\")\n",
    "axes[1, 1].axhline(0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sentiment_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSentiment analysis visualizations complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6-c7d8-9012-6789-012345678901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment by community\n",
    "posts_with_community = posts_df.merge(users_df[[\"user_id\", \"community\"]], on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Analyze top 10 communities\n",
    "top_communities = users_df[\"community\"].value_counts().head(10).index\n",
    "community_sentiment = (\n",
    "    posts_with_community[posts_with_community[\"community\"].isin(top_communities)]\n",
    "    .groupby(\"community\")[\"sentiment_compound\"]\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "print(\"\\nSentiment by community (top 10 communities):\")\n",
    "print(community_sentiment.round(3))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(community_sentiment))\n",
    "ax.bar(\n",
    "    x,\n",
    "    community_sentiment[\"mean\"],\n",
    "    yerr=community_sentiment[\"std\"],\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"Community {i}\" for i in community_sentiment.index])\n",
    "ax.set_xlabel(\"Community\")\n",
    "ax.set_ylabel(\"Average Sentiment Score\")\n",
    "ax.set_title(\"Sentiment Distribution Across Communities\")\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sentiment_by_community.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDifferent communities show distinct sentiment patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7-d8e9-0123-7890-123456789012",
   "metadata": {},
   "source": [
    "## 6. Influence Analysis\n",
    "\n",
    "We identify influential users based on network centrality and content engagement metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8-e9f0-1234-8901-234567890123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user-level post statistics\n",
    "user_post_stats = (\n",
    "    posts_df.groupby(\"user_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"post_id\": \"count\",\n",
    "            \"likes\": \"sum\",\n",
    "            \"retweets\": \"sum\",\n",
    "            \"replies\": \"sum\",\n",
    "            \"engagement_score\": [\"sum\", \"mean\"],\n",
    "            \"sentiment_compound\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_post_stats.columns = [\n",
    "    \"user_id\",\n",
    "    \"num_posts\",\n",
    "    \"total_likes\",\n",
    "    \"total_retweets\",\n",
    "    \"total_replies\",\n",
    "    \"total_engagement\",\n",
    "    \"avg_engagement\",\n",
    "    \"avg_sentiment\",\n",
    "]\n",
    "\n",
    "# Merge with user data\n",
    "users_enriched = users_df.merge(user_post_stats, on=\"user_id\", how=\"left\")\n",
    "users_enriched = users_enriched.fillna(0)\n",
    "\n",
    "# Calculate comprehensive influence metric\n",
    "users_enriched[\"content_influence\"] = (\n",
    "    users_enriched[\"total_engagement\"] * 0.4\n",
    "    + users_enriched[\"avg_engagement\"] * 0.3\n",
    "    + users_enriched[\"num_posts\"] * 0.3\n",
    ")\n",
    "\n",
    "users_enriched[\"overall_influence\"] = (\n",
    "    users_enriched[\"influence_score\"] * 0.5 + users_enriched[\"content_influence\"] * 0.5\n",
    ")\n",
    "\n",
    "# Identify top influencers\n",
    "top_influencers = users_enriched.nlargest(20, \"overall_influence\")\n",
    "\n",
    "print(\"Top 20 Most Influential Users:\")\n",
    "print(\"=\" * 100)\n",
    "print(\n",
    "    top_influencers[\n",
    "        [\n",
    "            \"username\",\n",
    "            \"in_degree\",\n",
    "            \"pagerank\",\n",
    "            \"num_posts\",\n",
    "            \"total_engagement\",\n",
    "            \"avg_sentiment\",\n",
    "            \"overall_influence\",\n",
    "        ]\n",
    "    ].to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9-f0a1-2345-9012-345678901234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize influence factors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Network influence vs Content influence\n",
    "axes[0, 0].scatter(\n",
    "    users_enriched[\"influence_score\"],\n",
    "    users_enriched[\"content_influence\"],\n",
    "    alpha=0.5,\n",
    "    s=50,\n",
    "    c=\"steelblue\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Network Influence Score\")\n",
    "axes[0, 0].set_ylabel(\"Content Influence Score\")\n",
    "axes[0, 0].set_title(\"Network vs Content Influence\")\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Followers vs Engagement\n",
    "axes[0, 1].scatter(\n",
    "    users_enriched[\"in_degree\"],\n",
    "    users_enriched[\"total_engagement\"],\n",
    "    alpha=0.5,\n",
    "    s=50,\n",
    "    c=\"coral\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Number of Followers\")\n",
    "axes[0, 1].set_ylabel(\"Total Engagement\")\n",
    "axes[0, 1].set_title(\"Followers vs Engagement\")\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Top influencers by category\n",
    "top_20 = users_enriched.nlargest(20, \"overall_influence\")\n",
    "axes[1, 0].barh(\n",
    "    range(len(top_20)),\n",
    "    top_20[\"overall_influence\"].values,\n",
    "    color=\"mediumseagreen\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1, 0].set_yticks(range(len(top_20)))\n",
    "axes[1, 0].set_yticklabels(top_20[\"username\"].values, fontsize=8)\n",
    "axes[1, 0].set_xlabel(\"Overall Influence Score\")\n",
    "axes[1, 0].set_title(\"Top 20 Most Influential Users\")\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Influence distribution\n",
    "axes[1, 1].hist(\n",
    "    users_enriched[\"overall_influence\"], bins=50, color=\"plum\", alpha=0.7, edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Overall Influence Score\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "axes[1, 1].set_title(\"Influence Score Distribution\")\n",
    "axes[1, 1].axvline(\n",
    "    users_enriched[\"overall_influence\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {users_enriched['overall_influence'].mean():.1f}\",\n",
    ")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"influence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInfluence analysis visualizations complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0-a1b2-3456-0123-456789012345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze engagement patterns\n",
    "print(\"\\nEngagement Patterns Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Post frequency vs engagement\n",
    "posting_bins = pd.qcut(\n",
    "    users_enriched[users_enriched[\"num_posts\"] > 0][\"num_posts\"],\n",
    "    q=4,\n",
    "    labels=[\"Low\", \"Medium\", \"High\", \"Very High\"],\n",
    ")\n",
    "posting_engagement = (\n",
    "    users_enriched[users_enriched[\"num_posts\"] > 0].groupby(posting_bins)[\"avg_engagement\"].mean()\n",
    ")\n",
    "\n",
    "print(\"\\nAverage engagement by posting frequency:\")\n",
    "print(posting_engagement)\n",
    "\n",
    "# Sentiment impact on engagement\n",
    "sentiment_engagement = posts_df.groupby(\"sentiment_category\")[\"engagement_score\"].mean()\n",
    "print(\"\\nAverage engagement by sentiment:\")\n",
    "print(sentiment_engagement)\n",
    "\n",
    "# Peak posting times\n",
    "posts_df[\"hour\"] = posts_df[\"timestamp\"].dt.hour\n",
    "posts_df[\"day_of_week\"] = posts_df[\"timestamp\"].dt.day_name()\n",
    "\n",
    "hourly_engagement = posts_df.groupby(\"hour\")[\"engagement_score\"].mean()\n",
    "print(\"\\nPeak engagement hours:\")\n",
    "print(hourly_engagement.nlargest(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1-b2c3-4567-1234-567890123456",
   "metadata": {},
   "source": [
    "## 7. Information Diffusion Analysis\n",
    "\n",
    "We analyze how information spreads through the network via retweets and identify viral content patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2-c3d4-5678-2345-678901234567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retweet_cascades(posts_df: pd.DataFrame) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Build retweet cascades showing how content spreads.\n",
    "\n",
    "    Args:\n",
    "        posts_df: DataFrame of posts\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping original post to list of retweet post IDs\n",
    "    \"\"\"\n",
    "    cascades = defaultdict(list)\n",
    "\n",
    "    for _, post in posts_df.iterrows():\n",
    "        if post[\"is_retweet\"] and post[\"original_post_id\"]:\n",
    "            cascades[post[\"original_post_id\"]].append(post[\"post_id\"])\n",
    "\n",
    "    return dict(cascades)\n",
    "\n",
    "\n",
    "# Build cascades\n",
    "retweet_cascades = build_retweet_cascades(posts_df)\n",
    "\n",
    "# Calculate cascade statistics\n",
    "cascade_sizes = {post_id: len(retweets) for post_id, retweets in retweet_cascades.items()}\n",
    "posts_df[\"cascade_size\"] = posts_df[\"post_id\"].map(cascade_sizes).fillna(0)\n",
    "\n",
    "print(f\"Total retweet cascades: {len(retweet_cascades):,}\")\n",
    "print(\"\\nCascade size statistics:\")\n",
    "cascade_sizes_series = pd.Series(list(cascade_sizes.values()))\n",
    "print(cascade_sizes_series.describe())\n",
    "\n",
    "# Identify viral content (top 20 by cascade size)\n",
    "viral_posts = posts_df.nlargest(20, \"cascade_size\")\n",
    "print(\"\\nTop 10 Most Viral Posts (by retweets):\")\n",
    "print(\"=\" * 100)\n",
    "print(\n",
    "    viral_posts[\n",
    "        [\"post_id\", \"username\", \"content\", \"retweets\", \"cascade_size\", \"sentiment_compound\"]\n",
    "    ]\n",
    "    .head(10)\n",
    "    .to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3-d4e5-6789-3456-789012345678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize information diffusion\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cascade size distribution\n",
    "cascade_sizes_list = [size for size in cascade_sizes.values() if size > 0]\n",
    "axes[0, 0].hist(cascade_sizes_list, bins=50, color=\"lightcoral\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0, 0].set_xlabel(\"Cascade Size (Number of Retweets)\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "axes[0, 0].set_title(\"Retweet Cascade Size Distribution\")\n",
    "axes[0, 0].axvline(\n",
    "    np.mean(cascade_sizes_list),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {np.mean(cascade_sizes_list):.1f}\",\n",
    ")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Virality over time\n",
    "daily_virality = posts_df.groupby(\"date\")[\"cascade_size\"].sum()\n",
    "axes[0, 1].plot(\n",
    "    daily_virality.index,\n",
    "    daily_virality.values,\n",
    "    color=\"green\",\n",
    "    linewidth=2,\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Date\")\n",
    "axes[0, 1].set_ylabel(\"Total Cascade Size\")\n",
    "axes[0, 1].set_title(\"Viral Activity Over Time\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Sentiment of viral content\n",
    "viral_threshold = posts_df[\"cascade_size\"].quantile(0.9)\n",
    "viral_sentiment = posts_df[posts_df[\"cascade_size\"] >= viral_threshold][\n",
    "    \"sentiment_category\"\n",
    "].value_counts()\n",
    "axes[1, 0].bar(\n",
    "    viral_sentiment.index,\n",
    "    viral_sentiment.values,\n",
    "    color=[\n",
    "        \"lightgreen\" if x == \"positive\" else \"lightcoral\" if x == \"negative\" else \"lightblue\"\n",
    "        for x in viral_sentiment.index\n",
    "    ],\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"Sentiment Category\")\n",
    "axes[1, 0].set_ylabel(\"Number of Viral Posts\")\n",
    "axes[1, 0].set_title(\"Sentiment of Viral Content (Top 10%)\")\n",
    "for i, (_cat, count) in enumerate(viral_sentiment.items()):\n",
    "    axes[1, 0].text(\n",
    "        i, count, f\"{count}\\n({count / viral_sentiment.sum() * 100:.1f}%)\", ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "# Engagement vs Virality\n",
    "axes[1, 1].scatter(\n",
    "    posts_df[\"engagement_score\"],\n",
    "    posts_df[\"cascade_size\"],\n",
    "    alpha=0.4,\n",
    "    s=30,\n",
    "    c=\"purple\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Engagement Score\")\n",
    "axes[1, 1].set_ylabel(\"Cascade Size\")\n",
    "axes[1, 1].set_title(\"Engagement vs Virality\")\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"information_diffusion.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInformation diffusion analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-e5f6-7890-4567-890123456789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diffusion through network communities\n",
    "posts_with_community = posts_df.merge(users_df[[\"user_id\", \"community\"]], on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Viral content by community\n",
    "top_communities = users_df[\"community\"].value_counts().head(10).index\n",
    "community_virality = (\n",
    "    posts_with_community[posts_with_community[\"community\"].isin(top_communities)]\n",
    "    .groupby(\"community\")[\"cascade_size\"]\n",
    "    .agg([\"sum\", \"mean\", \"max\"])\n",
    ")\n",
    "\n",
    "print(\"\\nVirality by community (top 10 communities):\")\n",
    "print(community_virality.round(2))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(community_virality))\n",
    "ax.bar(x, community_virality[\"mean\"], color=\"orange\", alpha=0.7, edgecolor=\"black\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"Community {i}\" for i in community_virality.index])\n",
    "ax.set_xlabel(\"Community\")\n",
    "ax.set_ylabel(\"Average Cascade Size\")\n",
    "ax.set_title(\"Viral Activity by Community\")\n",
    "ax.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"virality_by_community.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-5678-901234567890",
   "metadata": {},
   "source": [
    "## 8. Topic and Hashtag Analysis\n",
    "\n",
    "We extract and analyze hashtags to understand trending topics and their engagement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-6789-012345678901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(posts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract and analyze hashtags from posts.\n",
    "\n",
    "    Args:\n",
    "        posts_df: DataFrame of posts\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of hashtag statistics\n",
    "    \"\"\"\n",
    "    # Flatten hashtag lists\n",
    "    all_hashtags = []\n",
    "    for _, post in posts_df.iterrows():\n",
    "        for hashtag in post[\"hashtags\"]:\n",
    "            all_hashtags.append(\n",
    "                {\n",
    "                    \"hashtag\": hashtag.lower(),\n",
    "                    \"post_id\": post[\"post_id\"],\n",
    "                    \"engagement_score\": post[\"engagement_score\"],\n",
    "                    \"sentiment\": post[\"sentiment_compound\"],\n",
    "                    \"cascade_size\": post[\"cascade_size\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    hashtags_df = pd.DataFrame(all_hashtags)\n",
    "\n",
    "    # Aggregate statistics by hashtag\n",
    "    hashtag_stats = (\n",
    "        hashtags_df.groupby(\"hashtag\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"post_id\": \"count\",\n",
    "                \"engagement_score\": [\"sum\", \"mean\"],\n",
    "                \"sentiment\": \"mean\",\n",
    "                \"cascade_size\": [\"sum\", \"mean\"],\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    hashtag_stats.columns = [\n",
    "        \"hashtag\",\n",
    "        \"frequency\",\n",
    "        \"total_engagement\",\n",
    "        \"avg_engagement\",\n",
    "        \"avg_sentiment\",\n",
    "        \"total_virality\",\n",
    "        \"avg_virality\",\n",
    "    ]\n",
    "\n",
    "    return hashtag_stats.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "\n",
    "# Extract hashtags\n",
    "print(\"Extracting and analyzing hashtags...\")\n",
    "hashtag_stats = extract_hashtags(posts_df)\n",
    "\n",
    "print(f\"\\nTotal unique hashtags: {len(hashtag_stats):,}\")\n",
    "print(\"\\nTop 20 Most Frequent Hashtags:\")\n",
    "print(\"=\" * 100)\n",
    "print(hashtag_stats.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-b8c9-0123-7890-123456789012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hashtag analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top hashtags by frequency\n",
    "top_20_hashtags = hashtag_stats.head(20)\n",
    "axes[0, 0].barh(\n",
    "    range(len(top_20_hashtags)),\n",
    "    top_20_hashtags[\"frequency\"].values,\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[0, 0].set_yticks(range(len(top_20_hashtags)))\n",
    "axes[0, 0].set_yticklabels(top_20_hashtags[\"hashtag\"].values, fontsize=8)\n",
    "axes[0, 0].set_xlabel(\"Frequency\")\n",
    "axes[0, 0].set_title(\"Top 20 Hashtags by Frequency\")\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Hashtags by engagement\n",
    "top_engagement = hashtag_stats.nlargest(20, \"total_engagement\")\n",
    "axes[0, 1].barh(\n",
    "    range(len(top_engagement)),\n",
    "    top_engagement[\"total_engagement\"].values,\n",
    "    color=\"coral\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[0, 1].set_yticks(range(len(top_engagement)))\n",
    "axes[0, 1].set_yticklabels(top_engagement[\"hashtag\"].values, fontsize=8)\n",
    "axes[0, 1].set_xlabel(\"Total Engagement Score\")\n",
    "axes[0, 1].set_title(\"Top 20 Hashtags by Engagement\")\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Frequency vs Engagement\n",
    "axes[1, 0].scatter(\n",
    "    hashtag_stats[\"frequency\"],\n",
    "    hashtag_stats[\"total_engagement\"],\n",
    "    alpha=0.5,\n",
    "    s=50,\n",
    "    c=\"green\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"Hashtag Frequency\")\n",
    "axes[1, 0].set_ylabel(\"Total Engagement\")\n",
    "axes[1, 0].set_title(\"Hashtag Frequency vs Engagement\")\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Sentiment by top hashtags\n",
    "top_15_hashtags = hashtag_stats.head(15)\n",
    "colors_sentiment = [\n",
    "    \"lightgreen\" if x > 0 else \"lightcoral\" if x < 0 else \"lightblue\"\n",
    "    for x in top_15_hashtags[\"avg_sentiment\"]\n",
    "]\n",
    "axes[1, 1].barh(\n",
    "    range(len(top_15_hashtags)),\n",
    "    top_15_hashtags[\"avg_sentiment\"].values,\n",
    "    color=colors_sentiment,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1, 1].set_yticks(range(len(top_15_hashtags)))\n",
    "axes[1, 1].set_yticklabels(top_15_hashtags[\"hashtag\"].values, fontsize=8)\n",
    "axes[1, 1].set_xlabel(\"Average Sentiment Score\")\n",
    "axes[1, 1].set_title(\"Sentiment of Top 15 Hashtags\")\n",
    "axes[1, 1].axvline(0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hashtag_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHashtag analysis visualizations complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-c9d0-1234-8901-234567890123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal hashtag trends\n",
    "posts_df[\"week\"] = posts_df[\"timestamp\"].dt.to_period(\"W\")\n",
    "\n",
    "# Track top 5 hashtags over time\n",
    "top_5_hashtags = hashtag_stats.head(5)[\"hashtag\"].tolist()\n",
    "\n",
    "weekly_hashtag_trends = []\n",
    "for _, post in posts_df.iterrows():\n",
    "    for hashtag in post[\"hashtags\"]:\n",
    "        if hashtag.lower() in top_5_hashtags:\n",
    "            weekly_hashtag_trends.append({\"week\": post[\"week\"], \"hashtag\": hashtag.lower()})\n",
    "\n",
    "trends_df = pd.DataFrame(weekly_hashtag_trends)\n",
    "trends_pivot = trends_df.groupby([\"week\", \"hashtag\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot trends\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for hashtag in top_5_hashtags:\n",
    "    if hashtag in trends_pivot.columns:\n",
    "        ax.plot(\n",
    "            range(len(trends_pivot)),\n",
    "            trends_pivot[hashtag].values,\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            label=hashtag,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Week\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Trending Hashtags Over Time (Top 5)\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hashtag_trends.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHashtag trend analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-d0e1-2345-9012-345678901234",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Findings\n",
    "\n",
    "We synthesize our analysis to identify key insights about the social media network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0-e1f2-3456-0123-456789012345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary statistics\n",
    "summary = {\n",
    "    \"Network Statistics\": {\n",
    "        \"Total Users\": social_network.number_of_nodes(),\n",
    "        \"Total Connections\": social_network.number_of_edges(),\n",
    "        \"Network Density\": f\"{nx.density(social_network):.4f}\",\n",
    "        \"Average Degree\": f\"{sum(dict(social_network.degree()).values()) / social_network.number_of_nodes():.2f}\",\n",
    "        \"Number of Communities\": len(set(user_communities.values())),\n",
    "        \"Modularity Score\": f\"{modularity_score:.4f}\",\n",
    "    },\n",
    "    \"Content Statistics\": {\n",
    "        \"Total Posts\": len(posts_df),\n",
    "        \"Retweet Percentage\": f\"{posts_df['is_retweet'].sum() / len(posts_df) * 100:.1f}%\",\n",
    "        \"Average Engagement\": f\"{posts_df['engagement_score'].mean():.2f}\",\n",
    "        \"Total Unique Hashtags\": len(hashtag_stats),\n",
    "    },\n",
    "    \"Sentiment Statistics\": {\n",
    "        \"Average Sentiment\": f\"{posts_df['sentiment_compound'].mean():.3f}\",\n",
    "        \"Positive Posts\": f\"{(posts_df['sentiment_category'] == 'positive').sum()} ({(posts_df['sentiment_category'] == 'positive').sum() / len(posts_df) * 100:.1f}%)\",\n",
    "        \"Negative Posts\": f\"{(posts_df['sentiment_category'] == 'negative').sum()} ({(posts_df['sentiment_category'] == 'negative').sum() / len(posts_df) * 100:.1f}%)\",\n",
    "        \"Neutral Posts\": f\"{(posts_df['sentiment_category'] == 'neutral').sum()} ({(posts_df['sentiment_category'] == 'neutral').sum() / len(posts_df) * 100:.1f}%)\",\n",
    "    },\n",
    "    \"Virality Statistics\": {\n",
    "        \"Total Cascades\": len(retweet_cascades),\n",
    "        \"Average Cascade Size\": f\"{np.mean(list(cascade_sizes.values())):.2f}\"\n",
    "        if cascade_sizes\n",
    "        else \"0\",\n",
    "        \"Max Cascade Size\": f\"{max(cascade_sizes.values())}\" if cascade_sizes else \"0\",\n",
    "        \"Viral Posts (Top 10%)\": f\"{int(len(posts_df) * 0.1)}\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SOCIAL MEDIA NETWORK ANALYSIS - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for category, stats in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for metric, value in stats.items():\n",
    "        print(f\"  {metric:.<50} {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1-f2a3-4567-1234-567890123456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Findings Report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS AND INSIGHTS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Finding 1: Network Structure\n",
    "avg_degree = sum(dict(social_network.degree()).values()) / social_network.number_of_nodes()\n",
    "finding1 = f\"\"\"\n",
    "1. NETWORK STRUCTURE AND CONNECTIVITY\n",
    "   The social network exhibits a scale-free topology with power-law degree distribution,\n",
    "   indicating a small number of highly connected hubs and many peripheral users.\n",
    "\n",
    "   - Average connections per user: {avg_degree:.2f}\n",
    "   - Network density: {nx.density(social_network):.4f} (sparse network)\n",
    "   - This structure is typical of real-world social media platforms\n",
    "\"\"\"\n",
    "findings.append(finding1)\n",
    "\n",
    "# Finding 2: Community Structure\n",
    "finding2 = f\"\"\"\n",
    "2. COMMUNITY FORMATION\n",
    "   Strong community structure detected with modularity score of {modularity_score:.4f}.\n",
    "   {len(set(user_communities.values()))} distinct communities identified, suggesting natural\n",
    "   clustering based on shared interests or interaction patterns.\n",
    "\n",
    "   - Communities vary significantly in size and activity levels\n",
    "   - Different communities exhibit distinct sentiment patterns\n",
    "\"\"\"\n",
    "findings.append(finding2)\n",
    "\n",
    "# Finding 3: Sentiment Patterns\n",
    "pos_pct = (posts_df[\"sentiment_category\"] == \"positive\").sum() / len(posts_df) * 100\n",
    "neg_pct = (posts_df[\"sentiment_category\"] == \"negative\").sum() / len(posts_df) * 100\n",
    "finding3 = f\"\"\"\n",
    "3. SENTIMENT DYNAMICS\n",
    "   Overall sentiment is {\"predominantly positive\" if pos_pct > neg_pct else \"mixed\"}.\n",
    "\n",
    "   - Positive posts: {pos_pct:.1f}%\n",
    "   - Negative posts: {neg_pct:.1f}%\n",
    "   - Average sentiment score: {posts_df[\"sentiment_compound\"].mean():.3f}\n",
    "   - Sentiment correlates with engagement patterns\n",
    "\"\"\"\n",
    "findings.append(finding3)\n",
    "\n",
    "# Finding 4: Influence Patterns\n",
    "top_10_influence = users_enriched.nlargest(10, \"overall_influence\")[\"overall_influence\"].mean()\n",
    "finding4 = f\"\"\"\n",
    "4. INFLUENCE AND ENGAGEMENT\n",
    "   A small percentage of users drive majority of engagement (Pareto principle observed).\n",
    "\n",
    "   - Top 10 influencers average score: {top_10_influence:.2f}\n",
    "   - Influence determined by both network position and content quality\n",
    "   - High correlation between follower count and engagement\n",
    "\"\"\"\n",
    "findings.append(finding4)\n",
    "\n",
    "# Finding 5: Information Diffusion\n",
    "avg_cascade = np.mean(list(cascade_sizes.values())) if cascade_sizes else 0\n",
    "finding5 = f\"\"\"\n",
    "5. INFORMATION DIFFUSION PATTERNS\n",
    "   Content spreads through network via retweet cascades.\n",
    "\n",
    "   - Average cascade size: {avg_cascade:.2f}\n",
    "   - Maximum cascade size: {max(cascade_sizes.values()) if cascade_sizes else 0}\n",
    "   - Viral content often has emotional (positive or negative) sentiment\n",
    "   - Community boundaries affect diffusion patterns\n",
    "\"\"\"\n",
    "findings.append(finding5)\n",
    "\n",
    "# Finding 6: Topic Trends\n",
    "top_hashtag = hashtag_stats.iloc[0]\n",
    "finding6 = f\"\"\"\n",
    "6. TOPIC TRENDS AND HASHTAG USAGE\n",
    "   Most popular hashtag: {top_hashtag[\"hashtag\"]} (frequency: {int(top_hashtag[\"frequency\"])})\n",
    "\n",
    "   - {len(hashtag_stats)} unique hashtags identified\n",
    "   - Hashtag usage follows power-law distribution\n",
    "   - Different topics generate varying levels of engagement\n",
    "\"\"\"\n",
    "findings.append(finding6)\n",
    "\n",
    "for finding in findings:\n",
    "    print(finding)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-2345-678901234567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Implications\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESEARCH IMPLICATIONS AND APPLICATIONS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "implications = \"\"\"\n",
    "This analysis demonstrates several key methodologies for social media research:\n",
    "\n",
    "1. NETWORK ANALYSIS\n",
    "   - Centrality measures identify influential users and key information brokers\n",
    "   - Community detection reveals organic social clustering patterns\n",
    "   - Network topology informs understanding of information flow\n",
    "\n",
    "2. SENTIMENT ANALYSIS\n",
    "   - VADER provides reliable sentiment scoring for social media text\n",
    "   - Temporal sentiment tracking reveals mood shifts and events\n",
    "   - Community-level sentiment differences highlight subculture dynamics\n",
    "\n",
    "3. INFLUENCE MEASUREMENT\n",
    "   - Combining network centrality with engagement metrics provides holistic influence score\n",
    "   - Different types of influence (network vs content) can be distinguished\n",
    "   - Influence prediction enables targeted intervention strategies\n",
    "\n",
    "4. INFORMATION DIFFUSION\n",
    "   - Cascade analysis quantifies viral spread patterns\n",
    "   - Identifies factors associated with viral content\n",
    "   - Helps predict and model information propagation\n",
    "\n",
    "5. PRACTICAL APPLICATIONS\n",
    "   - Marketing: Identify influencers and optimal content strategies\n",
    "   - Public Health: Track health information dissemination\n",
    "   - Politics: Understand political discourse and polarization\n",
    "   - Crisis Communication: Monitor sentiment during emergencies\n",
    "   - Brand Management: Track brand perception and reputation\n",
    "\n",
    "6. METHODOLOGICAL CONSIDERATIONS\n",
    "   - Synthetic data allows safe experimentation and method development\n",
    "   - Real data requires careful ethical consideration and privacy protection\n",
    "   - Longitudinal analysis provides deeper insights than cross-sectional\n",
    "   - Multi-method approach (network + sentiment + content) yields richer understanding\n",
    "\n",
    "7. FUTURE DIRECTIONS\n",
    "   - Temporal network analysis to track evolution over time\n",
    "   - Natural language processing for deeper content analysis\n",
    "   - Machine learning for prediction and classification\n",
    "   - Cross-platform analysis for comprehensive social media understanding\n",
    "   - Causal inference to understand drivers of influence and virality\n",
    "\"\"\"\n",
    "\n",
    "print(implications)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\nAnalysis Complete!\")\n",
    "print(\"\\nAll visualizations saved:\")\n",
    "print(\"  - network_centrality_distributions.png\")\n",
    "print(\"  - social_network_visualization.png\")\n",
    "print(\"  - community_detection.png\")\n",
    "print(\"  - sentiment_analysis.png\")\n",
    "print(\"  - sentiment_by_community.png\")\n",
    "print(\"  - influence_analysis.png\")\n",
    "print(\"  - information_diffusion.png\")\n",
    "print(\"  - virality_by_community.png\")\n",
    "print(\"  - hashtag_analysis.png\")\n",
    "print(\"  - hashtag_trends.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-b4c5-6789-3456-789012345678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for future analysis\n",
    "print(\"\\nSaving processed datasets...\")\n",
    "\n",
    "# Save enriched user data\n",
    "users_enriched.to_csv(\"users_enriched.csv\", index=False)\n",
    "print(\"  - users_enriched.csv\")\n",
    "\n",
    "# Save posts with sentiment\n",
    "posts_df.to_csv(\"posts_with_sentiment.csv\", index=False)\n",
    "print(\"  - posts_with_sentiment.csv\")\n",
    "\n",
    "# Save hashtag statistics\n",
    "hashtag_stats.to_csv(\"hashtag_statistics.csv\", index=False)\n",
    "print(\"  - hashtag_statistics.csv\")\n",
    "\n",
    "# Save network\n",
    "nx.write_gexf(social_network, \"social_network.gexf\")\n",
    "print(\"  - social_network.gexf (for Gephi or other network tools)\")\n",
    "\n",
    "print(\"\\nAll data saved successfully!\")\n",
    "print(\"\\nThis notebook provides a comprehensive framework for social media analysis.\")\n",
    "print(\"Adapt these methods for your own research questions and datasets.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
