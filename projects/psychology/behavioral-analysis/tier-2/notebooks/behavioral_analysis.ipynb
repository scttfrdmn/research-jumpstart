{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Data Analysis with AWS\n",
    "\n",
    "This notebook demonstrates cloud-based behavioral data analysis using AWS services:\n",
    "- S3 for data storage\n",
    "- Lambda for serverless processing\n",
    "- DynamoDB for results storage\n",
    "\n",
    "**Duration:** 30-45 minutes  \n",
    "**Prerequisites:** AWS setup complete (see setup_guide.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "S3_BUCKET = 'behavioral-data-XXXXX'  # Replace with your bucket name\n",
    "LAMBDA_FUNCTION = 'analyze-behavioral-data'\n",
    "DYNAMODB_TABLE = 'BehavioralAnalysis'\n",
    "AWS_REGION = 'us-east-1'\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION)\n",
    "\n",
    "print(f\"AWS clients initialized for region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Data\n",
    "\n",
    "Generate sample behavioral data for different experimental paradigms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stroop_data(participant_id, n_trials=100):\n",
    "    \"\"\"Generate sample Stroop task data.\"\"\"\n",
    "    np.random.seed(hash(participant_id) % 2**32)\n",
    "    \n",
    "    data = []\n",
    "    for trial in range(1, n_trials + 1):\n",
    "        condition = 'congruent' if np.random.random() > 0.5 else 'incongruent'\n",
    "        \n",
    "        if condition == 'congruent':\n",
    "            rt = np.random.normal(500, 80)\n",
    "            accuracy = 1 if np.random.random() > 0.05 else 0\n",
    "        else:\n",
    "            rt = np.random.normal(650, 100)\n",
    "            accuracy = 1 if np.random.random() > 0.15 else 0\n",
    "        \n",
    "        rt = max(200, rt)\n",
    "        \n",
    "        data.append({\n",
    "            'participant_id': participant_id,\n",
    "            'trial': trial,\n",
    "            'task_type': 'stroop',\n",
    "            'stimulus': condition,\n",
    "            'response': np.random.choice(['left', 'right']),\n",
    "            'rt': round(rt, 2),\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_decision_data(participant_id, n_trials=100):\n",
    "    \"\"\"Generate sample decision making data.\"\"\"\n",
    "    np.random.seed(hash(participant_id) % 2**32 + 1)\n",
    "    \n",
    "    data = []\n",
    "    difficulties = ['easy', 'medium', 'hard']\n",
    "    \n",
    "    for trial in range(1, n_trials + 1):\n",
    "        difficulty = np.random.choice(difficulties)\n",
    "        \n",
    "        if difficulty == 'easy':\n",
    "            rt = np.random.normal(400, 60)\n",
    "            accuracy = 1 if np.random.random() > 0.05 else 0\n",
    "        elif difficulty == 'medium':\n",
    "            rt = np.random.normal(550, 80)\n",
    "            accuracy = 1 if np.random.random() > 0.20 else 0\n",
    "        else:\n",
    "            rt = np.random.normal(700, 120)\n",
    "            accuracy = 1 if np.random.random() > 0.35 else 0\n",
    "        \n",
    "        rt = max(200, rt)\n",
    "        \n",
    "        data.append({\n",
    "            'participant_id': participant_id,\n",
    "            'trial': trial,\n",
    "            'task_type': 'decision',\n",
    "            'stimulus': difficulty,\n",
    "            'response': np.random.choice(['option_a', 'option_b']),\n",
    "            'rt': round(rt, 2),\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Generate data for 10 participants\n",
    "n_participants = 10\n",
    "participant_data = {}\n",
    "\n",
    "for i in range(1, n_participants + 1):\n",
    "    participant_id = f'sub{i:03d}'\n",
    "    \n",
    "    # Each participant does 1-2 tasks\n",
    "    tasks = np.random.choice(['stroop', 'decision'], size=np.random.randint(1, 3), replace=False)\n",
    "    \n",
    "    for task in tasks:\n",
    "        if task == 'stroop':\n",
    "            df = generate_stroop_data(participant_id)\n",
    "        else:\n",
    "            df = generate_decision_data(participant_id)\n",
    "        \n",
    "        key = f\"{participant_id}_{task}\"\n",
    "        participant_data[key] = df\n",
    "\n",
    "print(f\"Generated data for {len(participant_data)} participant-task combinations\")\n",
    "print(f\"\\nSample data (first 5 rows):\")\n",
    "print(participant_data[list(participant_data.keys())[0]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Data to S3\n",
    "\n",
    "Upload trial-level data to S3 for Lambda processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe_to_s3(df, bucket, key):\n",
    "    \"\"\"Upload DataFrame as CSV to S3.\"\"\"\n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "    s3_client.put_object(Bucket=bucket, Key=key, Body=csv_buffer)\n",
    "    return f\"s3://{bucket}/{key}\"\n",
    "\n",
    "\n",
    "# Upload all data\n",
    "uploaded_files = []\n",
    "\n",
    "for key, df in participant_data.items():\n",
    "    s3_key = f\"raw/{key}.csv\"\n",
    "    s3_uri = upload_dataframe_to_s3(df, S3_BUCKET, s3_key)\n",
    "    uploaded_files.append(s3_uri)\n",
    "    print(f\"Uploaded: {s3_uri}\")\n",
    "\n",
    "print(f\"\\nTotal files uploaded: {len(uploaded_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Data with Lambda\n",
    "\n",
    "Invoke Lambda function to analyze each participant's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_lambda_for_file(bucket, key):\n",
    "    \"\"\"Invoke Lambda function for a specific S3 file.\"\"\"\n",
    "    payload = {\n",
    "        'bucket': bucket,\n",
    "        'key': key\n",
    "    }\n",
    "    \n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=LAMBDA_FUNCTION,\n",
    "        InvocationType='RequestResponse',\n",
    "        Payload=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['Payload'].read())\n",
    "    return result\n",
    "\n",
    "\n",
    "# Process all files\n",
    "lambda_results = []\n",
    "\n",
    "for key in participant_data.keys():\n",
    "    s3_key = f\"raw/{key}.csv\"\n",
    "    print(f\"Processing: {s3_key}...\")\n",
    "    \n",
    "    result = invoke_lambda_for_file(S3_BUCKET, s3_key)\n",
    "    lambda_results.append(result)\n",
    "    \n",
    "    if result['statusCode'] == 200:\n",
    "        print(f\"  ✓ Success\")\n",
    "    else:\n",
    "        print(f\"  ✗ Error: {result.get('body', 'Unknown error')}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(lambda_results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Results from DynamoDB\n",
    "\n",
    "Retrieve analysis results from DynamoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_all_results(table_name):\n",
    "    \"\"\"Scan DynamoDB table and return all results.\"\"\"\n",
    "    table = dynamodb.Table(table_name)\n",
    "    \n",
    "    response = table.scan()\n",
    "    items = response['Items']\n",
    "    \n",
    "    # Handle pagination\n",
    "    while 'LastEvaluatedKey' in response:\n",
    "        response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "        items.extend(response['Items'])\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "def decimal_to_float(obj):\n",
    "    \"\"\"Convert DynamoDB Decimal to float.\"\"\"\n",
    "    from decimal import Decimal\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        return [decimal_to_float(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: decimal_to_float(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "# Query results\n",
    "print(\"Querying DynamoDB...\")\n",
    "items = query_all_results(DYNAMODB_TABLE)\n",
    "items = [decimal_to_float(item) for item in items]\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(items)\n",
    "\n",
    "print(f\"Retrieved {len(results_df)} participant results\")\n",
    "print(f\"\\nColumns: {', '.join(results_df.columns[:10])}...\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Descriptive Statistics\n",
    "\n",
    "Calculate group-level descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal participants: {len(results_df)}\")\n",
    "print(f\"\\nTasks:\")\n",
    "print(results_df['task_type'].value_counts())\n",
    "\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(\"REACTION TIME\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"Mean RT (ms):\")\n",
    "print(f\"  Mean:   {results_df['mean_rt'].mean():.2f}\")\n",
    "print(f\"  Median: {results_df['mean_rt'].median():.2f}\")\n",
    "print(f\"  SD:     {results_df['mean_rt'].std():.2f}\")\n",
    "print(f\"  Range:  [{results_df['mean_rt'].min():.2f}, {results_df['mean_rt'].max():.2f}]\")\n",
    "\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(\"ACCURACY\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"Accuracy:\")\n",
    "print(f\"  Mean:   {results_df['accuracy'].mean():.3f}\")\n",
    "print(f\"  Median: {results_df['accuracy'].median():.3f}\")\n",
    "print(f\"  SD:     {results_df['accuracy'].std():.3f}\")\n",
    "print(f\"  Range:  [{results_df['accuracy'].min():.3f}, {results_df['accuracy'].max():.3f}]\")\n",
    "\n",
    "# By task type\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(\"BY TASK TYPE\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "for task in results_df['task_type'].unique():\n",
    "    task_df = results_df[results_df['task_type'] == task]\n",
    "    print(f\"\\n{task.upper()}:\")\n",
    "    print(f\"  N = {len(task_df)}\")\n",
    "    print(f\"  Mean RT: {task_df['mean_rt'].mean():.2f} ms (SD = {task_df['mean_rt'].std():.2f})\")\n",
    "    print(f\"  Accuracy: {task_df['accuracy'].mean():.3f} (SD = {task_df['accuracy'].std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Create visualizations of behavioral data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(results_df['mean_rt'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(results_df['mean_rt'].mean(), color='red', linestyle='--', \n",
    "                label=f\"Mean: {results_df['mean_rt'].mean():.2f} ms\")\n",
    "axes[0].set_xlabel('Mean RT (ms)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Mean Reaction Times', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot by task\n",
    "task_data = [results_df[results_df['task_type'] == task]['mean_rt'].values \n",
    "             for task in results_df['task_type'].unique()]\n",
    "task_labels = results_df['task_type'].unique()\n",
    "\n",
    "axes[1].boxplot(task_data, labels=task_labels)\n",
    "axes[1].set_ylabel('Mean RT (ms)', fontsize=12)\n",
    "axes[1].set_xlabel('Task Type', fontsize=12)\n",
    "axes[1].set_title('Reaction Time by Task Type', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(results_df['accuracy'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(results_df['accuracy'].mean(), color='red', linestyle='--',\n",
    "                label=f\"Mean: {results_df['accuracy'].mean():.3f}\")\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Bar plot by task\n",
    "task_accuracy = results_df.groupby('task_type')['accuracy'].agg(['mean', 'std'])\n",
    "task_accuracy.plot(kind='bar', y='mean', yerr='std', ax=axes[1], \n",
    "                   capsize=5, legend=False, color='steelblue')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_xlabel('Task Type', fontsize=12)\n",
    "axes[1].set_title('Accuracy by Task Type', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy-RT tradeoff\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for task in results_df['task_type'].unique():\n",
    "    task_df = results_df[results_df['task_type'] == task]\n",
    "    plt.scatter(task_df['mean_rt'], task_df['accuracy'], \n",
    "                label=task.capitalize(), alpha=0.6, s=100)\n",
    "\n",
    "plt.xlabel('Mean RT (ms)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Speed-Accuracy Tradeoff', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task-Specific Analysis\n",
    "\n",
    "Analyze task-specific effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroop effect analysis\n",
    "stroop_df = results_df[results_df['task_type'] == 'stroop'].copy()\n",
    "\n",
    "if len(stroop_df) > 0 and 'stroop_effect_rt' in stroop_df.columns:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STROOP EFFECT ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    stroop_effect = stroop_df['stroop_effect_rt'].dropna()\n",
    "    \n",
    "    print(f\"\\nStroop Effect (Incongruent - Congruent RT):\")\n",
    "    print(f\"  Mean:   {stroop_effect.mean():.2f} ms\")\n",
    "    print(f\"  Median: {stroop_effect.median():.2f} ms\")\n",
    "    print(f\"  SD:     {stroop_effect.std():.2f} ms\")\n",
    "    print(f\"  Range:  [{stroop_effect.min():.2f}, {stroop_effect.max():.2f}]\")\n",
    "    \n",
    "    # One-sample t-test (is Stroop effect > 0?)\n",
    "    t_stat, p_value = stats.ttest_1samp(stroop_effect, 0)\n",
    "    print(f\"\\nOne-sample t-test (H0: effect = 0):\")\n",
    "    print(f\"  t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Stroop effect distribution\n",
    "    axes[0].hist(stroop_effect, bins=15, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(stroop_effect.mean(), color='red', linestyle='--',\n",
    "                    label=f\"Mean: {stroop_effect.mean():.2f} ms\")\n",
    "    axes[0].axvline(0, color='black', linestyle='-', linewidth=2, label='No effect')\n",
    "    axes[0].set_xlabel('Stroop Effect (ms)', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Stroop Effect Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # RT by condition\n",
    "    if 'mean_rt_congruent' in stroop_df.columns and 'mean_rt_incongruent' in stroop_df.columns:\n",
    "        congruent_rts = stroop_df['mean_rt_congruent'].dropna()\n",
    "        incongruent_rts = stroop_df['mean_rt_incongruent'].dropna()\n",
    "        \n",
    "        axes[1].boxplot([congruent_rts, incongruent_rts], \n",
    "                        labels=['Congruent', 'Incongruent'])\n",
    "        axes[1].set_ylabel('Mean RT (ms)', fontsize=12)\n",
    "        axes[1].set_xlabel('Condition', fontsize=12)\n",
    "        axes[1].set_title('RT by Stroop Condition', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Stroop data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision task difficulty analysis\n",
    "decision_df = results_df[results_df['task_type'] == 'decision'].copy()\n",
    "\n",
    "if len(decision_df) > 0 and 'difficulty_effect_rt' in decision_df.columns:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DECISION DIFFICULTY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    difficulty_effect = decision_df['difficulty_effect_rt'].dropna()\n",
    "    \n",
    "    print(f\"\\nDifficulty Effect (Hard - Easy RT):\")\n",
    "    print(f\"  Mean:   {difficulty_effect.mean():.2f} ms\")\n",
    "    print(f\"  Median: {difficulty_effect.median():.2f} ms\")\n",
    "    print(f\"  SD:     {difficulty_effect.std():.2f} ms\")\n",
    "    \n",
    "    # Visualization\n",
    "    if all(col in decision_df.columns for col in ['mean_rt_easy', 'mean_rt_medium', 'mean_rt_hard']):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # RT by difficulty\n",
    "        difficulty_data = [\n",
    "            decision_df['mean_rt_easy'].dropna(),\n",
    "            decision_df['mean_rt_medium'].dropna(),\n",
    "            decision_df['mean_rt_hard'].dropna()\n",
    "        ]\n",
    "        \n",
    "        axes[0].boxplot(difficulty_data, labels=['Easy', 'Medium', 'Hard'])\n",
    "        axes[0].set_ylabel('Mean RT (ms)', fontsize=12)\n",
    "        axes[0].set_xlabel('Difficulty', fontsize=12)\n",
    "        axes[0].set_title('RT by Difficulty Level', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Accuracy by difficulty\n",
    "        accuracy_data = [\n",
    "            decision_df['accuracy_easy'].dropna(),\n",
    "            decision_df['accuracy_medium'].dropna(),\n",
    "            decision_df['accuracy_hard'].dropna()\n",
    "        ]\n",
    "        \n",
    "        axes[1].boxplot(accuracy_data, labels=['Easy', 'Medium', 'Hard'])\n",
    "        axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "        axes[1].set_xlabel('Difficulty', fontsize=12)\n",
    "        axes[1].set_title('Accuracy by Difficulty Level', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No decision making data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Signal Detection Theory Analysis\n",
    "\n",
    "Analyze d-prime and response criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dprime' in results_df.columns:\n",
    "    dprime_data = results_df['dprime'].dropna()\n",
    "    criterion_data = results_df['criterion'].dropna()\n",
    "    \n",
    "    if len(dprime_data) > 0:\n",
    "        print(\"=\"*60)\n",
    "        print(\"SIGNAL DETECTION THEORY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nd-prime (sensitivity):\")\n",
    "        print(f\"  Mean:   {dprime_data.mean():.3f}\")\n",
    "        print(f\"  Median: {dprime_data.median():.3f}\")\n",
    "        print(f\"  SD:     {dprime_data.std():.3f}\")\n",
    "        print(f\"  Range:  [{dprime_data.min():.3f}, {dprime_data.max():.3f}]\")\n",
    "        \n",
    "        if len(criterion_data) > 0:\n",
    "            print(f\"\\nCriterion (response bias):\")\n",
    "            print(f\"  Mean:   {criterion_data.mean():.3f}\")\n",
    "            print(f\"  Median: {criterion_data.median():.3f}\")\n",
    "            print(f\"  SD:     {criterion_data.std():.3f}\")\n",
    "            \n",
    "            # One-sample t-test (is criterion different from 0?)\n",
    "            t_stat, p_value = stats.ttest_1samp(criterion_data, 0)\n",
    "            print(f\"\\nOne-sample t-test (H0: criterion = 0):\")\n",
    "            print(f\"  t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        axes[0].hist(dprime_data, bins=15, edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(dprime_data.mean(), color='red', linestyle='--',\n",
    "                        label=f\"Mean: {dprime_data.mean():.3f}\")\n",
    "        axes[0].set_xlabel(\"d-prime\", fontsize=12)\n",
    "        axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[0].set_title('Sensitivity Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        if len(criterion_data) > 0:\n",
    "            axes[1].hist(criterion_data, bins=15, edgecolor='black', alpha=0.7)\n",
    "            axes[1].axvline(criterion_data.mean(), color='red', linestyle='--',\n",
    "                            label=f\"Mean: {criterion_data.mean():.3f}\")\n",
    "            axes[1].axvline(0, color='black', linestyle='-', linewidth=2, \n",
    "                            label='No bias')\n",
    "            axes[1].set_xlabel('Criterion', fontsize=12)\n",
    "            axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "            axes[1].set_title('Response Bias Distribution', fontsize=14, fontweight='bold')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No signal detection theory data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Tests\n",
    "\n",
    "Perform group-level statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare RT between tasks\n",
    "if len(results_df['task_type'].unique()) >= 2:\n",
    "    tasks = results_df['task_type'].unique()[:2]\n",
    "    task1_rt = results_df[results_df['task_type'] == tasks[0]]['mean_rt']\n",
    "    task2_rt = results_df[results_df['task_type'] == tasks[1]]['mean_rt']\n",
    "    \n",
    "    print(f\"\\nIndependent t-test: {tasks[0]} vs {tasks[1]} (RT)\")\n",
    "    t_stat, p_value = stats.ttest_ind(task1_rt, task2_rt)\n",
    "    print(f\"  t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "    print(f\"  Effect size (Cohen's d): {(task1_rt.mean() - task2_rt.mean()) / np.sqrt((task1_rt.std()**2 + task2_rt.std()**2) / 2):.3f}\")\n",
    "    \n",
    "    # Compare accuracy\n",
    "    task1_acc = results_df[results_df['task_type'] == tasks[0]]['accuracy']\n",
    "    task2_acc = results_df[results_df['task_type'] == tasks[1]]['accuracy']\n",
    "    \n",
    "    print(f\"\\nIndependent t-test: {tasks[0]} vs {tasks[1]} (Accuracy)\")\n",
    "    t_stat, p_value = stats.ttest_ind(task1_acc, task2_acc)\n",
    "    print(f\"  t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "    print(f\"  Effect size (Cohen's d): {(task1_acc.mean() - task2_acc.mean()) / np.sqrt((task1_acc.std()**2 + task2_acc.std()**2) / 2):.3f}\")\n",
    "\n",
    "# RT-Accuracy correlation\n",
    "print(f\"\\nCorrelation: RT vs Accuracy\")\n",
    "r, p = stats.pearsonr(results_df['mean_rt'], results_df['accuracy'])\n",
    "print(f\"  r = {r:.3f}, p = {p:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Export results for further analysis or publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_filename = f'behavioral_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "print(f\"Results exported to: {output_filename}\")\n",
    "\n",
    "# Summary statistics\n",
    "summary = results_df.groupby('task_type').agg({\n",
    "    'mean_rt': ['mean', 'std', 'count'],\n",
    "    'accuracy': ['mean', 'std']\n",
    "})\n",
    "\n",
    "summary_filename = f'summary_stats_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "summary.to_csv(summary_filename)\n",
    "print(f\"Summary statistics exported to: {summary_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Generating sample behavioral data\n",
    "2. ✅ Uploading data to S3\n",
    "3. ✅ Processing with Lambda (serverless)\n",
    "4. ✅ Storing results in DynamoDB\n",
    "5. ✅ Querying and aggregating results\n",
    "6. ✅ Statistical analysis and visualization\n",
    "7. ✅ Task-specific analyses (Stroop, decision making)\n",
    "8. ✅ Signal detection theory\n",
    "9. ✅ Exporting results\n",
    "\n",
    "### Next Steps\n",
    "- Analyze your own behavioral data\n",
    "- Implement additional computational models\n",
    "- Scale to hundreds or thousands of participants\n",
    "- Move to Tier 3 for production CloudFormation deployment\n",
    "\n",
    "### Cost Tracking\n",
    "Remember to check AWS Cost Explorer to monitor your spending!\n",
    "\n",
    "### Cleanup\n",
    "When finished, follow `cleanup_guide.md` to delete all AWS resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
