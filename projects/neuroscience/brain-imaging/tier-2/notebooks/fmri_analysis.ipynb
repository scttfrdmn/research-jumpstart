{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis with AWS\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Download processed fMRI data from S3\n",
    "2. Calculate functional connectivity\n",
    "3. Visualize brain networks\n",
    "4. Generate statistical summaries\n",
    "\n",
    "**Prerequisites:**\n",
    "- AWS credentials configured locally\n",
    "- Processed fMRI files in S3 output bucket\n",
    "- Python dependencies installed: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Configure AWS credentials and S3 bucket information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, signal\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# UPDATE THESE WITH YOUR ACTUAL BUCKET NAMES\n",
    "AWS_REGION = 'us-east-1'\n",
    "OUTPUT_BUCKET = 'fmri-output-{your-username}'  # CHANGE THIS\n",
    "LOCAL_RESULTS_DIR = '../results'\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Create local results directory\n",
    "os.makedirs(LOCAL_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print(f\"Output Bucket: {OUTPUT_BUCKET}\")\n",
    "print(f\"Results Directory: {LOCAL_RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Processed Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_file(bucket, key, local_path):\n",
    "    \"\"\"Download file from S3.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Downloading s3://{bucket}/{key}\")\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        logger.info(f\"Saved to {local_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading: {e}\")\n",
    "        return False\n",
    "\n",
    "def list_s3_objects(bucket, prefix=''):\n",
    "    \"\"\"List objects in S3 bucket.\"\"\"\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        if 'Contents' not in response:\n",
    "            return []\n",
    "        return [obj['Key'] for obj in response['Contents']]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error listing S3: {e}\")\n",
    "        return []\n",
    "\n",
    "# List available results\n",
    "print(f\"Listing results in s3://{OUTPUT_BUCKET}/\")\n",
    "objects = list_s3_objects(OUTPUT_BUCKET)\n",
    "print(f\"Found {len(objects)} objects:\")\n",
    "for obj in objects[:10]:  # Show first 10\n",
    "    print(f\"  - {obj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download processed fMRI files\n",
    "# Find smoothed file (final processed output)\n",
    "smoothed_files = [obj for obj in objects if 'smoothed' in obj.lower()]\n",
    "\n",
    "if smoothed_files:\n",
    "    smoothed_key = smoothed_files[0]\n",
    "    smoothed_path = os.path.join(LOCAL_RESULTS_DIR, 'fmri_smoothed.nii.gz')\n",
    "    \n",
    "    download_s3_file(OUTPUT_BUCKET, smoothed_key, smoothed_path)\n",
    "    print(f\"✓ Downloaded processed fMRI: {smoothed_path}\")\nelse:\n",
    "    print(\"⚠ No smoothed files found in S3. Did Lambda processing complete?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore fMRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fMRI data\n",
    "fmri_path = os.path.join(LOCAL_RESULTS_DIR, 'fmri_smoothed.nii.gz')\n",
    "\n",
    "if os.path.exists(fmri_path):\n",
    "    print(f\"Loading {fmri_path}\")\n",
    "    img = nib.load(fmri_path)\n",
    "    fmri_data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "    \n",
    "    print(f\"\\nfMRI Data Info:\")\n",
    "    print(f\"  Shape: {fmri_data.shape}\")\n",
    "    print(f\"  Data type: {fmri_data.dtype}\")\n",
    "    print(f\"  Range: [{np.min(fmri_data):.3f}, {np.max(fmri_data):.3f}]\")\n",
    "    print(f\"  Mean: {np.mean(fmri_data):.3f}\")\n",
    "    print(f\"  Std: {np.std(fmri_data):.3f}\")\n",
    "    print(f\"  Affine shape: {affine.shape}\")\n",
    "else:\n",
    "    print(f\"File not found: {fmri_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract brain voxels (non-zero)\n",
    "# Reshape 4D (x, y, z, time) to 2D (voxels, time)\n",
    "x, y, z, t = fmri_data.shape\n",
    "print(f\"Extracting voxel time series...\")\n",
    "\n",
    "# Reshape to (voxels, timepoints)\n",
    "fmri_2d = fmri_data.reshape(-1, t)\n",
    "print(f\"Reshaped fMRI: {fmri_2d.shape}\")\n",
    "\n",
    "# Remove voxels with zero variance (non-brain voxels)\n",
    "voxel_var = np.var(fmri_2d, axis=1)\n",
    "brain_mask = voxel_var > 0\n",
    "fmri_brain = fmri_2d[brain_mask, :]\n",
    "\n",
    "print(f\"Brain voxels: {fmri_brain.shape[0]}\")\n",
    "print(f\"Timepoints: {fmri_brain.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Functional Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data (z-score normalization)\n",
    "print(\"Standardizing fMRI data...\")\n",
    "fmri_standardized = stats.zscore(fmri_brain, axis=1)\n",
    "\n",
    "# Compute correlation matrix (functional connectivity)\n",
    "print(\"Computing functional connectivity matrix...\")\n",
    "connectivity = np.corrcoef(fmri_standardized)\n",
    "print(f\"Connectivity matrix shape: {connectivity.shape}\")\n",
    "\n",
    "# Handle NaN values\n",
    "connectivity = np.nan_to_num(connectivity, nan=0.0)\n",
    "\n",
    "print(f\"Connectivity range: [{np.min(connectivity):.3f}, {np.max(connectivity):.3f}]\")\n",
    "print(f\"Connectivity mean: {np.mean(connectivity):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample connectivity matrix for visualization (too large otherwise)\n",
    "# Select every Nth voxel\n",
    "subsample_factor = 10\n",
    "indices = np.arange(0, connectivity.shape[0], subsample_factor)\n",
    "connectivity_sub = connectivity[np.ix_(indices, indices)]\n",
    "\n",
    "print(f\"Subsampled connectivity shape: {connectivity_sub.shape}\")\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(connectivity_sub, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_title('Functional Connectivity Matrix (Subsampled)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Voxel Index')\n",
    "ax.set_ylabel('Voxel Index')\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Pearson Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Connectivity heatmap created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Network Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate network statistics\n",
    "print(\"Computing network statistics...\")\n",
    "\n",
    "# Threshold connectivity at 0.5 correlation\n",
    "threshold = 0.5\n",
    "connectivity_thresholded = connectivity.copy()\n",
    "connectivity_thresholded[np.abs(connectivity_thresholded) < threshold] = 0\n",
    "\n",
    "# Degree (number of connections per voxel)\n",
    "degree = np.sum(connectivity_thresholded > 0, axis=1)\n",
    "\n",
    "# Strength (sum of connection weights)\n",
    "strength = np.sum(np.abs(connectivity_thresholded), axis=1)\n",
    "\n",
    "# Clustering coefficient\n",
    "def clustering_coefficient(adj_matrix):\n",
    "    \"\"\"Calculate clustering coefficient.\"\"\"\n",
    "    cc = []\n",
    "    n = adj_matrix.shape[0]\n",
    "    for i in range(min(n, 100)):  # Compute for subset (faster)\n",
    "        neighbors = np.where(adj_matrix[i] > 0)[0]\n",
    "        if len(neighbors) > 1:\n",
    "            subgraph = adj_matrix[np.ix_(neighbors, neighbors)]\n",
    "            c = np.sum(subgraph > 0) / (len(neighbors) * (len(neighbors) - 1))\n",
    "            cc.append(c)\n",
    "    return np.mean(cc) if cc else 0\n",
    "\n",
    "cc = clustering_coefficient(connectivity_thresholded)\n",
    "\n",
    "print(f\"\\nNetwork Statistics (threshold={threshold}):\")\n",
    "print(f\"  Total voxels: {len(degree)}\")\n",
    "print(f\"  Mean degree: {np.mean(degree):.2f}\")\n",
    "print(f\"  Max degree: {np.max(degree):.0f}\")\n",
    "print(f\"  Mean strength: {np.mean(strength):.3f}\")\n",
    "print(f\"  Clustering coefficient: {cc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hub voxels (high degree)\n",
    "hub_percentile = 95\n",
    "hub_threshold = np.percentile(degree, hub_percentile)\n",
    "hub_voxels = np.where(degree >= hub_threshold)[0]\n",
    "\n",
    "print(f\"Hub voxels (top {100-hub_percentile}%): {len(hub_voxels)}\")\n",
    "\n",
    "# Visualize degree distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Degree distribution\n",
    "axes[0].hist(degree, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(np.mean(degree), color='r', linestyle='--', label=f'Mean: {np.mean(degree):.2f}')\n",
    "axes[0].set_xlabel('Degree (Number of Connections)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Node Degree Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Strength distribution\n",
    "axes[1].hist(strength, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(np.mean(strength), color='r', linestyle='--', label=f'Mean: {np.mean(strength):.3f}')\n",
    "axes[1].set_xlabel('Strength (Sum of Weights)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Node Strength Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Network statistics plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns\n",
    "print(\"Analyzing temporal patterns...\")\n",
    "\n",
    "# Extract mean time series\n",
    "mean_timeseries = np.mean(fmri_brain, axis=0)\n",
    "\n",
    "# Power spectrum analysis\n",
    "from scipy import signal\n",
    "freqs, power = signal.periodogram(mean_timeseries)\n",
    "\n",
    "# Plot time series and power spectrum\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Time series\n",
    "axes[0].plot(mean_timeseries, linewidth=0.5)\n",
    "axes[0].set_xlabel('Time (TR)')\n",
    "axes[0].set_ylabel('Mean BOLD Signal')\n",
    "axes[0].set_title('Mean Brain BOLD Time Series')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Power spectrum (log scale)\n",
    "axes[1].semilogy(freqs[:len(freqs)//2], power[:len(power)//2])\n",
    "axes[1].set_xlabel('Frequency (Hz)')\n",
    "axes[1].set_ylabel('Power Spectral Density')\n",
    "axes[1].set_title('Power Spectrum of Mean BOLD Signal')\n",
    "axes[1].grid(alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Time series plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'Analysis Date': pd.Timestamp.now().isoformat(),\n",
    "    'fMRI Shape': str(fmri_data.shape),\n",
    "    'Brain Voxels': int(fmri_brain.shape[0]),\n",
    "    'Timepoints': int(fmri_brain.shape[1]),\n",
    "    'Mean Signal': float(np.mean(fmri_brain)),\n",
    "    'Std Signal': float(np.std(fmri_brain)),\n",
    "    'Signal Range': f\"[{np.min(fmri_brain):.3f}, {np.max(fmri_brain):.3f}]\",\n",
    "    'Network - Mean Degree': float(np.mean(degree)),\n",
    "    'Network - Max Degree': float(np.max(degree)),\n",
    "    'Network - Mean Strength': float(np.mean(strength)),\n",
    "    'Network - Clustering Coeff': float(cc),\n",
    "    'Hub Voxels': int(len(hub_voxels)),\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(LOCAL_RESULTS_DIR, 'analysis_summary.csv')\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n✓ Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save connectivity data\n",
    "connectivity_path = os.path.join(LOCAL_RESULTS_DIR, 'connectivity_matrix.npy')\n",
    "np.save(connectivity_path, connectivity[:1000, :1000])  # Save first 1000x1000 for size\n",
    "print(f\"✓ Connectivity matrix saved to {connectivity_path}\")\n",
    "\n",
    "# Save network metrics\n",
    "metrics_path = os.path.join(LOCAL_RESULTS_DIR, 'network_metrics.csv')\n",
    "metrics_df = pd.DataFrame({\n",
    "    'voxel_id': np.arange(len(degree)),\n",
    "    'degree': degree,\n",
    "    'strength': strength\n",
    "})\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"✓ Network metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps and Cleanup\n",
    "\n",
    "After completing this analysis:\n",
    "\n",
    "1. **Review Results:** Check analysis_summary.csv and visualizations\n",
    "2. **Save Important Files:** Keep any results you want to preserve\n",
    "3. **Cleanup AWS Resources:** Run the cleanup script to avoid charges\n",
    "\n",
    "```bash\n",
    "cd /path/to/tier-2/\n",
    "python scripts/cleanup.py \\\n",
    "    --input-bucket fmri-input-{your-username} \\\n",
    "    --output-bucket fmri-output-{your-username} \\\n",
    "    --lambda-function fmri-preprocessor \\\n",
    "    --iam-role lambda-fmri-processor \\\n",
    "    --confirm\n",
    "```\n",
    "\n",
    "**Important:** This will delete all S3 buckets and AWS resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults saved to: {LOCAL_RESULTS_DIR}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "for file in os.listdir(LOCAL_RESULTS_DIR):\n",
    "    path = os.path.join(LOCAL_RESULTS_DIR, file)\n",
    "    if os.path.isfile(path):\n",
    "        size = os.path.getsize(path) / 1e6\n",
    "        print(f\"  - {file} ({size:.2f}MB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
