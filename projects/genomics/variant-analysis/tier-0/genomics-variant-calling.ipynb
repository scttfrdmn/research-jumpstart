{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Calling with Deep Learning on 1000 Genomes Data\n",
    "\n",
    "**Duration:** 60-90 minutes | **Platform:** Colab or Studio Lab | **Data:** ~1.5GB\n",
    "\n",
    "Train a CNN to call genetic variants from sequencing reads using real 1000 Genomes Project data.\n",
    "\n",
    "## Research Goal\n",
    "\n",
    "Build a deep learning variant caller that identifies SNPs and indels from aligned BAM files, competing with traditional tools like GATK HaplotypeCaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (pre-installed in Studio Lab)\n",
    "!pip install -q pysam biopython tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import auc, precision_recall_fscore_support, roc_curve\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download 1000 Genomes Data\n",
    "\n",
    "Download a subset of chromosome 20 BAM file from individual NA12878 (~1.5GB, 15-20 minutes).\n",
    "\n",
    "**Note:** Data downloads from AWS Open Data Registry (no credentials required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# AWS Open Data Registry URLs (public, no credentials)\n",
    "BASE_URL = \"https://s3.amazonaws.com/1000genomes/\"\n",
    "\n",
    "# Files to download\n",
    "files_to_download = [\n",
    "    \"phase3/data/NA12878/alignment/NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam\",\n",
    "    \"phase3/data/NA12878/alignment/NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam.bai\",\n",
    "]\n",
    "\n",
    "# Reference genome (hg19) - subset for chr20\n",
    "reference_url = \"https://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chr20.fa.gz\"\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download file with progress bar.\"\"\"\n",
    "    if destination.exists():\n",
    "        print(f\"✓ Using cached file: {destination.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading {destination.name}...\")\n",
    "    urllib.request.urlretrieve(url, destination)\n",
    "    print(f\"✓ Downloaded {destination.name}\")\n",
    "\n",
    "\n",
    "# Download BAM files\n",
    "for file_path in files_to_download:\n",
    "    filename = Path(file_path).name\n",
    "    url = BASE_URL + file_path\n",
    "    download_file(url, data_dir / filename)\n",
    "\n",
    "# Download reference genome\n",
    "download_file(reference_url, data_dir / \"chr20.fa.gz\")\n",
    "\n",
    "# Uncompress reference\n",
    "if not (data_dir / \"chr20.fa\").exists():\n",
    "    !gunzip -k {data_dir}/chr20.fa.gz\n",
    "    print(\"✓ Uncompressed reference genome\")\n",
    "\n",
    "print(\"\\n✓ Data download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the BAM File\n",
    "\n",
    "Load and inspect the aligned sequencing reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BAM file\n",
    "bam_file = data_dir / \"NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam\"\n",
    "bamfile = pysam.AlignmentFile(str(bam_file), \"rb\")\n",
    "\n",
    "# Get basic statistics\n",
    "print(f\"BAM file: {bam_file.name}\")\n",
    "print(f\"References: {bamfile.references[:5]}\")  # Chromosome names\n",
    "print(f\"Number of mapped reads: {bamfile.mapped}\")\n",
    "print(f\"Number of unmapped reads: {bamfile.unmapped}\")\n",
    "\n",
    "# Sample a few reads\n",
    "print(\"\\nSample reads:\")\n",
    "for i, read in enumerate(bamfile.fetch(\"20\", 10000000, 10000100)):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(\n",
    "        f\"  Position: {read.reference_start:,}, Length: {read.query_length}, Quality: {read.mapping_quality}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Pileup Tensors\n",
    "\n",
    "Convert aligned reads to image-like tensors for CNN input.\n",
    "\n",
    "**Pileup representation:**\n",
    "- Each genomic position becomes a row\n",
    "- Each overlapping read becomes a column\n",
    "- Channels encode: base, quality, mapping quality, strand, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding dictionaries\n",
    "BASE_ENCODING = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3, \"N\": 4}\n",
    "STRAND_ENCODING = {\"+\": 0, \"-\": 1}\n",
    "\n",
    "\n",
    "def create_pileup_tensor(bamfile, reference, chrom, start, end, max_depth=100):\n",
    "    \"\"\"\n",
    "    Create pileup tensor for a genomic region.\n",
    "\n",
    "    Args:\n",
    "        bamfile: pysam.AlignmentFile object\n",
    "        reference: Reference sequence (string)\n",
    "        chrom: Chromosome name\n",
    "        start, end: Genomic coordinates (0-based)\n",
    "        max_depth: Maximum read depth to consider\n",
    "\n",
    "    Returns:\n",
    "        tensor: Shape (length, max_depth, n_channels)\n",
    "    \"\"\"\n",
    "    length = end - start\n",
    "    n_channels = 7  # base, base_qual, map_qual, strand, is_match, is_del, is_ins\n",
    "\n",
    "    tensor = np.zeros((length, max_depth, n_channels), dtype=np.float32)\n",
    "\n",
    "    # For each position, collect overlapping reads\n",
    "    for pos_idx, pos in enumerate(range(start, end)):\n",
    "        pileup_column = bamfile.pileup(chrom, pos, pos + 1, truncate=True, max_depth=max_depth)\n",
    "\n",
    "        read_idx = 0\n",
    "        for pileup_col in pileup_column:\n",
    "            for pileup_read in pileup_col.pileups:\n",
    "                if read_idx >= max_depth:\n",
    "                    break\n",
    "\n",
    "                read = pileup_read.alignment\n",
    "\n",
    "                # Channel 0: Base encoding\n",
    "                if not pileup_read.is_del and not pileup_read.is_refskip:\n",
    "                    base = read.query_sequence[pileup_read.query_position]\n",
    "                    tensor[pos_idx, read_idx, 0] = BASE_ENCODING.get(base, 4) / 4.0\n",
    "\n",
    "                    # Channel 1: Base quality\n",
    "                    base_qual = read.query_qualities[pileup_read.query_position]\n",
    "                    tensor[pos_idx, read_idx, 1] = base_qual / 40.0  # Normalize by typical max\n",
    "\n",
    "                # Channel 2: Mapping quality\n",
    "                tensor[pos_idx, read_idx, 2] = read.mapping_quality / 60.0\n",
    "\n",
    "                # Channel 3: Strand\n",
    "                tensor[pos_idx, read_idx, 3] = 0 if read.is_reverse else 1\n",
    "\n",
    "                # Channel 4: Is match to reference\n",
    "                ref_base = reference[pos - start]\n",
    "                if not pileup_read.is_del and not pileup_read.is_refskip:\n",
    "                    read_base = read.query_sequence[pileup_read.query_position]\n",
    "                    tensor[pos_idx, read_idx, 4] = 1.0 if read_base == ref_base else 0.0\n",
    "\n",
    "                # Channel 5: Is deletion\n",
    "                tensor[pos_idx, read_idx, 5] = 1.0 if pileup_read.is_del else 0.0\n",
    "\n",
    "                # Channel 6: Is insertion (simplified)\n",
    "                tensor[pos_idx, read_idx, 6] = 1.0 if pileup_read.indel > 0 else 0.0\n",
    "\n",
    "                read_idx += 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "print(\"✓ Pileup tensor generation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference sequence\n",
    "reference_file = pysam.FastaFile(str(data_dir / \"chr20.fa\"))\n",
    "\n",
    "# Generate a sample pileup tensor\n",
    "test_region = (10000000, 10000221)  # 221bp window\n",
    "ref_seq = reference_file.fetch(\"20\", test_region[0], test_region[1])\n",
    "sample_tensor = create_pileup_tensor(\n",
    "    bamfile, ref_seq, \"20\", test_region[0], test_region[1], max_depth=100\n",
    ")\n",
    "\n",
    "print(f\"Sample pileup tensor shape: {sample_tensor.shape}\")\n",
    "print(\n",
    "    f\"  Position x Read Depth x Channels: {sample_tensor.shape[0]} x {sample_tensor.shape[1]} x {sample_tensor.shape[2]}\"\n",
    ")\n",
    "\n",
    "# Visualize pileup (base channel only)\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(sample_tensor[:, :, 0].T, aspect=\"auto\", cmap=\"viridis\", interpolation=\"none\")\n",
    "plt.colorbar(label=\"Base Encoding\")\n",
    "plt.xlabel(\"Genomic Position\")\n",
    "plt.ylabel(\"Read Depth\")\n",
    "plt.title(\"Pileup Visualization (Base Channel)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Truth Set Labels\n",
    "\n",
    "Download GIAB (Genome in a Bottle) high-confidence variant calls for NA12878.\n",
    "\n",
    "**Note:** This is a simulated section. Real implementation would download GIAB VCF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real implementation, download GIAB truth set:\n",
    "# GIAB_URL = \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/NA12878_HG001/latest/GRCh37/\"\n",
    "# truth_vcf = \"HG001_GRCh37_1_22_v4.2.1_benchmark.vcf.gz\"\n",
    "\n",
    "# For this demo, create synthetic labels\n",
    "def generate_training_data(bamfile, reference_file, chrom, regions, window_size=221):\n",
    "    \"\"\"\n",
    "    Generate training examples from BAM file.\n",
    "\n",
    "    In real implementation, would use GIAB truth set for labels.\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for start, end in tqdm(regions, desc=\"Generating training data\"):\n",
    "        for pos in range(start, end - window_size, window_size // 2):  # 50% overlap\n",
    "            window_end = pos + window_size\n",
    "\n",
    "            # Get reference sequence\n",
    "            ref_seq = reference_file.fetch(chrom, pos, window_end)\n",
    "\n",
    "            # Create pileup tensor\n",
    "            tensor = create_pileup_tensor(bamfile, ref_seq, chrom, pos, window_end)\n",
    "\n",
    "            # Generate labels (simplified - real version uses GIAB VCF)\n",
    "            # Label: 1 if variant, 0 if reference\n",
    "            # In practice, this would come from truth VCF\n",
    "            label = np.random.binomial(1, 0.001, window_size)  # ~0.1% variant rate\n",
    "\n",
    "            X_train.append(tensor)\n",
    "            y_train.append(label)\n",
    "\n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "\n",
    "# Define training and validation regions\n",
    "train_regions = [\n",
    "    (10000000, 12000000),  # 2Mb for training\n",
    "]\n",
    "\n",
    "val_regions = [\n",
    "    (12000000, 13000000),  # 1Mb for validation\n",
    "]\n",
    "\n",
    "print(\"⚠️  NOTE: This demo uses simulated labels.\")\n",
    "print(\"    Real implementation would use GIAB high-confidence variant calls.\")\n",
    "print(\"\\nGenerating training data (this will take 10-15 minutes)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "X_train, y_train = generate_training_data(\n",
    "    bamfile, reference_file, \"20\", train_regions, window_size=221\n",
    ")\n",
    "\n",
    "X_val, y_val = generate_training_data(bamfile, reference_file, \"20\", val_regions, window_size=221)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Validation labels shape: {y_val.shape}\")\n",
    "print(f\"\\nVariant rate in training data: {y_train.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build CNN Variant Caller\n",
    "\n",
    "Create a convolutional neural network architecture for variant calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_variant_caller_cnn(input_shape=(221, 100, 7)):\n",
    "    \"\"\"\n",
    "    Build CNN variant caller model.\n",
    "\n",
    "    Architecture inspired by DeepVariant (Poplin et al. 2018).\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            # Input layer\n",
    "            keras.layers.Input(shape=input_shape),\n",
    "            # Conv block 1\n",
    "            keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.MaxPooling2D((2, 2)),\n",
    "            # Conv block 2\n",
    "            keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.MaxPooling2D((2, 2)),\n",
    "            # Conv block 3\n",
    "            keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.MaxPooling2D((2, 2)),\n",
    "            # Global pooling\n",
    "            keras.layers.GlobalAveragePooling2D(),\n",
    "            # Dense layers\n",
    "            keras.layers.Dense(256, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(128, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            # Output layer (per-position variant probability)\n",
    "            keras.layers.Dense(221, activation=\"sigmoid\"),  # 221 positions\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = build_variant_caller_cnn()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()],\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Train the CNN variant caller (60-75 minutes on GPU, 4-6 hours on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"variant_caller_best.h5\", monitor=\"val_loss\", save_best_only=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"Training CNN variant caller...\")\n",
    "print(\"This will take 60-75 minutes on GPU (4-6 hours on CPU)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history[\"loss\"], label=\"Train\")\n",
    "axes[0, 0].plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history[\"accuracy\"], label=\"Train\")\n",
    "axes[0, 1].plot(history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "axes[0, 1].set_title(\"Accuracy\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history[\"precision\"], label=\"Train\")\n",
    "axes[1, 0].plot(history.history[\"val_precision\"], label=\"Validation\")\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Precision\")\n",
    "axes[1, 0].set_title(\"Precision\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history[\"recall\"], label=\"Train\")\n",
    "axes[1, 1].plot(history.history[\"val_recall\"], label=\"Validation\")\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Recall\")\n",
    "axes[1, 1].set_title(\"Recall\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Performance\n",
    "\n",
    "Assess variant calling accuracy on held-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "y_pred_prob = model.predict(X_val)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Flatten for metrics calculation\n",
    "y_val_flat = y_val.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "y_pred_prob_flat = y_pred_prob.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_val_flat, y_pred_flat, average=\"binary\"\n",
    ")\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"\\nNote: These metrics are based on simulated labels.\")\n",
    "print(\"      Real evaluation would use GIAB truth set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val_flat, y_pred_prob_flat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Variant Calling Performance\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Call Variants on Test Region\n",
    "\n",
    "Apply trained model to call variants in a held-out genomic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test region\n",
    "test_region = (15000000, 15010000)  # 10kb test region\n",
    "\n",
    "# Generate test data\n",
    "X_test, _ = generate_training_data(bamfile, reference_file, \"20\", [test_region], window_size=221)\n",
    "\n",
    "# Call variants\n",
    "print(f\"Calling variants in region chr20:{test_region[0]:,}-{test_region[1]:,}\")\n",
    "variant_probs = model.predict(X_test)\n",
    "\n",
    "# Identify high-confidence variants (prob > 0.9)\n",
    "high_conf_threshold = 0.9\n",
    "variants = []\n",
    "\n",
    "for window_idx, window_probs in enumerate(variant_probs):\n",
    "    window_start = test_region[0] + window_idx * (221 // 2)  # 50% overlap\n",
    "\n",
    "    for pos_offset, prob in enumerate(window_probs):\n",
    "        if prob > high_conf_threshold:\n",
    "            pos = window_start + pos_offset\n",
    "            variants.append({\"chrom\": \"20\", \"pos\": pos, \"prob\": prob})\n",
    "\n",
    "# Convert to DataFrame\n",
    "variants_df = pd.DataFrame(variants)\n",
    "\n",
    "if len(variants_df) > 0:\n",
    "    # Remove duplicates (from overlapping windows)\n",
    "    variants_df = variants_df.drop_duplicates(subset=[\"pos\"]).sort_values(\"pos\")\n",
    "\n",
    "    print(f\"\\n✓ Called {len(variants_df)} high-confidence variants\")\n",
    "    print(\"\\nTop 10 variants:\")\n",
    "    print(variants_df.head(10))\n",
    "else:\n",
    "    print(\"\\nNo high-confidence variants found in test region.\")\n",
    "    print(\"(This is expected with simulated labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "- Downloaded 1.5GB of 1000 Genomes data\n",
    "- Generated pileup tensors from BAM files\n",
    "- Trained a CNN variant caller (60-75 min)\n",
    "- Evaluated performance on validation set\n",
    "- Called variants on held-out region\n",
    "\n",
    "### Limitations in Colab\n",
    "\n",
    "You may have noticed:\n",
    "- **20-minute download** at every session start\n",
    "- **Training time close to timeout** (75 min)\n",
    "- **Can't save model** between sessions\n",
    "- **Limited to small regions** (~11GB RAM limit)\n",
    "\n",
    "### Tier 1: Studio Lab (Multi-Cohort Analysis)\n",
    "\n",
    "Upgrade to [Tier 1](../tier-1/) for:\n",
    "- **8-12GB cached data** (download once, persist forever)\n",
    "- **Multi-sample ensemble callers** (5-6 hour training)\n",
    "- **Model checkpointing** (resume training)\n",
    "- **Population analysis** (multiple individuals)\n",
    "- **No session timeouts**\n",
    "\n",
    "### Real Research Applications\n",
    "\n",
    "This approach (deep learning for variant calling) is used in:\n",
    "- **Clinical diagnostics:** Rare disease variant identification\n",
    "- **Cancer genomics:** Somatic mutation detection\n",
    "- **Population genetics:** Large-scale GWAS studies\n",
    "- **Agricultural genomics:** Crop improvement\n",
    "\n",
    "### References\n",
    "\n",
    "- Poplin et al. (2018) \"A universal SNP and small-indel variant caller using deep neural networks\" *Nature Biotechnology* 36:983-987\n",
    "- 1000 Genomes Project Consortium (2015) *Nature* 526:68-74\n",
    "- Genome in a Bottle Consortium: https://www.nist.gov/programs-projects/genome-bottle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clean Up\n",
    "\n",
    "Close file handles to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamfile.close()\n",
    "reference_file.close()\n",
    "print(\"✓ Resources cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
