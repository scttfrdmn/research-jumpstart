{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic Variant Analysis with AWS Lambda and DynamoDB\n",
    "\n",
    "**Duration:** 60-90 minutes\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Uploading BAM files to S3\n",
    "2. Triggering Lambda functions for variant calling\n",
    "3. Querying results from DynamoDB\n",
    "4. Visualizing variant statistics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured in `.env`\n",
    "- All AWS resources created (see `../setup_guide.md`)\n",
    "- Python dependencies installed (`pip install -r ../requirements.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# AWS configuration\n",
    "AWS_REGION = os.getenv('AWS_REGION', 'us-east-1')\n",
    "BUCKET_INPUT = os.getenv('BUCKET_INPUT')\n",
    "BUCKET_RESULTS = os.getenv('BUCKET_RESULTS')\n",
    "TABLE_NAME = os.getenv('TABLE_NAME', 'variant-metadata')\n",
    "LAMBDA_FUNCTION = os.getenv('LAMBDA_FUNCTION', 'variant-calling')\n",
    "\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print(f\"Input Bucket: {BUCKET_INPUT}\")\n",
    "print(f\"Results Bucket: {BUCKET_RESULTS}\")\n",
    "print(f\"DynamoDB Table: {TABLE_NAME}\")\n",
    "print(f\"Lambda Function: {LAMBDA_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Sample Data to S3\n",
    "\n",
    "First, we'll upload BAM files and reference data to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# List what's already in the bucket\n",
    "print(\"Current S3 bucket contents:\")\n",
    "response = s3_client.list_objects_v2(Bucket=BUCKET_INPUT, MaxKeys=100)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        size_mb = obj['Size'] / (1024 * 1024)\n",
    "        print(f\"  {obj['Key']:<50} {size_mb:>10.2f} MB\")\n",
    "else:\n",
    "    print(\"  Bucket is empty. Run upload_to_s3.py to add sample data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload sample data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If bucket is empty, run upload script\n",
    "# Uncomment the line below to run the upload script\n",
    "\n",
    "# import subprocess\n",
    "# subprocess.run(['python', '../scripts/upload_to_s3.py', '--synthetic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Invoke Lambda Function\n",
    "\n",
    "Now we'll trigger the Lambda function to call variants on a specific genomic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lambda client\n",
    "lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "\n",
    "# Define the region to analyze\n",
    "genomic_region = 'chr20:1000000-1010000'\n",
    "sample_id = 'NA12878'\n",
    "\n",
    "# Create the event payload\n",
    "event = {\n",
    "    'bucket': BUCKET_INPUT,\n",
    "    'key': 'samples/NA12878.chr20.bam',  # Adjust if your file has different name\n",
    "    'region': genomic_region,\n",
    "    'sample_id': sample_id\n",
    "}\n",
    "\n",
    "print(f\"Invoking Lambda function: {LAMBDA_FUNCTION}\")\n",
    "print(f\"Event payload: {json.dumps(event, indent=2)}\")\n",
    "print()\n",
    "print(\"Calling variant-calling Lambda function...\")\n",
    "\n",
    "# Invoke Lambda\n",
    "try:\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=LAMBDA_FUNCTION,\n",
    "        InvocationType='RequestResponse',  # Wait for response\n",
    "        Payload=json.dumps(event)\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    payload = json.loads(response['Payload'].read())\n",
    "    \n",
    "    print(f\"Response: {json.dumps(payload, indent=2)}\")\n",
    "    \n",
    "    # Extract results\n",
    "    if payload.get('statusCode') == 200:\n",
    "        result = payload.get('body', {})\n",
    "        print(f\"\\n✓ Lambda execution successful!\")\n",
    "        print(f\"  Variants called: {result.get('variants', 0)}\")\n",
    "        print(f\"  VCF file: {result.get('vcf_file', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Lambda execution failed\")\n",
    "        print(f\"  Error: {payload.get('body', {}).get('error', 'Unknown')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error invoking Lambda: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Query DynamoDB for Variant Metadata\n",
    "\n",
    "After Lambda execution, variant metadata is stored in DynamoDB. Let's query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DynamoDB resource\n",
    "dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION)\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "print(f\"Querying DynamoDB table: {TABLE_NAME}\")\n",
    "print(f\"Sample: {sample_id}\")\n",
    "print()\n",
    "\n",
    "# Query DynamoDB\n",
    "try:\n",
    "    response = table.scan(\n",
    "        FilterExpression='sample_id = :sample',\n",
    "        ExpressionAttributeValues={':sample': sample_id}\n",
    "    )\n",
    "    \n",
    "    items = response.get('Items', [])\n",
    "    \n",
    "    # Handle pagination\n",
    "    while 'LastEvaluatedKey' in response:\n",
    "        response = table.scan(\n",
    "            FilterExpression='sample_id = :sample',\n",
    "            ExpressionAttributeValues={':sample': sample_id},\n",
    "            ExclusiveStartKey=response['LastEvaluatedKey']\n",
    "        )\n",
    "        items.extend(response.get('Items', []))\n",
    "    \n",
    "    print(f\"Retrieved {len(items)} variants from DynamoDB\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if items:\n",
    "        df_variants = pd.DataFrame(items)\n",
    "        print(f\"\\nVariants DataFrame shape: {df_variants.shape}\")\n",
    "        print(f\"\\nFirst few variants:\")\n",
    "        print(df_variants[['chromosome', 'position', 'ref', 'alt', 'quality', 'depth']].head())\n",
    "    else:\n",
    "        print(\"No variants found in DynamoDB. Check Lambda logs for errors.\")\n",
    "        df_variants = pd.DataFrame()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error querying DynamoDB: {str(e)}\")\n",
    "    df_variants = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Variant Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_variants.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Variant Summary Statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nTotal variants: {len(df_variants)}\")\n",
    "    print(f\"Unique chromosomes: {df_variants['chromosome'].nunique()}\")\n",
    "    \n",
    "    # Variant types\n",
    "    df_variants['var_type'] = df_variants.apply(\n",
    "        lambda x: 'SNP' if len(x['ref']) == len(x['alt']) else 'INDEL',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nVariant types:\")\n",
    "    print(df_variants['var_type'].value_counts())\n",
    "    \n",
    "    # Quality statistics\n",
    "    print(f\"\\nQuality metrics:\")\n",
    "    print(f\"  Mean: {df_variants['quality'].mean():.2f}\")\n",
    "    print(f\"  Median: {df_variants['quality'].median():.2f}\")\n",
    "    print(f\"  Min: {df_variants['quality'].min():.2f}\")\n",
    "    print(f\"  Max: {df_variants['quality'].max():.2f}\")\n",
    "    \n",
    "    # Depth statistics\n",
    "    print(f\"\\nDepth metrics:\")\n",
    "    print(f\"  Mean: {df_variants['depth'].mean():.2f}\")\n",
    "    print(f\"  Median: {df_variants['depth'].median():.2f}\")\n",
    "    print(f\"  Min: {df_variants['depth'].min():.2f}\")\n",
    "    print(f\"  Max: {df_variants['depth'].max():.2f}\")\n",
    "    \n",
    "    # Allele frequency statistics\n",
    "    print(f\"\\nAllele frequency metrics:\")\n",
    "    print(f\"  Mean: {df_variants['allele_freq'].mean():.4f}\")\n",
    "    print(f\"  Median: {df_variants['allele_freq'].median():.4f}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No data available for statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_variants.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Quality distribution\n",
    "    axes[0, 0].hist(df_variants['quality'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Quality Score')\n",
    "    axes[0, 0].set_ylabel('Number of Variants')\n",
    "    axes[0, 0].set_title('Quality Score Distribution')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Depth distribution\n",
    "    axes[0, 1].hist(df_variants['depth'], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[0, 1].set_xlabel('Depth (Read Coverage)')\n",
    "    axes[0, 1].set_ylabel('Number of Variants')\n",
    "    axes[0, 1].set_title('Depth Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Variant types\n",
    "    var_type_counts = df_variants['var_type'].value_counts()\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    axes[1, 0].bar(var_type_counts.index, var_type_counts.values, color=colors, edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Variant Type')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('Variant Type Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Quality vs Allele Frequency\n",
    "    scatter = axes[1, 1].scatter(df_variants['allele_freq'], df_variants['quality'], \n",
    "                                  c=df_variants['depth'], cmap='viridis', s=100, alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('Allele Frequency')\n",
    "    axes[1, 1].set_ylabel('Quality Score')\n",
    "    axes[1, 1].set_title('Quality vs Allele Frequency (colored by depth)')\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 1])\n",
    "    cbar.set_label('Depth')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results_visualization.png', dpi=100, bbox_inches='tight')\n",
    "    print(\"✓ Visualization saved to results_visualization.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download VCF Results from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List VCF files in results bucket\n",
    "print(f\"Listing VCF files in s3://{BUCKET_RESULTS}/results/\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=BUCKET_RESULTS,\n",
    "        Prefix='results/'\n",
    "    )\n",
    "    \n",
    "    vcf_files = []\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            if obj['Key'].endswith('.vcf'):\n",
    "                vcf_files.append(obj['Key'])\n",
    "                size_kb = obj['Size'] / 1024\n",
    "                print(f\"  {obj['Key']:<60} {size_kb:>10.2f} KB\")\n",
    "    \n",
    "    if not vcf_files:\n",
    "        print(\"  No VCF files found. Check Lambda logs for errors.\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Found {len(vcf_files)} VCF file(s)\")\n",
    "        \n",
    "        # Download the first VCF\n",
    "        vcf_key = vcf_files[0]\n",
    "        local_vcf_path = f\"../downloads/{Path(vcf_key).name}\"\n",
    "        \n",
    "        print(f\"\\nDownloading: {vcf_key}\")\n",
    "        os.makedirs('../downloads', exist_ok=True)\n",
    "        s3_client.download_file(BUCKET_RESULTS, vcf_key, local_vcf_path)\n",
    "        print(f\"✓ Downloaded to: {local_vcf_path}\")\n",
    "        \n",
    "        # Display VCF content\n",
    "        print(f\"\\nVCF file contents:\")\n",
    "        with open(local_vcf_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[:20]:  # Show first 20 lines\n",
    "                print(line.rstrip())\n",
    "            if len(lines) > 20:\n",
    "                print(f\"... ({len(lines) - 20} more lines)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_variants.empty:\n",
    "    # Export to CSV\n",
    "    output_csv = f\"../variant_results_{sample_id}.csv\"\n",
    "    df_variants.to_csv(output_csv, index=False)\n",
    "    print(f\"✓ Exported {len(df_variants)} variants to {output_csv}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nFirst 5 variants:\")\n",
    "    print(df_variants[['chromosome', 'position', 'ref', 'alt', 'quality', 'depth', 'allele_freq']].head())\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    print(df_variants.info())\n",
    "else:\n",
    "    print(\"No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Quality and Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_variants.empty:\n",
    "    # Create quality categories\n",
    "    df_variants['quality_category'] = pd.cut(\n",
    "        df_variants['quality'],\n",
    "        bins=[0, 20, 40, 60, 100],\n",
    "        labels=['Low', 'Medium', 'High', 'Very High']\n",
    "    )\n",
    "    \n",
    "    print(\"Quality categories:\")\n",
    "    print(df_variants['quality_category'].value_counts().sort_index())\n",
    "    print()\n",
    "    \n",
    "    # Create depth categories\n",
    "    df_variants['depth_category'] = pd.cut(\n",
    "        df_variants['depth'],\n",
    "        bins=[0, 10, 20, 50, 1000],\n",
    "        labels=['Low', 'Medium', 'High', 'Very High']\n",
    "    )\n",
    "    \n",
    "    print(\"Depth categories:\")\n",
    "    print(df_variants['depth_category'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not df_variants.empty:\n",
    "    print(f\"\\nResults Summary:\")\n",
    "    print(f\"  Sample: {sample_id}\")\n",
    "    print(f\"  Region: {genomic_region}\")\n",
    "    print(f\"  Variants Found: {len(df_variants)}\")\n",
    "    print(f\"  SNPs: {(df_variants['var_type'] == 'SNP').sum()}\")\n",
    "    print(f\"  Indels: {(df_variants['var_type'] == 'INDEL').sum()}\")\n",
    "    print(f\"  Mean Quality: {df_variants['quality'].mean():.2f}\")\n",
    "    print(f\"  Mean Depth: {df_variants['depth'].mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nOutputs:\")\n",
    "    print(f\"  - CSV: ../variant_results_{sample_id}.csv\")\n",
    "    print(f\"  - Visualization: ../results_visualization.png\")\n",
    "    print(f\"  - VCF: ../downloads/\")\n",
    "else:\n",
    "    print(f\"\\nNo variants found. Check:\")\n",
    "    print(f\"  1. Lambda logs in CloudWatch\")\n",
    "    print(f\"  2. S3 bucket contains BAM files\")\n",
    "    print(f\"  3. DynamoDB table exists and is accessible\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Review results in CSV and visualizations\")\n",
    "print(f\"  2. Download VCF files for further analysis\")\n",
    "print(f\"  3. Run cleanup_guide.md to delete AWS resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
