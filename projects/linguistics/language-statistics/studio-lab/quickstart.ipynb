{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics: Statistical Analysis of Natural Language\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook explores fundamental concepts in **statistical linguistics** and **quantitative language analysis**. We'll analyze a multilingual dataset containing parallel content across four languages to uncover universal patterns and language-specific characteristics.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "Our dataset contains text samples from four languages:\n",
    "- **English** (Germanic, Latin script)\n",
    "- **Spanish** (Romance, Latin script)\n",
    "- **German** (Germanic, Latin script)\n",
    "- **Japanese** (Japonic, mixed script - Kanji, Hiragana, Katakana)\n",
    "\n",
    "### Statistical Linguistics Methods\n",
    "\n",
    "We'll explore several key concepts:\n",
    "\n",
    "1. **Zipf's Law**: A fundamental observation that word frequency is inversely proportional to rank (frequency ∝ 1/rank)\n",
    "2. **Lexical Diversity**: Measures of vocabulary richness including Type-Token Ratio (TTR)\n",
    "3. **Power Law Distribution**: Mathematical relationship describing word frequency distributions\n",
    "4. **N-gram Analysis**: Patterns in consecutive word sequences\n",
    "5. **Cross-linguistic Comparison**: Universal vs language-specific statistical properties\n",
    "\n",
    "These methods form the foundation of computational linguistics, corpus linguistics, and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "We'll use standard scientific Python libraries along with NLTK for natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Natural language processing\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Multilingual Dataset\n",
    "\n",
    "First, we'll load the CSV file containing texts from four languages and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the multilingual dataset\n",
    "df = pd.read_csv('../data/multilingual_texts.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nLanguages in dataset:\", df['language'].unique())\n",
    "print(\"\\nRows per language:\")\n",
    "print(df['language'].value_counts())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(\"\\nText length statistics by language:\")\n",
    "df.groupby('language')['text_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization: Breaking Text into Words\n",
    "\n",
    "**Tokenization** is the process of splitting text into individual units (tokens), typically words. This is the foundation for most linguistic analysis.\n",
    "\n",
    "### Challenges:\n",
    "- **Latin scripts** (English, Spanish, German): Words separated by spaces\n",
    "- **Japanese**: No word boundaries, mixed scripts (Kanji, Hiragana, Katakana)\n",
    "\n",
    "We'll use regular expressions for Latin scripts and character-based tokenization for Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, language='english'):\n",
    "    \"\"\"\n",
    "    Tokenize text into words, handling different scripts.\n",
    "    \n",
    "    For Latin scripts: split on whitespace and punctuation\n",
    "    For Japanese: use character-based tokenization (simplified)\n",
    "    \"\"\"\n",
    "    if language.lower() == 'japanese':\n",
    "        # For Japanese, we'll use a simple character-based approach\n",
    "        # In production, use tools like MeCab or SudachiPy\n",
    "        # Here we'll split into characters and filter out punctuation\n",
    "        tokens = [char for char in text if char.strip() and not re.match(r'[\\s\\p{P}]', char)]\n",
    "    else:\n",
    "        # For Latin scripts: lowercase and split on word boundaries\n",
    "        text = text.lower()\n",
    "        # Keep only alphabetic characters and spaces\n",
    "        text = re.sub(r'[^a-záéíóúñü\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "    \n",
    "    return [t for t in tokens if t]  # Remove empty strings\n",
    "\n",
    "# Apply tokenization to each language\n",
    "df['tokens'] = df.apply(lambda row: tokenize_text(row['text'], row['language']), axis=1)\n",
    "df['token_count'] = df['tokens'].apply(len)\n",
    "\n",
    "# Display results\n",
    "print(\"Token count statistics by language:\")\n",
    "print(df.groupby('language')['token_count'].describe())\n",
    "\n",
    "# Show example tokenization\n",
    "print(\"\\nExample tokenization for each language:\")\n",
    "for lang in df['language'].unique():\n",
    "    sample = df[df['language'] == lang].iloc[0]\n",
    "    print(f\"\\n{lang}:\")\n",
    "    print(f\"Original: {sample['text'][:100]}...\")\n",
    "    print(f\"Tokens (first 10): {sample['tokens'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis\n",
    "\n",
    "**Word frequency** is a fundamental measure in linguistics. The most frequent words in any language tend to be **function words** (articles, prepositions, conjunctions) while **content words** (nouns, verbs, adjectives) are less frequent but carry more semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequencies for each language\n",
    "language_frequencies = {}\n",
    "\n",
    "for lang in df['language'].unique():\n",
    "    # Combine all tokens for this language\n",
    "    all_tokens = []\n",
    "    for tokens in df[df['language'] == lang]['tokens']:\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count frequencies\n",
    "    freq_counter = Counter(all_tokens)\n",
    "    language_frequencies[lang] = freq_counter\n",
    "    \n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    print(f\"Total tokens: {len(all_tokens):,}\")\n",
    "    print(f\"Unique tokens (vocabulary size): {len(freq_counter):,}\")\n",
    "    print(f\"\\nTop 10 most frequent words:\")\n",
    "    for word, count in freq_counter.most_common(10):\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words per language\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lang in enumerate(sorted(df['language'].unique())):\n",
    "    freq_counter = language_frequencies[lang]\n",
    "    top_words = freq_counter.most_common(15)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    axes[idx].barh(range(len(words)), counts, color=sns.color_palette(\"husl\", 4)[idx])\n",
    "    axes[idx].set_yticks(range(len(words)))\n",
    "    axes[idx].set_yticklabels(words)\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].set_xlabel('Frequency', fontsize=12)\n",
    "    axes[idx].set_title(f'Top 15 Words in {lang}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word_frequencies.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: High-frequency words are typically function words (articles, prepositions, pronouns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law: The Power Law of Language\n",
    "\n",
    "**Zipf's Law** is one of the most remarkable discoveries in quantitative linguistics. It states that:\n",
    "\n",
    "> The frequency of any word is inversely proportional to its rank in the frequency table.\n",
    "\n",
    "Mathematically: `frequency ∝ 1/rank^α` where α ≈ 1\n",
    "\n",
    "On a log-log plot, this creates a **straight line**, demonstrating a **power law distribution**.\n",
    "\n",
    "### Implications:\n",
    "- A few words are extremely common\n",
    "- Most words are rare (**hapax legomena**: words appearing only once)\n",
    "- This pattern is universal across human languages\n",
    "- Important for compression, information theory, and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rank-frequency data for each language\n",
    "zipf_data = {}\n",
    "\n",
    "for lang in df['language'].unique():\n",
    "    freq_counter = language_frequencies[lang]\n",
    "    \n",
    "    # Get frequencies in descending order\n",
    "    frequencies = [count for word, count in freq_counter.most_common()]\n",
    "    ranks = np.arange(1, len(frequencies) + 1)\n",
    "    \n",
    "    zipf_data[lang] = {'ranks': ranks, 'frequencies': frequencies}\n",
    "    \n",
    "    # Calculate theoretical Zipf distribution\n",
    "    # frequency = C / rank (where C is a constant)\n",
    "    C = frequencies[0]  # Frequency of most common word\n",
    "    theoretical_freq = C / ranks\n",
    "    zipf_data[lang]['theoretical'] = theoretical_freq\n",
    "    \n",
    "    # Calculate correlation in log space\n",
    "    log_ranks = np.log10(ranks)\n",
    "    log_freqs = np.log10(frequencies)\n",
    "    correlation = np.corrcoef(log_ranks, log_freqs)[0, 1]\n",
    "    \n",
    "    print(f\"{lang}: Zipf correlation (log-log) = {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zipf's Law on log-log scale\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "colors = sns.color_palette(\"husl\", 4)\n",
    "\n",
    "for idx, lang in enumerate(sorted(df['language'].unique())):\n",
    "    ranks = zipf_data[lang]['ranks']\n",
    "    frequencies = zipf_data[lang]['frequencies']\n",
    "    theoretical = zipf_data[lang]['theoretical']\n",
    "    \n",
    "    # Plot observed frequencies\n",
    "    axes[idx].loglog(ranks, frequencies, 'o', alpha=0.6, markersize=4, \n",
    "                     color=colors[idx], label='Observed')\n",
    "    \n",
    "    # Plot theoretical Zipf distribution\n",
    "    axes[idx].loglog(ranks, theoretical, '--', linewidth=2, \n",
    "                     color='red', label='Theoretical Zipf (1/rank)')\n",
    "    \n",
    "    axes[idx].set_xlabel('Rank (log scale)', fontsize=12)\n",
    "    axes[idx].set_ylabel('Frequency (log scale)', fontsize=12)\n",
    "    axes[idx].set_title(f\"Zipf's Law: {lang}\", fontsize=14, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('zipfs_law.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nZipf's Law demonstrates a power law: straight line on log-log plot\")\n",
    "print(\"This universal pattern appears in all human languages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Zipf exponent (slope) for each language using linear regression in log space\n",
    "print(\"Zipf's Law Exponent Analysis:\")\n",
    "print(\"\\nTheoretical Zipf exponent α = 1.0\")\n",
    "print(\"Actual values often range from 0.8 to 1.2\\n\")\n",
    "\n",
    "for lang in sorted(df['language'].unique()):\n",
    "    ranks = zipf_data[lang]['ranks']\n",
    "    frequencies = zipf_data[lang]['frequencies']\n",
    "    \n",
    "    # Linear regression in log space\n",
    "    log_ranks = np.log10(ranks)\n",
    "    log_freqs = np.log10(frequencies)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(log_ranks, log_freqs)\n",
    "    \n",
    "    print(f\"{lang}:\")\n",
    "    print(f\"  Exponent α: {-slope:.3f}\")\n",
    "    print(f\"  R² (fit quality): {r_value**2:.4f}\")\n",
    "    print(f\"  Equation: frequency = 10^{intercept:.2f} × rank^{slope:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity: Measuring Vocabulary Richness\n",
    "\n",
    "**Lexical diversity** (or vocabulary richness) measures how varied the vocabulary is in a text.\n",
    "\n",
    "### Type-Token Ratio (TTR):\n",
    "```\n",
    "TTR = (Number of unique words) / (Total number of words)\n",
    "TTR = Types / Tokens\n",
    "```\n",
    "\n",
    "- **High TTR** (closer to 1.0): Rich, varied vocabulary\n",
    "- **Low TTR** (closer to 0): Repetitive, limited vocabulary\n",
    "\n",
    "### Related Concepts:\n",
    "- **Types**: Unique words in a text\n",
    "- **Tokens**: Total words (including repetitions)\n",
    "- **Hapax legomena**: Words appearing exactly once\n",
    "- **Hapax dislegomena**: Words appearing exactly twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lexical diversity metrics for each language\n",
    "lexical_diversity = {}\n",
    "\n",
    "for lang in df['language'].unique():\n",
    "    freq_counter = language_frequencies[lang]\n",
    "    \n",
    "    # Basic counts\n",
    "    total_tokens = sum(freq_counter.values())\n",
    "    unique_types = len(freq_counter)\n",
    "    \n",
    "    # Type-Token Ratio\n",
    "    ttr = unique_types / total_tokens\n",
    "    \n",
    "    # Hapax legomena (words appearing once)\n",
    "    hapax = sum(1 for count in freq_counter.values() if count == 1)\n",
    "    hapax_percentage = (hapax / unique_types) * 100\n",
    "    \n",
    "    # Hapax dislegomena (words appearing twice)\n",
    "    dis_legomena = sum(1 for count in freq_counter.values() if count == 2)\n",
    "    \n",
    "    # Store results\n",
    "    lexical_diversity[lang] = {\n",
    "        'tokens': total_tokens,\n",
    "        'types': unique_types,\n",
    "        'ttr': ttr,\n",
    "        'hapax': hapax,\n",
    "        'hapax_pct': hapax_percentage,\n",
    "        'dis_legomena': dis_legomena\n",
    "    }\n",
    "    \n",
    "    print(f\"{lang.upper()}:\")\n",
    "    print(f\"  Total tokens: {total_tokens:,}\")\n",
    "    print(f\"  Unique types: {unique_types:,}\")\n",
    "    print(f\"  Type-Token Ratio: {ttr:.4f}\")\n",
    "    print(f\"  Hapax legomena: {hapax:,} ({hapax_percentage:.1f}% of vocabulary)\")\n",
    "    print(f\"  Hapax dislegomena: {dis_legomena:,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize lexical diversity metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "languages = sorted(df['language'].unique())\n",
    "colors = sns.color_palette(\"husl\", len(languages))\n",
    "\n",
    "# Plot 1: Type-Token Ratio\n",
    "ttrs = [lexical_diversity[lang]['ttr'] for lang in languages]\n",
    "axes[0].bar(languages, ttrs, color=colors)\n",
    "axes[0].set_ylabel('Type-Token Ratio', fontsize=12)\n",
    "axes[0].set_title('Lexical Diversity (TTR)\\nHigher = More Varied Vocabulary', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, max(ttrs) * 1.2)\n",
    "for i, v in enumerate(ttrs):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Vocabulary size\n",
    "vocab_sizes = [lexical_diversity[lang]['types'] for lang in languages]\n",
    "axes[1].bar(languages, vocab_sizes, color=colors)\n",
    "axes[1].set_ylabel('Vocabulary Size (Types)', fontsize=12)\n",
    "axes[1].set_title('Total Unique Words', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(vocab_sizes):\n",
    "    axes[1].text(i, v + max(vocab_sizes)*0.02, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Hapax legomena percentage\n",
    "hapax_pcts = [lexical_diversity[lang]['hapax_pct'] for lang in languages]\n",
    "axes[2].bar(languages, hapax_pcts, color=colors)\n",
    "axes[2].set_ylabel('Hapax Legomena (%)', fontsize=12)\n",
    "axes[2].set_title('Words Appearing Only Once\\n(% of Vocabulary)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(hapax_pcts):\n",
    "    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lexical_diversity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher TTR indicates more diverse vocabulary usage\")\n",
    "print(\"- Hapax legomena are important for vocabulary growth and creativity\")\n",
    "print(\"- Typically 30-50% of words in a corpus appear only once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Analysis: Discovering Common Phrases\n",
    "\n",
    "**N-grams** are contiguous sequences of n items (words) from a text.\n",
    "\n",
    "- **Unigram** (1-gram): Single word\n",
    "- **Bigram** (2-gram): Two consecutive words\n",
    "- **Trigram** (3-gram): Three consecutive words\n",
    "\n",
    "### Applications:\n",
    "- Identify common phrases and collocations\n",
    "- Language modeling and prediction\n",
    "- Text generation\n",
    "- Statistical machine translation\n",
    "- Phrase detection in search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens, n=2):\n",
    "    \"\"\"\n",
    "    Extract n-grams from a list of tokens.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = ' '.join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Calculate bigrams and trigrams for each language\n",
    "ngram_data = {}\n",
    "\n",
    "for lang in df['language'].unique():\n",
    "    # Get all tokens for this language\n",
    "    all_tokens = []\n",
    "    for tokens in df[df['language'] == lang]['tokens']:\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Extract bigrams and trigrams\n",
    "    bigrams = extract_ngrams(all_tokens, n=2)\n",
    "    trigrams = extract_ngrams(all_tokens, n=3)\n",
    "    \n",
    "    # Count frequencies\n",
    "    bigram_counter = Counter(bigrams)\n",
    "    trigram_counter = Counter(trigrams)\n",
    "    \n",
    "    ngram_data[lang] = {\n",
    "        'bigrams': bigram_counter,\n",
    "        'trigrams': trigram_counter\n",
    "    }\n",
    "    \n",
    "    print(f\"{lang.upper()} - Most Common Phrases:\")\n",
    "    print(\"\\nTop 10 Bigrams (2-word phrases):\")\n",
    "    for ngram, count in bigram_counter.most_common(10):\n",
    "        print(f\"  '{ngram}': {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Trigrams (3-word phrases):\")\n",
    "    for ngram, count in trigram_counter.most_common(10):\n",
    "        print(f\"  '{ngram}': {count}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize most common bigrams\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lang in enumerate(sorted(df['language'].unique())):\n",
    "    bigram_counter = ngram_data[lang]['bigrams']\n",
    "    top_bigrams = bigram_counter.most_common(12)\n",
    "    \n",
    "    if top_bigrams:\n",
    "        phrases, counts = zip(*top_bigrams)\n",
    "        \n",
    "        axes[idx].barh(range(len(phrases)), counts, color=sns.color_palette(\"husl\", 4)[idx])\n",
    "        axes[idx].set_yticks(range(len(phrases)))\n",
    "        axes[idx].set_yticklabels(phrases, fontsize=10)\n",
    "        axes[idx].invert_yaxis()\n",
    "        axes[idx].set_xlabel('Frequency', fontsize=12)\n",
    "        axes[idx].set_title(f'Top Bigrams: {lang}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bigrams.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Common bigrams often include function words (articles, prepositions)\")\n",
    "print(\"These reveal grammatical patterns specific to each language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-linguistic Comparison\n",
    "\n",
    "Now we'll compare statistical properties across all four languages to identify:\n",
    "1. **Universal patterns** (similar across all languages)\n",
    "2. **Language-specific characteristics** (unique to each language)\n",
    "\n",
    "We'll examine:\n",
    "- Average word length\n",
    "- Vocabulary size\n",
    "- Frequency distribution patterns\n",
    "- Type-token ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average word length for each language\n",
    "comparison_data = {}\n",
    "\n",
    "for lang in df['language'].unique():\n",
    "    all_tokens = []\n",
    "    for tokens in df[df['language'] == lang]['tokens']:\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Word length statistics\n",
    "    word_lengths = [len(word) for word in all_tokens]\n",
    "    avg_length = np.mean(word_lengths)\n",
    "    median_length = np.median(word_lengths)\n",
    "    std_length = np.std(word_lengths)\n",
    "    \n",
    "    # Frequency distribution stats\n",
    "    frequencies = list(language_frequencies[lang].values())\n",
    "    \n",
    "    comparison_data[lang] = {\n",
    "        'avg_word_length': avg_length,\n",
    "        'median_word_length': median_length,\n",
    "        'std_word_length': std_length,\n",
    "        'word_length_distribution': word_lengths,\n",
    "        'frequency_distribution': frequencies\n",
    "    }\n",
    "    \n",
    "    print(f\"{lang}:\")\n",
    "    print(f\"  Average word length: {avg_length:.2f} characters\")\n",
    "    print(f\"  Median word length: {median_length:.1f} characters\")\n",
    "    print(f\"  Std deviation: {std_length:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "colors = sns.color_palette(\"husl\", 4)\n",
    "\n",
    "for idx, lang in enumerate(sorted(df['language'].unique())):\n",
    "    word_lengths = comparison_data[lang]['word_length_distribution']\n",
    "    \n",
    "    axes[idx].hist(word_lengths, bins=30, color=colors[idx], alpha=0.7, edgecolor='black')\n",
    "    axes[idx].axvline(comparison_data[lang]['avg_word_length'], \n",
    "                      color='red', linestyle='--', linewidth=2, \n",
    "                      label=f\"Mean: {comparison_data[lang]['avg_word_length']:.2f}\")\n",
    "    axes[idx].set_xlabel('Word Length (characters)', fontsize=12)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[idx].set_title(f'Word Length Distribution: {lang}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word_length_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vocabulary growth across languages\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for idx, lang in enumerate(sorted(df['language'].unique())):\n",
    "    # Get all tokens for this language\n",
    "    all_tokens = []\n",
    "    for tokens in df[df['language'] == lang]['tokens']:\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Calculate cumulative vocabulary size\n",
    "    vocab_growth = []\n",
    "    seen_words = set()\n",
    "    sample_points = np.linspace(0, len(all_tokens), 100, dtype=int)\n",
    "    \n",
    "    for point in sample_points:\n",
    "        seen_words.update(all_tokens[:point])\n",
    "        vocab_growth.append(len(seen_words))\n",
    "    \n",
    "    plt.plot(sample_points, vocab_growth, marker='o', linewidth=2, \n",
    "             label=lang, color=sns.color_palette(\"husl\", 4)[idx])\n",
    "\n",
    "plt.xlabel('Number of Tokens', fontsize=13)\n",
    "plt.ylabel('Vocabulary Size (Unique Words)', fontsize=13)\n",
    "plt.title('Vocabulary Growth Across Languages\\n(Heaps\\' Law)', \n",
    "          fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('vocabulary_growth.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Heaps' Law: Vocabulary size grows as V = K × N^β\")\n",
    "print(\"where N is text length and β is typically 0.4-0.6\")\n",
    "print(\"This shows diminishing returns in vocabulary growth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics: Comprehensive Language Comparison\n",
    "\n",
    "Let's compile all our findings into a comprehensive comparison table showing the statistical properties of each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "for lang in sorted(df['language'].unique()):\n",
    "    ld = lexical_diversity[lang]\n",
    "    cd = comparison_data[lang]\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    freq_counter = language_frequencies[lang]\n",
    "    \n",
    "    # Top 10 words coverage (what % of text is covered by top 10 words)\n",
    "    top10_freq = sum(count for word, count in freq_counter.most_common(10))\n",
    "    top10_coverage = (top10_freq / ld['tokens']) * 100\n",
    "    \n",
    "    # Top 100 words coverage\n",
    "    top100_freq = sum(count for word, count in freq_counter.most_common(100))\n",
    "    top100_coverage = (top100_freq / ld['tokens']) * 100\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Language': lang,\n",
    "        'Total Tokens': f\"{ld['tokens']:,}\",\n",
    "        'Vocabulary Size': f\"{ld['types']:,}\",\n",
    "        'Type-Token Ratio': f\"{ld['ttr']:.4f}\",\n",
    "        'Avg Word Length': f\"{cd['avg_word_length']:.2f}\",\n",
    "        'Hapax Legomena': f\"{ld['hapax']:,} ({ld['hapax_pct']:.1f}%)\",\n",
    "        'Top 10 Coverage': f\"{top10_coverage:.1f}%\",\n",
    "        'Top 100 Coverage': f\"{top100_coverage:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE STATISTICAL LINGUISTICS SUMMARY\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual summary dashboard\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "languages = sorted(df['language'].unique())\n",
    "colors = sns.color_palette(\"husl\", len(languages))\n",
    "\n",
    "# Plot 1: Total tokens\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "tokens = [lexical_diversity[lang]['tokens'] for lang in languages]\n",
    "ax1.bar(languages, tokens, color=colors)\n",
    "ax1.set_title('Total Tokens', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Vocabulary size\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "types = [lexical_diversity[lang]['types'] for lang in languages]\n",
    "ax2.bar(languages, types, color=colors)\n",
    "ax2.set_title('Vocabulary Size', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('Unique Words')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Type-Token Ratio\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ttrs = [lexical_diversity[lang]['ttr'] for lang in languages]\n",
    "ax3.bar(languages, ttrs, color=colors)\n",
    "ax3.set_title('Type-Token Ratio', fontweight='bold', fontsize=12)\n",
    "ax3.set_ylabel('TTR')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Average word length\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "avg_lengths = [comparison_data[lang]['avg_word_length'] for lang in languages]\n",
    "ax4.bar(languages, avg_lengths, color=colors)\n",
    "ax4.set_title('Average Word Length', fontweight='bold', fontsize=12)\n",
    "ax4.set_ylabel('Characters')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 5: Hapax percentage\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "hapax_pcts = [lexical_diversity[lang]['hapax_pct'] for lang in languages]\n",
    "ax5.bar(languages, hapax_pcts, color=colors)\n",
    "ax5.set_title('Hapax Legomena %', fontweight='bold', fontsize=12)\n",
    "ax5.set_ylabel('Percentage')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 6: Comparison radar chart\n",
    "ax6 = fig.add_subplot(gs[1, 2], projection='polar')\n",
    "\n",
    "# Normalize metrics for radar chart (0-1 scale)\n",
    "metrics = ['TTR', 'Avg Length', 'Hapax %']\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for idx, lang in enumerate(languages):\n",
    "    values = [\n",
    "        lexical_diversity[lang]['ttr'] / max(ttrs),\n",
    "        comparison_data[lang]['avg_word_length'] / max(avg_lengths),\n",
    "        lexical_diversity[lang]['hapax_pct'] / max(hapax_pcts)\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    ax6.plot(angles, values, 'o-', linewidth=2, label=lang, color=colors[idx])\n",
    "    ax6.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax6.set_xticks(angles[:-1])\n",
    "ax6.set_xticklabels(metrics)\n",
    "ax6.set_ylim(0, 1)\n",
    "ax6.set_title('Normalized Metrics', fontweight='bold', fontsize=12, pad=20)\n",
    "ax6.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax6.grid(True)\n",
    "\n",
    "fig.suptitle('Statistical Linguistics: Comprehensive Language Comparison', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('summary_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Conclusions\n",
    "\n",
    "### Universal Patterns (Observed Across All Languages):\n",
    "\n",
    "1. **Zipf's Law Holds**: All languages show power law distribution (straight line on log-log plot)\n",
    "2. **High-Frequency Function Words**: Articles, prepositions, and pronouns dominate\n",
    "3. **Hapax Legomena**: ~30-50% of vocabulary appears only once\n",
    "4. **Vocabulary Growth**: Follows Heaps' Law (sublinear growth)\n",
    "\n",
    "### Language-Specific Observations:\n",
    "\n",
    "- **German**: Longer average word length (compound words)\n",
    "- **Japanese**: Different tokenization requirements (character-based)\n",
    "- **English/Spanish**: Similar TTR and frequency patterns\n",
    "- **Morphology Effects**: Inflected languages may show different TTR\n",
    "\n",
    "### Applications in NLP:\n",
    "\n",
    "1. **Language Modeling**: Frequency distributions inform probability models\n",
    "2. **Text Compression**: Zipf's Law enables efficient encoding\n",
    "3. **Information Retrieval**: TF-IDF based on frequency principles\n",
    "4. **Machine Translation**: N-gram patterns reveal translation units\n",
    "5. **Authorship Attribution**: Lexical diversity as stylistic marker\n",
    "\n",
    "### Further Exploration:\n",
    "\n",
    "- **Pointwise Mutual Information (PMI)**: Measure word association strength\n",
    "- **Perplexity**: Evaluate language model quality\n",
    "- **Entropy**: Measure linguistic uncertainty\n",
    "- **Syllable-based metrics**: For phonological analysis\n",
    "- **Morphological analysis**: Study word formation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPUTATIONAL LINGUISTICS ANALYSIS COMPLETE\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nKey Statistical Concepts Demonstrated:\")\n",
    "print(\"  ✓ Zipf's Law - Power law distribution of word frequencies\")\n",
    "print(\"  ✓ Type-Token Ratio - Lexical diversity measurement\")\n",
    "print(\"  ✓ Hapax Legomena - Rare word analysis\")\n",
    "print(\"  ✓ N-gram Analysis - Phrase detection and collocation\")\n",
    "print(\"  ✓ Heaps' Law - Vocabulary growth patterns\")\n",
    "print(\"  ✓ Cross-linguistic Comparison - Universal vs specific patterns\")\n",
    "print(\"\\nVisualizations Generated:\")\n",
    "print(\"  • Word frequency distributions\")\n",
    "print(\"  • Zipf's law log-log plots\")\n",
    "print(\"  • Lexical diversity metrics\")\n",
    "print(\"  • N-gram frequency charts\")\n",
    "print(\"  • Word length distributions\")\n",
    "print(\"  • Vocabulary growth curves\")\n",
    "print(\"  • Comprehensive summary dashboard\")\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
