# Corpus Linguistics at Scale

Large-scale computational analysis of language across billions of words with frequency analysis, collocation detection, diachronic semantics, and multilingual comparison on cloud infrastructure.

## Quick Start by Tier

**New here?** Start with tier-0 (60-90 min, free) to learn corpus linguistics fundamentals.

### ðŸŸ¢ Tier 0: Corpus Linguistics and Collocations (60-90 min, FREE)
**[Launch tier-0 project â†’](tier-0/)**

Analyze linguistic patterns in large text corpora:
- âœ… Multiple corpora (~1GB: Brown, BNC samples, OpenSubtitles, ~4M words total)
- âœ… Frequency analysis (word/n-gram frequencies, Zipf's law validation)
- âœ… Collocation extraction (PMI, t-score, log-likelihood statistical measures)
- âœ… Concordance analysis (KWIC - Keywords in Context)
- âœ… POS pattern extraction (part-of-speech sequence analysis)
- âœ… Cross-linguistic comparison (English, Spanish, German, French)
- âœ… Complete in 60-90 minutes
- âœ… No AWS account needed (Colab or Studio Lab)

**Platform**: Google Colab or SageMaker Studio Lab
**Cost**: $0

[View tier-0 README â†’](tier-0/README.md) | [Open in Colab â†’](https://colab.research.google.com/github/scttfrdmn/research-jumpstart/blob/main/projects/linguistics/corpus-linguistics/tier-0/corpus-analysis.ipynb)

---

### ðŸŸ¡ Tier 1: Large-Scale Corpus Analysis (4-8 hours, FREE)
**[Launch tier-1 project â†’](tier-1/)**

Comprehensive corpus linguistics with massive datasets:
- âœ… 10-50GB corpora (full BNC 100M words, COCA 1B words, Google Books samples)
- âœ… Diachronic semantic analysis (track meaning changes 1800-2020)
- âœ… Advanced collocation measures (multiple statistical tests, effect sizes)
- âœ… Dialectal variation analysis (compare regional varieties)
- âœ… Multilingual semantics (cross-linguistic comparison across 10+ languages)
- âœ… Persistent storage and indexed corpora (Studio Lab)
- âœ… Still free, no AWS account

**Platform**: SageMaker Studio Lab
**Cost**: $0

[View tier-1 README â†’](tier-1/README.md)

---

### ðŸŸ  Tier 2: Production Corpus Platform (2-3 days, $450-600/month for 1B words)
**[Launch tier-2 project â†’](tier-2/)**

Research-grade corpus linguistics infrastructure:
- âœ… CloudFormation one-click deployment
- âœ… Billion-word corpora on S3 (COCA 1B, Google Books 500B+, Common Crawl)
- âœ… Distributed processing with EMR Spark (n-gram extraction, POS tagging)
- âœ… Elasticsearch for sub-second concordance queries on billion-word corpora
- âœ… SageMaker for diachronic word embeddings (Word2Vec, FastText, BERT alignment)
- âœ… Multilingual NLP pipelines (spaCy, Stanza, UDPipe for 100+ languages)
- âœ… Publication-ready collocation networks and frequency data

**Platform**: AWS with CloudFormation
**Cost**: $450-600/month for 1B-word corpus

[View tier-2 README â†’](tier-2/README.md)

---

### ðŸ”´ Tier 3: Massive-Scale Linguistic Platform (Ongoing, $2.5K-25K/month)
**[Launch tier-3 project â†’](tier-3/)**

Production platform for linguistic research centers:
- âœ… 10B-100B+ word corpora (Google Books, Common Crawl, social media archives)
- âœ… Distributed corpus processing at petabyte scale
- âœ… Real-time corpus queries with sub-second response times
- âœ… Advanced diachronic semantics (aligned embeddings across centuries)
- âœ… Cross-linguistic semantic spaces (mBERT, XLM-R for 100+ languages)
- âœ… AI-assisted interpretation (Amazon Bedrock for linguistic analysis)
- âœ… Team collaboration with versioned corpora and annotation layers

**Platform**: AWS multi-account with enterprise support
**Cost**: $2.5K-4K/month (1-10B words), $15K-25K/month (100B+ words)

[View tier-3 README â†’](tier-3/README.md)

---

## What You'll Learn

Across all tiers, this project teaches:
- Frequency analysis and Zipf's law in natural language
- Collocation extraction with statistical measures (PMI, t-score, log-likelihood)
- Concordance analysis and Keywords in Context (KWIC)
- Diachronic semantic change detection (word meaning evolution)
- Dialectal variation analysis across regional varieties
- Multilingual corpus comparison across 100+ languages
- Distributed text processing on cloud infrastructure

## Technologies & Tools

- **Data sources**: Brown Corpus, BNC, COCA (1B words), Google Books (500B+ words), Common Crawl, OpenSubtitles, Wiki40B
- **Languages**: Python 3.9+
- **Core libraries**: pandas, numpy, scipy, scikit-learn, NLTK
- **NLP tools**: spaCy, Stanza, UDPipe (100+ languages), POS tagging
- **Embeddings**: Word2Vec, FastText, BERT, mBERT (multilingual), XLM-R
- **Processing**: EMR Spark (distributed n-gram extraction), Dask
- **Search**: Elasticsearch (concordance queries, sub-second on billion-word corpora)
- **Cloud services** (tier 2+): S3, EMR (Spark), SageMaker (embeddings), Glue, Athena, Lambda, Elasticsearch Service

## Project Structure

```
corpus-linguistics/
â”œâ”€â”€ tier-0/              # Collocation analysis (60-90 min, FREE)
â”‚   â”œâ”€â”€ corpus-analysis.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ tier-1/              # Large-scale (4-8 hours, FREE)
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ environment.yml
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tier-2/              # Production (2-3 days, $450-600/mo)
â”‚   â”œâ”€â”€ cloudformation/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ README.md
â””â”€â”€ tier-3/              # Massive-scale (ongoing, $2.5K-25K/mo)
    â”œâ”€â”€ cloudformation/
    â”œâ”€â”€ notebooks/
    â”œâ”€â”€ src/
    â”œâ”€â”€ infrastructure/
    â””â”€â”€ README.md
```

## Progression Path

```
Tier 0           â†’ Tier 1          â†’ Tier 2            â†’ Tier 3
Collocations       Large-Scale        Production          Massive-Scale
4M words           100M-1B words      1B-10B words        10B-100B+ words
60-90 min          4-8 hours          2-3 days            Ongoing
FREE               FREE               $450-600/mo         $2.5K-25K/mo
```

You can:
- âœ… Skip tiers if you have AWS experience and billion-word corpus needs
- âœ… Stop at any tier - tier-1 is great for dissertations, tier-2 for major research grants
- âœ… Mix and match - use tier-0 for methods, tier-2 for publications

[Understanding tiers â†’](../../../docs/projects/tiers.md)

## Corpus Linguistics Applications

- **Diachronic semantics**: Track word meaning changes across centuries (e.g., "gay," "broadcast," "nice" from 1800-2020)
- **Collocation analysis**: Find statistically significant word combinations across genres and time periods
- **Dialectology**: Compare language varieties across regions (US vs UK English, regional dialects)
- **Multilingual semantics**: Cross-linguistic comparison of concepts (kinship terms, color words, emotions)
- **Register classification**: Classify texts by genre/register with 90-95% accuracy using BERT
- **Lexical change**: Quantify vocabulary innovation and obsolescence over time

## Related Projects

- **[Language Analysis](../language-analysis/)** - Dialect classification and speech analysis
- **[Digital Humanities - Text Analysis](../../digital-humanities/text-analysis/)** - Literary text mining
- **[Social Science - Social Media Analysis](../../social-science/social-media-analysis/)** - Online language variation

## Common Use Cases

- **Academic linguists**: Study language variation, diachronic change, collocation patterns
- **Lexicographers**: Create frequency-based dictionaries and usage guides
- **Language teachers**: Develop authentic teaching materials based on corpus evidence
- **NLP researchers**: Build better language models informed by corpus statistics
- **Historical linguists**: Track semantic shifts and grammaticalization over centuries
- **Sociolinguists**: Analyze language variation across social groups and communities

## Cost Estimates

**Tier 2 Production (1 Billion Words - COCA-Scale)**:
- **S3 storage** (100GB preprocessed corpus): $2.30/month
- **EMR Spark** (distributed n-gram extraction, monthly updates): $150/month
- **Elasticsearch** (concordance search, 3-node cluster): $250/month
- **SageMaker** (diachronic embeddings, monthly): ml.p3.2xlarge, 8 hours = $80/month
- **Lambda** (preprocessing, tokenization): $20/month
- **Total**: $450-600/month for 1B-word operational corpus

**Scaling**:
- 10B words (Google Books subset): $2,500-4,000/month
- 100B+ words (full Google Books, Common Crawl): $15,000-25,000/month

**Optimization tips**:
- Use S3 for cold storage of raw corpora ($0.023/GB vs $2.30 for indexed)
- Cache n-gram frequency lists to avoid recomputation
- Use spot instances for EMR jobs (60-70% savings)
- Precompute collocations for common queries

## Support

- **Questions**: [GitHub Discussions](https://github.com/scttfrdmn/research-jumpstart/discussions)
- **Issues**: [GitHub Issues](https://github.com/scttfrdmn/research-jumpstart/issues)
- **Office Hours**: [Every Tuesday](../../../docs/community/office-hours.md)

## Citation

If you use this project in your research, please cite:

```bibtex
@software{research_jumpstart_corpus_linguistics,
  title = {Corpus Linguistics at Scale: Research Jumpstart},
  author = {Research Jumpstart Community},
  year = {2025},
  url = {https://github.com/scttfrdmn/research-jumpstart},
  note = {Accessed: [date]}
}
```

Also cite the appropriate corpora:
- **COCA**: Davies, M. (2008-) Corpus of Contemporary American English
- **BNC**: British National Corpus Consortium
- **Google Books Ngrams**: Michel et al. (2011) *Science*

## License

Apache 2.0 - See [LICENSE](../../../LICENSE) for details.

---

*Part of [Research Jumpstart](https://github.com/scttfrdmn/research-jumpstart) - Pre-built research workflows for cloud computing*
