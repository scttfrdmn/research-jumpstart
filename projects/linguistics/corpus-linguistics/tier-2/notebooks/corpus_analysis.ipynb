{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Linguistics Analysis with AWS\n",
    "\n",
    "This notebook demonstrates cloud-based corpus linguistics analysis using:\n",
    "- **S3** for corpus storage\n",
    "- **Lambda** for automated linguistic annotation\n",
    "- **DynamoDB** for storing and querying linguistic features\n",
    "\n",
    "**What you'll do:**\n",
    "1. Download or prepare a multilingual corpus\n",
    "2. Upload corpus to S3\n",
    "3. Trigger Lambda for linguistic analysis\n",
    "4. Query and visualize results\n",
    "5. Perform cross-linguistic comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from decimal import Decimal\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "# Replace with your actual bucket name and region\n",
    "BUCKET_NAME = \"linguistic-corpus-YOUR_ID\"  # TODO: Update this\n",
    "DYNAMODB_TABLE = \"LinguisticAnalysis\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client(\"s3\", region_name=REGION)\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(DYNAMODB_TABLE)\n",
    "\n",
    "print(f\"Connected to S3 bucket: {BUCKET_NAME}\")\n",
    "print(f\"Connected to DynamoDB table: {DYNAMODB_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Sample Corpus\n",
    "\n",
    "We'll create a small multilingual corpus for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample corpus locally\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "corpus_dir = Path(\"./sample_corpus\")\n",
    "corpus_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Sample texts in different languages and genres\n",
    "samples = {\n",
    "    \"english/academic\": \"\"\"\n",
    "    Corpus linguistics employs computational methods to analyze large collections of texts.\n",
    "    The methodology relies on frequency analysis, collocation detection, and concordance generation.\n",
    "    Researchers use statistical measures to identify significant linguistic patterns and variations.\n",
    "    Modern corpus analysis tools enable processing of millions of words in multiple languages.\n",
    "    Lexical diversity metrics provide insights into vocabulary richness and text complexity.\n",
    "    \"\"\",\n",
    "    \"english/news\": \"\"\"\n",
    "    Scientists announced a breakthrough in natural language processing technology yesterday.\n",
    "    The new system can analyze text in dozens of languages simultaneously.\n",
    "    Experts believe this development will revolutionize translation and language learning.\n",
    "    Several universities have already requested access to the research data.\n",
    "    The findings were published in a leading computational linguistics journal.\n",
    "    \"\"\",\n",
    "    \"english/fiction\": \"\"\"\n",
    "    Emma walked through the library, running her fingers along the old books.\n",
    "    The scent of aged paper and leather filled the quiet room.\n",
    "    She found the volume she was looking for on the highest shelf.\n",
    "    Its pages contained stories from centuries past, waiting to be discovered.\n",
    "    Outside, the rain began to fall softly against the windows.\n",
    "    \"\"\",\n",
    "    \"spanish/academic\": \"\"\"\n",
    "    La lingüística de corpus estudia el lenguaje mediante análisis de grandes colecciones de textos.\n",
    "    Los métodos cuantitativos permiten identificar patrones frecuentes en el uso lingüístico.\n",
    "    Las colocaciones revelan combinaciones de palabras estadísticamente significativas.\n",
    "    Los investigadores emplean software especializado para procesar millones de palabras.\n",
    "    La diversidad léxica indica la riqueza vocabular de un texto o corpus.\n",
    "    \"\"\",\n",
    "    \"spanish/news\": \"\"\"\n",
    "    El gobierno anunció nuevas políticas educativas para mejorar la enseñanza de idiomas.\n",
    "    Los expertos consideran que el aprendizaje de lenguas extranjeras es fundamental.\n",
    "    Las escuelas recibirán recursos adicionales para programas multilingües.\n",
    "    Los estudiantes podrán elegir entre varios idiomas según sus intereses.\n",
    "    La comunidad educativa ha recibido positivamente las reformas propuestas.\n",
    "    \"\"\",\n",
    "    \"french/academic\": \"\"\"\n",
    "    La linguistique de corpus analyse de grandes quantités de données textuelles.\n",
    "    Les chercheurs utilisent des méthodes statistiques pour identifier des tendances linguistiques.\n",
    "    Les collocations montrent des associations récurrentes entre les mots.\n",
    "    L'analyse automatique permet de traiter des millions de mots rapidement.\n",
    "    La diversité lexicale mesure la richesse du vocabulaire utilisé dans un texte.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "# Write sample files\n",
    "for path, content in samples.items():\n",
    "    file_path = corpus_dir / f\"{path}.txt\"\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content.strip())\n",
    "\n",
    "print(f\"Created {len(samples)} sample texts in {corpus_dir}\")\n",
    "print(\"\\nCorpus structure:\")\n",
    "for root, _dirs, files in os.walk(corpus_dir):\n",
    "    level = root.replace(str(corpus_dir), \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Corpus to S3\n",
    "\n",
    "Upload the corpus files to S3, organizing by language and genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload corpus files to S3\n",
    "\n",
    "uploaded_files = []\n",
    "\n",
    "for root, _dirs, files in os.walk(corpus_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            local_path = os.path.join(root, file)\n",
    "\n",
    "            # Create S3 key maintaining structure\n",
    "            relative_path = os.path.relpath(local_path, corpus_dir)\n",
    "            s3_key = f\"raw/{relative_path}\"\n",
    "\n",
    "            # Upload file\n",
    "            s3_client.upload_file(local_path, BUCKET_NAME, s3_key)\n",
    "            uploaded_files.append(s3_key)\n",
    "            print(f\"Uploaded: {s3_key}\")\n",
    "\n",
    "print(f\"\\nTotal files uploaded: {len(uploaded_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Trigger Lambda Processing\n",
    "\n",
    "If you configured S3 triggers, Lambda will automatically process the files.\n",
    "Otherwise, you can manually invoke Lambda for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Lambda to process files (if using S3 triggers)\n",
    "import time\n",
    "\n",
    "print(\"Waiting for Lambda to process files...\")\n",
    "time.sleep(30)  # Wait 30 seconds for processing\n",
    "print(\"Processing should be complete. Let's check DynamoDB...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manually invoke Lambda for each file\n",
    "lambda_client = boto3.client(\"lambda\", region_name=REGION)\n",
    "\n",
    "# Uncomment to manually invoke Lambda\n",
    "# for s3_key in uploaded_files:\n",
    "#     event = {\n",
    "#         'Records': [{\n",
    "#             's3': {\n",
    "#                 'bucket': {'name': BUCKET_NAME},\n",
    "#                 'object': {'key': s3_key}\n",
    "#             }\n",
    "#         }]\n",
    "#     }\n",
    "#\n",
    "#     response = lambda_client.invoke(\n",
    "#         FunctionName='analyze-linguistic-corpus',\n",
    "#         InvocationType='Event',  # Async\n",
    "#         Payload=json.dumps(event)\n",
    "#     )\n",
    "#     print(f\"Invoked Lambda for {s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Results from DynamoDB\n",
    "\n",
    "Retrieve linguistic analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert Decimal to float\n",
    "def decimal_to_float(obj):\n",
    "    \"\"\"Convert DynamoDB Decimal types to float.\"\"\"\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: decimal_to_float(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [decimal_to_float(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "# Scan all results\n",
    "response = table.scan()\n",
    "items = response[\"Items\"]\n",
    "\n",
    "# Handle pagination\n",
    "while \"LastEvaluatedKey\" in response:\n",
    "    response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n",
    "    items.extend(response[\"Items\"])\n",
    "\n",
    "# Convert Decimals\n",
    "items = [decimal_to_float(item) for item in items]\n",
    "\n",
    "print(f\"Retrieved {len(items)} texts from DynamoDB\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "if items:\n",
    "    df = pd.json_normalize(items)\n",
    "    print(\"\\nDataFrame columns:\")\n",
    "    print(df.columns.tolist())\n",
    "else:\n",
    "    print(\"\\nNo items found in DynamoDB. Make sure Lambda has processed the files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "if not df.empty:\n",
    "    summary_cols = [\n",
    "        \"text_id\",\n",
    "        \"language\",\n",
    "        \"genre\",\n",
    "        \"word_count\",\n",
    "        \"sentence_count\",\n",
    "        \"unique_words\",\n",
    "        \"lexical_diversity.ttr\",\n",
    "    ]\n",
    "    available_cols = [col for col in summary_cols if col in df.columns]\n",
    "    print(df[available_cols].to_string())\n",
    "else:\n",
    "    print(\"No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Word Frequencies\n",
    "\n",
    "Create word clouds for each language/genre combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds\n",
    "if not df.empty and \"top_words\" in df.columns:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx >= 6:  # Limit to 6 plots\n",
    "            break\n",
    "\n",
    "        # Extract word frequencies\n",
    "        top_words = row.get(\"top_words\", [])\n",
    "        if top_words and isinstance(top_words, list):\n",
    "            word_freq = {item[\"word\"]: item[\"freq\"] for item in top_words if isinstance(item, dict)}\n",
    "\n",
    "            if word_freq:\n",
    "                # Generate word cloud\n",
    "                wordcloud = WordCloud(\n",
    "                    width=400, height=300, background_color=\"white\"\n",
    "                ).generate_from_frequencies(word_freq)\n",
    "\n",
    "                # Plot\n",
    "                axes[idx].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "                axes[idx].axis(\"off\")\n",
    "                title = f\"{row.get('language', 'Unknown')} - {row.get('genre', 'Unknown')}\"\n",
    "                axes[idx].set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(df), 6):\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"wordclouds.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Word clouds saved to wordclouds.png\")\n",
    "else:\n",
    "    print(\"No word frequency data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Lexical Diversity Comparison\n",
    "\n",
    "Compare lexical diversity across languages and genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lexical diversity metrics\n",
    "if not df.empty and \"lexical_diversity.ttr\" in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # By language\n",
    "    df.boxplot(column=\"lexical_diversity.ttr\", by=\"language\", ax=axes[0])\n",
    "    axes[0].set_title(\"Lexical Diversity (TTR) by Language\")\n",
    "    axes[0].set_xlabel(\"Language\")\n",
    "    axes[0].set_ylabel(\"Type-Token Ratio\")\n",
    "    axes[0].get_figure().suptitle(\"\")  # Remove default title\n",
    "\n",
    "    # By genre\n",
    "    df.boxplot(column=\"lexical_diversity.ttr\", by=\"genre\", ax=axes[1])\n",
    "    axes[1].set_title(\"Lexical Diversity (TTR) by Genre\")\n",
    "    axes[1].set_xlabel(\"Genre\")\n",
    "    axes[1].set_ylabel(\"Type-Token Ratio\")\n",
    "    axes[1].get_figure().suptitle(\"\")  # Remove default title\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lexical_diversity.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Lexical diversity plots saved to lexical_diversity.png\")\n",
    "else:\n",
    "    print(\"No lexical diversity data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: POS Distribution Analysis\n",
    "\n",
    "Analyze part-of-speech distributions across texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract POS distributions\nif not df.empty and \"pos_distribution\" in df.columns:\n    pos_data = []\n\n    for _, row in df.iterrows():\n        pos_dist = row.get(\"pos_distribution\", {})\n        if isinstance(pos_dist, dict):\n            for pos, count in pos_dist.items():\n                pos_data.append(\n                    {\n                        \"text_id\": row.get(\"text_id\", \"\"),\n                        \"language\": row.get(\"language\", \"\"),\n                        \"genre\": row.get(\"genre\", \"\"),\n                        \"pos\": pos,\n                        \"count\": count,\n                    }\n                )\n\n    if pos_data:\n        pos_df = pd.DataFrame(pos_data)\n\n        # Calculate percentages\n        totals = pos_df.groupby(\"text_id\")[\"count\"].sum()\n        pos_df[\"percentage\"] = pos_df.apply(\n            lambda row: (row[\"count\"] / totals[row[\"text_id\"]] * 100)\n            if row[\"text_id\"] in totals\n            else 0,\n            axis=1,\n        )\n\n        # Plot POS distribution by genre\n        pivot = pos_df.pivot_table(\n            index=\"pos\", columns=\"genre\", values=\"percentage\", aggfunc=\"mean\"\n        )\n\n        pivot.plot(kind=\"bar\", figsize=(12, 6))\n        plt.title(\"Average POS Distribution by Genre\", fontsize=14, fontweight=\"bold\")\n        plt.xlabel(\"Part of Speech\")\n        plt.ylabel(\"Percentage\")\n        plt.legend(title=\"Genre\")\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.savefig(\"pos_distribution.png\", dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        print(\"POS distribution plot saved to pos_distribution.png\")\n    else:\n        print(\"No POS data to visualize\")\nelse:\n    print(\"No POS distribution data available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Collocation Network Visualization\n",
    "\n",
    "Visualize top collocations as a network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create collocation network for English academic texts\nif not df.empty and \"collocations.bigrams\" in df.columns:\n    # Filter for English academic texts\n    english_academic = df[(df[\"language\"] == \"english\") & (df[\"genre\"] == \"academic\")]\n\n    if not english_academic.empty:\n        G = nx.Graph()\n\n        for _, row in english_academic.iterrows():\n            bigrams = row.get(\"collocations.bigrams\", [])\n            if isinstance(bigrams, list):\n                for bigram_data in bigrams[:5]:  # Top 5 collocations\n                    if isinstance(bigram_data, dict):\n                        bigram = bigram_data.get(\"bigram\", \"\")\n                        pmi = bigram_data.get(\"pmi\", 0)\n\n                        if \" \" in bigram:\n                            word1, word2 = bigram.split(\" \", 1)\n                            G.add_edge(word1, word2, weight=pmi)\n\n        if G.number_of_nodes() > 0:\n            # Draw network\n            plt.figure(figsize=(12, 8))\n            pos = nx.spring_layout(G, k=2, iterations=50)\n\n            # Draw nodes\n            nx.draw_networkx_nodes(G, pos, node_size=1000, node_color=\"lightblue\", alpha=0.7)\n\n            # Draw edges with varying thickness\n            edges = G.edges()\n            weights = [G[u][v][\"weight\"] for u, v in edges]\n            nx.draw_networkx_edges(G, pos, width=[w / 2 for w in weights], alpha=0.5)\n\n            # Draw labels\n            nx.draw_networkx_labels(G, pos, font_size=10, font_weight=\"bold\")\n\n            plt.title(\"Collocation Network (English Academic)\", fontsize=14, fontweight=\"bold\")\n            plt.axis(\"off\")\n            plt.tight_layout()\n            plt.savefig(\"collocation_network.png\", dpi=150, bbox_inches=\"tight\")\n            plt.show()\n            print(\"Collocation network saved to collocation_network.png\")\n        else:\n            print(\"No collocation data to visualize\")\n    else:\n        print(\"No English academic texts found\")\nelse:\n    print(\"No collocation data available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Syntactic Complexity Analysis\n",
    "\n",
    "Compare syntactic complexity across languages and genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot syntactic complexity\n",
    "if not df.empty and \"syntactic_complexity.avg_sentence_length\" in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Average sentence length by language\n",
    "    df.groupby(\"language\")[\"syntactic_complexity.avg_sentence_length\"].mean().plot(\n",
    "        kind=\"bar\", ax=axes[0], color=\"steelblue\"\n",
    "    )\n",
    "    axes[0].set_title(\"Average Sentence Length by Language\")\n",
    "    axes[0].set_xlabel(\"Language\")\n",
    "    axes[0].set_ylabel(\"Words per Sentence\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Average sentence length by genre\n",
    "    df.groupby(\"genre\")[\"syntactic_complexity.avg_sentence_length\"].mean().plot(\n",
    "        kind=\"bar\", ax=axes[1], color=\"coral\"\n",
    "    )\n",
    "    axes[1].set_title(\"Average Sentence Length by Genre\")\n",
    "    axes[1].set_xlabel(\"Genre\")\n",
    "    axes[1].set_ylabel(\"Words per Sentence\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"syntactic_complexity.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Syntactic complexity plots saved to syntactic_complexity.png\")\n",
    "else:\n",
    "    print(\"No syntactic complexity data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Export Results\n",
    "\n",
    "Export analysis results to CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "if not df.empty:\n",
    "    export_cols = [\n",
    "        \"text_id\",\n",
    "        \"language\",\n",
    "        \"genre\",\n",
    "        \"word_count\",\n",
    "        \"sentence_count\",\n",
    "        \"unique_words\",\n",
    "        \"avg_word_length\",\n",
    "        \"lexical_diversity.ttr\",\n",
    "        \"lexical_diversity.mattr\",\n",
    "        \"syntactic_complexity.avg_sentence_length\",\n",
    "    ]\n",
    "\n",
    "    available_export_cols = [col for col in export_cols if col in df.columns]\n",
    "    df[available_export_cols].to_csv(\"corpus_analysis_results.csv\", index=False)\n",
    "    print(\"Results exported to corpus_analysis_results.csv\")\n",
    "    print(f\"Columns: {', '.join(available_export_cols)}\")\n",
    "else:\n",
    "    print(\"No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you've:\n",
    "1. Created a multilingual corpus\n",
    "2. Uploaded it to S3\n",
    "3. Processed texts with Lambda for linguistic annotation\n",
    "4. Queried results from DynamoDB\n",
    "5. Visualized word frequencies, lexical diversity, POS distributions, and collocations\n",
    "6. Compared syntactic complexity across languages and genres\n",
    "7. Exported results for further analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- Add more texts to your corpus\n",
    "- Experiment with different languages\n",
    "- Perform diachronic analysis (language change over time)\n",
    "- Compare register differences (formal vs. informal)\n",
    "- Move to Tier 3 for production-scale corpus analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}