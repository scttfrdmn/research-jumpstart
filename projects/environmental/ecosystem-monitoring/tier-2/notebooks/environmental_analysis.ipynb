{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Sensor Data Analysis with AWS\n",
    "\n",
    "This notebook demonstrates a complete environmental monitoring workflow using AWS services:\n",
    "- Generate synthetic sensor data (air quality, water quality, weather)\n",
    "- Upload data to S3\n",
    "- Process with Lambda (AQI/WQI calculation, alerts)\n",
    "- Query results from DynamoDB\n",
    "- Visualize trends and patterns\n",
    "\n",
    "**Prerequisites:**\n",
    "- AWS account configured\n",
    "- Completed setup_guide.md\n",
    "- All AWS resources created (S3, Lambda, DynamoDB, SNS)\n",
    "\n",
    "**Duration:** 30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.append(str(Path('..') / 'scripts'))\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "# TODO: Update these values with your AWS resources\n",
    "\n",
    "BUCKET_NAME = 'environmental-data-YOUR-BUCKET'  # Your S3 bucket\n",
    "DYNAMODB_TABLE = 'EnvironmentalReadings'        # DynamoDB table name\n",
    "LAMBDA_FUNCTION = 'process-sensor-data'          # Lambda function name\n",
    "REGION = 'us-east-1'                             # AWS region\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "dynamodb = boto3.resource('dynamodb', region_name=REGION)\n",
    "lambda_client = boto3.client('lambda', region_name=REGION)\n",
    "\n",
    "print(f\"‚úì AWS clients initialized\")\n",
    "print(f\"  Region: {REGION}\")\n",
    "print(f\"  Bucket: {BUCKET_NAME}\")\n",
    "print(f\"  DynamoDB: {DYNAMODB_TABLE}\")\n",
    "print(f\"  Lambda: {LAMBDA_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Environmental Data\n",
    "\n",
    "Generate synthetic sensor data with realistic patterns:\n",
    "- Diurnal variations (day/night cycles)\n",
    "- Rush hour effects (air quality)\n",
    "- Photosynthesis effects (water quality)\n",
    "- Random anomalies and pollution events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_to_s3 import EnvironmentalDataGenerator\n",
    "\n",
    "# Initialize data generator\n",
    "generator = EnvironmentalDataGenerator(seed=42)\n",
    "\n",
    "# Generate data for 7 days\n",
    "DAYS = 7\n",
    "\n",
    "print(f\"Generating {DAYS} days of sensor data...\")\n",
    "air_data = generator.generate_air_quality_data(days=DAYS)\n",
    "water_data = generator.generate_water_quality_data(days=DAYS)\n",
    "weather_data = generator.generate_weather_data(days=DAYS)\n",
    "\n",
    "print(f\"\\n‚úì Data generation complete:\")\n",
    "print(f\"  Air quality: {len(air_data)} readings from {len(generator.locations)} stations\")\n",
    "print(f\"  Water quality: {len(water_data)} readings from {len(generator.water_locations)} sites\")\n",
    "print(f\"  Weather: {len(weather_data)} readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview air quality data\n",
    "print(\"Air Quality Data Sample:\")\n",
    "air_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview water quality data\n",
    "print(\"Water Quality Data Sample:\")\n",
    "water_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Data to S3\n",
    "\n",
    "Upload generated data to S3. This will automatically trigger Lambda processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_to_s3 import S3Uploader\n",
    "\n",
    "# Initialize uploader\n",
    "uploader = S3Uploader(BUCKET_NAME, region=REGION)\n",
    "\n",
    "# Upload air quality data\n",
    "timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "s3_key = f'raw/air_quality_{timestamp}.csv'\n",
    "print(f\"Uploading air quality data to s3://{BUCKET_NAME}/{s3_key}\")\n",
    "uploader.upload_dataframe(air_data, s3_key, format='csv')\n",
    "\n",
    "# Upload water quality data\n",
    "s3_key = f'raw/water_quality_{timestamp}.csv'\n",
    "print(f\"Uploading water quality data to s3://{BUCKET_NAME}/{s3_key}\")\n",
    "uploader.upload_dataframe(water_data, s3_key, format='csv')\n",
    "\n",
    "# Upload weather data\n",
    "s3_key = f'raw/weather_{timestamp}.csv'\n",
    "print(f\"Uploading weather data to s3://{BUCKET_NAME}/{s3_key}\")\n",
    "uploader.upload_dataframe(weather_data, s3_key, format='csv')\n",
    "\n",
    "print(\"\\n‚úì All data uploaded to S3\")\n",
    "print(\"\\n‚è≥ Lambda processing triggered automatically...\")\n",
    "print(\"   Wait 30-60 seconds for processing to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List uploaded files\n",
    "print(\"\\nUploaded files in S3:\")\n",
    "files = uploader.list_uploaded_files(prefix='raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor Lambda Processing\n",
    "\n",
    "Check Lambda execution logs and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Wait for Lambda processing\n",
    "print(\"Waiting for Lambda processing (30 seconds)...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Check Lambda function status\n",
    "try:\n",
    "    response = lambda_client.get_function(FunctionName=LAMBDA_FUNCTION)\n",
    "    print(f\"\\n‚úì Lambda function status: {response['Configuration']['State']}\")\n",
    "    print(f\"  Last modified: {response['Configuration']['LastModified']}\")\n",
    "    print(f\"  Memory: {response['Configuration']['MemorySize']} MB\")\n",
    "    print(f\"  Timeout: {response['Configuration']['Timeout']} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not check Lambda status: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent Lambda invocations\n",
    "logs_client = boto3.client('logs', region_name=REGION)\n",
    "log_group = f'/aws/lambda/{LAMBDA_FUNCTION}'\n",
    "\n",
    "try:\n",
    "    # Get recent log streams\n",
    "    response = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecent Lambda invocations (last 3):\")\n",
    "    for stream in response['logStreams']:\n",
    "        last_event = datetime.fromtimestamp(stream['lastEventTimestamp']/1000)\n",
    "        print(f\"  {stream['logStreamName']}: {last_event}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not retrieve logs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Results from DynamoDB\n",
    "\n",
    "Retrieve processed sensor readings from DynamoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_results import EnvironmentalDataQuery, convert_decimal\n",
    "\n",
    "# Initialize query client\n",
    "query_client = EnvironmentalDataQuery(\n",
    "    table_name=DYNAMODB_TABLE,\n",
    "    region=REGION\n",
    ")\n",
    "\n",
    "print(\"‚úì DynamoDB query client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique locations\n",
    "locations = query_client.get_all_locations()\n",
    "print(f\"\\nAvailable sensor locations ({len(locations)}):\")\n",
    "for loc in locations:\n",
    "    print(f\"  - {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query air quality station\n",
    "if locations:\n",
    "    # Get first air quality station\n",
    "    air_stations = [loc for loc in locations if loc.startswith('station-')]\n",
    "    if air_stations:\n",
    "        location = air_stations[0]\n",
    "        print(f\"Querying location: {location}\")\n",
    "        \n",
    "        results = query_client.query_by_location(location, days=DAYS, limit=100)\n",
    "        \n",
    "        print(f\"\\n‚úì Retrieved {len(results)} readings from {location}\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        results_df = pd.DataFrame(convert_decimal(results))\n",
    "        print(f\"\\nData shape: {results_df.shape}\")\n",
    "        results_df.head()\n",
    "    else:\n",
    "        print(\"‚ö† No air quality stations found. Wait longer for processing.\")\n",
    "else:\n",
    "    print(\"‚ö† No data in DynamoDB yet. Wait for Lambda processing to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query alerts\n",
    "critical_alerts = query_client.query_by_alert_status('critical', limit=50)\n",
    "warning_alerts = query_client.query_by_alert_status('warning', limit=50)\n",
    "\n",
    "print(f\"\\nAlert Summary:\")\n",
    "print(f\"  Critical alerts: {len(critical_alerts)}\")\n",
    "print(f\"  Warning alerts: {len(warning_alerts)}\")\n",
    "\n",
    "if critical_alerts:\n",
    "    print(f\"\\nCritical alerts:\")\n",
    "    for alert in critical_alerts[:5]:\n",
    "        print(f\"  {alert['timestamp']}: {alert.get('alert_message', 'No message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Analysis and Statistics\n",
    "\n",
    "Calculate statistics and identify trends in environmental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for all locations\n",
    "all_stats = {}\n",
    "\n",
    "for location in locations:\n",
    "    stats = query_client.get_statistics(location, days=DAYS)\n",
    "    if stats:\n",
    "        all_stats[location] = stats\n",
    "\n",
    "print(f\"\\n‚úì Calculated statistics for {len(all_stats)} locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics for air quality stations\n",
    "air_stations = {k: v for k, v in all_stats.items() if k.startswith('station-')}\n",
    "\n",
    "if air_stations:\n",
    "    print(\"\\nAir Quality Statistics:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_data = []\n",
    "    for location, stats in air_stations.items():\n",
    "        if 'air_quality' in stats:\n",
    "            aq = stats['air_quality']\n",
    "            summary_data.append({\n",
    "                'Location': location,\n",
    "                'Readings': stats['total_readings'],\n",
    "                'Avg AQI': f\"{aq.get('avg_aqi', 0):.1f}\",\n",
    "                'Max AQI': f\"{aq.get('max_aqi', 0):.0f}\",\n",
    "                'Avg PM2.5': f\"{aq.get('avg_pm25', 0):.2f}\",\n",
    "                'Alerts': stats['alerts']['critical'] + stats['alerts']['warning']\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"‚ö† No air quality statistics available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics for water quality sites\n",
    "water_sites = {k: v for k, v in all_stats.items() if k.startswith(('river-', 'lake-'))}\n",
    "\n",
    "if water_sites:\n",
    "    print(\"\\nWater Quality Statistics:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_data = []\n",
    "    for location, stats in water_sites.items():\n",
    "        if 'water_quality' in stats:\n",
    "            wq = stats['water_quality']\n",
    "            summary_data.append({\n",
    "                'Location': location,\n",
    "                'Readings': stats['total_readings'],\n",
    "                'Avg WQI': f\"{wq.get('avg_wqi', 0):.1f}\",\n",
    "                'Max WQI': f\"{wq.get('max_wqi', 0):.0f}\",\n",
    "                'Avg pH': f\"{wq.get('avg_ph', 0):.2f}\",\n",
    "                'Alerts': stats['alerts']['critical'] + stats['alerts']['warning']\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"‚ö† No water quality statistics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Visualization\n",
    "\n",
    "Create visualizations to identify patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "if air_stations:\n",
    "    # Get detailed data for first air station\n",
    "    location = list(air_stations.keys())[0]\n",
    "    air_readings = query_client.query_by_location(location, days=DAYS, limit=1000)\n",
    "    \n",
    "    # Convert to DataFrame and parse data\n",
    "    air_df = pd.DataFrame(convert_decimal(air_readings))\n",
    "    air_df['timestamp'] = pd.to_datetime(air_df['timestamp'])\n",
    "    air_df = air_df.sort_values('timestamp')\n",
    "    \n",
    "    # Extract parameters\n",
    "    air_df['pm25'] = air_df['parameters'].apply(lambda x: x.get('pm25', np.nan))\n",
    "    air_df['pm10'] = air_df['parameters'].apply(lambda x: x.get('pm10', np.nan))\n",
    "    air_df['co2'] = air_df['parameters'].apply(lambda x: x.get('co2', np.nan))\n",
    "    air_df['temperature'] = air_df['parameters'].apply(lambda x: x.get('temperature', np.nan))\n",
    "    air_df['humidity'] = air_df['parameters'].apply(lambda x: x.get('humidity', np.nan))\n",
    "    air_df['aqi'] = air_df['calculated_metrics'].apply(lambda x: x.get('aqi', np.nan))\n",
    "    \n",
    "    print(f\"‚úì Prepared {len(air_df)} air quality readings for visualization\")\n",
    "else:\n",
    "    print(\"‚ö† No air quality data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot: PM2.5 and AQI\n",
    "if air_stations and not air_df.empty:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # PM2.5 over time\n",
    "    axes[0].plot(air_df['timestamp'], air_df['pm25'], linewidth=1.5, alpha=0.8)\n",
    "    axes[0].axhline(y=35.4, color='orange', linestyle='--', label='Warning threshold')\n",
    "    axes[0].axhline(y=55.4, color='red', linestyle='--', label='Critical threshold')\n",
    "    axes[0].set_ylabel('PM2.5 (Œºg/m¬≥)', fontsize=12)\n",
    "    axes[0].set_title(f'PM2.5 Levels - {location}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # AQI over time with color zones\n",
    "    axes[1].plot(air_df['timestamp'], air_df['aqi'], linewidth=1.5, alpha=0.8, color='steelblue')\n",
    "    axes[1].axhspan(0, 50, alpha=0.1, color='green', label='Good')\n",
    "    axes[1].axhspan(51, 100, alpha=0.1, color='yellow', label='Moderate')\n",
    "    axes[1].axhspan(101, 150, alpha=0.1, color='orange', label='Unhealthy for sensitive')\n",
    "    axes[1].axhspan(151, 200, alpha=0.1, color='red', label='Unhealthy')\n",
    "    axes[1].set_xlabel('Time', fontsize=12)\n",
    "    axes[1].set_ylabel('AQI', fontsize=12)\n",
    "    axes[1].set_title('Air Quality Index (AQI)', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right', fontsize=9)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† Cannot create visualization - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diurnal pattern analysis\n",
    "if air_stations and not air_df.empty:\n",
    "    # Add hour of day\n",
    "    air_df['hour'] = air_df['timestamp'].dt.hour\n",
    "    \n",
    "    # Calculate hourly averages\n",
    "    hourly_avg = air_df.groupby('hour')[['pm25', 'aqi', 'co2']].mean()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # PM2.5 by hour\n",
    "    axes[0].bar(hourly_avg.index, hourly_avg['pm25'], alpha=0.7, color='steelblue')\n",
    "    axes[0].set_xlabel('Hour of Day', fontsize=11)\n",
    "    axes[0].set_ylabel('PM2.5 (Œºg/m¬≥)', fontsize=11)\n",
    "    axes[0].set_title('Average PM2.5 by Hour', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # AQI by hour\n",
    "    axes[1].bar(hourly_avg.index, hourly_avg['aqi'], alpha=0.7, color='orange')\n",
    "    axes[1].set_xlabel('Hour of Day', fontsize=11)\n",
    "    axes[1].set_ylabel('AQI', fontsize=11)\n",
    "    axes[1].set_title('Average AQI by Hour', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # CO2 by hour\n",
    "    axes[2].bar(hourly_avg.index, hourly_avg['co2'], alpha=0.7, color='green')\n",
    "    axes[2].set_xlabel('Hour of Day', fontsize=11)\n",
    "    axes[2].set_ylabel('CO2 (ppm)', fontsize=11)\n",
    "    axes[2].set_title('Average CO2 by Hour', fontsize=12, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Diurnal Pattern Insights:\")\n",
    "    peak_hour = hourly_avg['pm25'].idxmax()\n",
    "    print(f\"  - Peak PM2.5 at hour {peak_hour}:00 ({hourly_avg['pm25'].loc[peak_hour]:.1f} Œºg/m¬≥)\")\n",
    "    print(f\"  - Lowest PM2.5 at hour {hourly_avg['pm25'].idxmin()}:00 ({hourly_avg['pm25'].min():.1f} Œºg/m¬≥)\")\n",
    "else:\n",
    "    print(\"‚ö† Cannot create diurnal pattern - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "if air_stations and not air_df.empty:\n",
    "    # Select numeric columns\n",
    "    numeric_cols = ['pm25', 'pm10', 'co2', 'temperature', 'humidity', 'aqi']\n",
    "    corr_data = air_df[numeric_cols].dropna()\n",
    "    \n",
    "    if not corr_data.empty:\n",
    "        correlation = corr_data.corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                    center=0, square=True, linewidths=1)\n",
    "        plt.title('Environmental Parameter Correlations', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä Correlation Insights:\")\n",
    "        print(f\"  - PM2.5 vs Temperature: {correlation.loc['pm25', 'temperature']:.2f}\")\n",
    "        print(f\"  - PM2.5 vs Humidity: {correlation.loc['pm25', 'humidity']:.2f}\")\n",
    "        print(f\"  - AQI vs PM2.5: {correlation.loc['aqi', 'pm25']:.2f}\")\n",
    "    else:\n",
    "        print(\"‚ö† Not enough data for correlation analysis\")\n",
    "else:\n",
    "    print(\"‚ö† Cannot create correlation heatmap - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water quality visualization (if available)\n",
    "if water_sites:\n",
    "    location = list(water_sites.keys())[0]\n",
    "    water_readings = query_client.query_by_location(location, days=DAYS, limit=1000)\n",
    "    \n",
    "    if water_readings:\n",
    "        # Convert to DataFrame\n",
    "        water_df = pd.DataFrame(convert_decimal(water_readings))\n",
    "        water_df['timestamp'] = pd.to_datetime(water_df['timestamp'])\n",
    "        water_df = water_df.sort_values('timestamp')\n",
    "        \n",
    "        # Extract parameters\n",
    "        water_df['ph'] = water_df['parameters'].apply(lambda x: x.get('ph', np.nan))\n",
    "        water_df['dissolved_oxygen'] = water_df['parameters'].apply(lambda x: x.get('dissolved_oxygen', np.nan))\n",
    "        water_df['turbidity'] = water_df['parameters'].apply(lambda x: x.get('turbidity', np.nan))\n",
    "        water_df['wqi'] = water_df['calculated_metrics'].apply(lambda x: x.get('wqi', np.nan))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # pH over time\n",
    "        axes[0, 0].plot(water_df['timestamp'], water_df['ph'], linewidth=1.5, color='blue')\n",
    "        axes[0, 0].axhspan(6.5, 8.5, alpha=0.2, color='green', label='Acceptable range')\n",
    "        axes[0, 0].set_ylabel('pH', fontsize=11)\n",
    "        axes[0, 0].set_title('pH Levels', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Dissolved oxygen\n",
    "        axes[0, 1].plot(water_df['timestamp'], water_df['dissolved_oxygen'], linewidth=1.5, color='cyan')\n",
    "        axes[0, 1].axhline(y=5.0, color='red', linestyle='--', label='Critical threshold')\n",
    "        axes[0, 1].set_ylabel('Dissolved Oxygen (mg/L)', fontsize=11)\n",
    "        axes[0, 1].set_title('Dissolved Oxygen', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Turbidity\n",
    "        axes[1, 0].plot(water_df['timestamp'], water_df['turbidity'], linewidth=1.5, color='brown')\n",
    "        axes[1, 0].set_xlabel('Time', fontsize=11)\n",
    "        axes[1, 0].set_ylabel('Turbidity (NTU)', fontsize=11)\n",
    "        axes[1, 0].set_title('Turbidity', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # WQI\n",
    "        axes[1, 1].plot(water_df['timestamp'], water_df['wqi'], linewidth=1.5, color='green')\n",
    "        axes[1, 1].axhspan(0, 25, alpha=0.1, color='green', label='Excellent')\n",
    "        axes[1, 1].axhspan(26, 50, alpha=0.1, color='blue', label='Good')\n",
    "        axes[1, 1].axhspan(51, 75, alpha=0.1, color='yellow', label='Fair')\n",
    "        axes[1, 1].axhspan(76, 100, alpha=0.1, color='red', label='Poor')\n",
    "        axes[1, 1].set_xlabel('Time', fontsize=11)\n",
    "        axes[1, 1].set_ylabel('WQI', fontsize=11)\n",
    "        axes[1, 1].set_title('Water Quality Index', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].legend(loc='upper right', fontsize=9)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'Water Quality Monitoring - {location}', fontsize=14, fontweight='bold', y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö† No water quality data available\")\n",
    "else:\n",
    "    print(\"‚ö† No water quality sites found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alert Analysis\n",
    "\n",
    "Analyze pollution alerts and threshold violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count alerts by location\n",
    "alert_summary = []\n",
    "\n",
    "for location in locations:\n",
    "    readings = query_client.query_by_location(location, days=DAYS, limit=1000)\n",
    "    if readings:\n",
    "        critical = sum(1 for r in readings if r.get('alert_status') == 'critical')\n",
    "        warning = sum(1 for r in readings if r.get('alert_status') == 'warning')\n",
    "        none = sum(1 for r in readings if r.get('alert_status') == 'none')\n",
    "        \n",
    "        alert_summary.append({\n",
    "            'Location': location,\n",
    "            'Total': len(readings),\n",
    "            'Critical': critical,\n",
    "            'Warning': warning,\n",
    "            'None': none,\n",
    "            'Alert Rate': f\"{(critical + warning) / len(readings) * 100:.1f}%\"\n",
    "        })\n",
    "\n",
    "alert_df = pd.DataFrame(alert_summary)\n",
    "alert_df = alert_df.sort_values('Critical', ascending=False)\n",
    "\n",
    "print(\"\\nAlert Summary by Location:\")\n",
    "display(alert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alert distribution\n",
    "if not alert_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Stacked bar chart\n",
    "    alert_df.set_index('Location')[['Critical', 'Warning', 'None']].plot(\n",
    "        kind='bar', stacked=True, ax=axes[0], color=['red', 'orange', 'green'], alpha=0.7\n",
    "    )\n",
    "    axes[0].set_ylabel('Number of Readings', fontsize=11)\n",
    "    axes[0].set_title('Alerts by Location', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend(title='Status')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart of overall alerts\n",
    "    total_critical = alert_df['Critical'].sum()\n",
    "    total_warning = alert_df['Warning'].sum()\n",
    "    total_none = alert_df['None'].sum()\n",
    "    \n",
    "    axes[1].pie(\n",
    "        [total_critical, total_warning, total_none],\n",
    "        labels=['Critical', 'Warning', 'None'],\n",
    "        colors=['red', 'orange', 'green'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    axes[1].set_title('Overall Alert Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† No alert data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Save analysis results for further use or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('..') / 'output'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Export statistics\n",
    "stats_file = output_dir / f'statistics_{timestamp}.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(convert_decimal(all_stats), f, indent=2)\n",
    "print(f\"‚úì Statistics exported to {stats_file}\")\n",
    "\n",
    "# Export alert summary\n",
    "alert_file = output_dir / f'alert_summary_{timestamp}.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"‚úì Alert summary exported to {alert_file}\")\n",
    "\n",
    "# Export air quality data\n",
    "if air_stations and not air_df.empty:\n",
    "    air_file = output_dir / f'air_quality_{timestamp}.csv'\n",
    "    air_df.to_csv(air_file, index=False)\n",
    "    print(f\"‚úì Air quality data exported to {air_file}\")\n",
    "\n",
    "print(f\"\\n‚úì All results exported to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENVIRONMENTAL MONITORING ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nData Overview:\")\n",
    "print(f\"  - Monitoring period: {DAYS} days\")\n",
    "print(f\"  - Total locations: {len(locations)}\")\n",
    "print(f\"  - Air quality stations: {len(air_stations)}\")\n",
    "print(f\"  - Water quality sites: {len(water_sites)}\")\n",
    "\n",
    "print(f\"\\nAlert Summary:\")\n",
    "if not alert_df.empty:\n",
    "    print(f\"  - Total readings: {alert_df['Total'].sum()}\")\n",
    "    print(f\"  - Critical alerts: {alert_df['Critical'].sum()}\")\n",
    "    print(f\"  - Warning alerts: {alert_df['Warning'].sum()}\")\n",
    "    print(f\"  - Overall alert rate: {(alert_df['Critical'].sum() + alert_df['Warning'].sum()) / alert_df['Total'].sum() * 100:.1f}%\")\n",
    "\n",
    "if air_stations:\n",
    "    print(f\"\\nAir Quality Insights:\")\n",
    "    for location, stats in list(air_stations.items())[:3]:\n",
    "        if 'air_quality' in stats:\n",
    "            aq = stats['air_quality']\n",
    "            print(f\"  {location}:\")\n",
    "            print(f\"    - Average AQI: {aq.get('avg_aqi', 0):.1f}\")\n",
    "            print(f\"    - Maximum AQI: {aq.get('max_aqi', 0):.0f}\")\n",
    "            print(f\"    - Average PM2.5: {aq.get('avg_pm25', 0):.2f} Œºg/m¬≥\")\n",
    "\n",
    "if water_sites:\n",
    "    print(f\"\\nWater Quality Insights:\")\n",
    "    for location, stats in list(water_sites.items())[:2]:\n",
    "        if 'water_quality' in stats:\n",
    "            wq = stats['water_quality']\n",
    "            print(f\"  {location}:\")\n",
    "            print(f\"    - Average WQI: {wq.get('avg_wqi', 0):.1f}\")\n",
    "            print(f\"    - Average pH: {wq.get('avg_ph', 0):.2f}\")\n",
    "\n",
    "print(f\"\\nAWS Services Used:\")\n",
    "print(f\"  - S3: Data storage\")\n",
    "print(f\"  - Lambda: Serverless processing (AQI/WQI calculation)\")\n",
    "print(f\"  - DynamoDB: Time series database\")\n",
    "print(f\"  - SNS: Alert notifications\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis complete! Next steps:\")\n",
    "print(\"  1. Review exported files in output/ directory\")\n",
    "print(\"  2. Check SNS email for any pollution alerts\")\n",
    "print(\"  3. Run cleanup_guide.md to delete AWS resources\")\n",
    "print(\"  4. Try Tier 3 for production-grade CloudFormation deployment\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
