{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Analytics Platform - Student Performance Analysis\n",
    "\n",
    "This notebook demonstrates a complete learning analytics pipeline using AWS services:\n",
    "- Generate and upload student activity data to S3\n",
    "- Trigger Lambda functions for analytics processing\n",
    "- Query results from DynamoDB\n",
    "- Run SQL queries with Athena\n",
    "- Visualize learning insights and identify at-risk students\n",
    "\n",
    "**Duration:** 60-90 minutes  \n",
    "**Cost:** $6-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install -q boto3 pandas numpy scipy matplotlib seaborn python-dotenv tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET_NAME\", \"learning-analytics-your-user-id\")\n",
    "DYNAMODB_TABLE = os.getenv(\"DYNAMODB_TABLE\", \"StudentAnalytics\")\n",
    "LAMBDA_FUNCTION = os.getenv(\"LAMBDA_FUNCTION_NAME\", \"analyze-student-performance\")\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "lambda_client = boto3.client(\"lambda\", region_name=AWS_REGION)\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=AWS_REGION)\n",
    "athena_client = boto3.client(\"athena\", region_name=AWS_REGION)\n",
    "\n",
    "# DynamoDB table\n",
    "table = dynamodb.Table(DYNAMODB_TABLE)\n",
    "\n",
    "print(\"✓ AWS Configuration:\")\n",
    "print(f\"  Region: {AWS_REGION}\")\n",
    "print(f\"  S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"  DynamoDB Table: {DYNAMODB_TABLE}\")\n",
    "print(f\"  Lambda Function: {LAMBDA_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Sample Student Data\n",
    "\n",
    "We'll generate synthetic student activity data including:\n",
    "- Quiz scores\n",
    "- Assignment submissions\n",
    "- Engagement metrics (time on task, resource views)\n",
    "- Multiple courses and assessment types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scripts directory to path\n",
    "sys.path.insert(0, \"../scripts\")\n",
    "from upload_to_s3 import generate_sample_data\n",
    "\n",
    "# Generate sample data\n",
    "NUM_STUDENTS = 500\n",
    "NUM_COURSES = 3\n",
    "ASSESSMENTS_PER_COURSE = 10\n",
    "\n",
    "print(f\"Generating sample data for {NUM_STUDENTS} students...\")\n",
    "df = generate_sample_data(\n",
    "    num_students=NUM_STUDENTS,\n",
    "    num_courses=NUM_COURSES,\n",
    "    assessments_per_course=ASSESSMENTS_PER_COURSE,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(df)} activity records\")\n",
    "print(\"\\nSample data:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Students: {df['student_id'].nunique()}\")\n",
    "print(f\"  Courses: {df['course_id'].nunique()}\")\n",
    "print(f\"  Assessment types: {df['assessment_type'].unique()}\")\n",
    "print(f\"  Date range: {df['submission_date'].min()} to {df['submission_date'].max()}\")\n",
    "print(f\"\\nSubmission rate: {df['submitted'].mean() * 100:.1f}%\")\n",
    "print(f\"Average score (submitted): {df[df['submitted']]['score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Data to S3\n",
    "\n",
    "Upload student data to S3, which will trigger Lambda processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data locally (split by course for realistic scenario)\n",
    "output_dir = Path(\"../generated_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "uploaded_files = []\n",
    "\n",
    "for course_id in df[\"course_id\"].unique():\n",
    "    course_df = df[df[\"course_id\"] == course_id]\n",
    "\n",
    "    # Remove original_id (keep only anonymized)\n",
    "    if \"original_id\" in course_df.columns:\n",
    "        course_df = course_df.drop(columns=[\"original_id\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f\"student_data_{course_id}.csv\"\n",
    "    filepath = output_dir / filename\n",
    "    course_df.to_csv(filepath, index=False)\n",
    "    uploaded_files.append(filepath)\n",
    "    print(f\"✓ Saved {filename} ({len(course_df)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "from upload_to_s3 import S3DataUploader\n",
    "\n",
    "uploader = S3DataUploader(S3_BUCKET, region=AWS_REGION)\n",
    "\n",
    "print(f\"Uploading files to s3://{S3_BUCKET}/raw-data/\")\n",
    "results = uploader.upload_directory(str(output_dir), s3_prefix=\"raw-data/\")\n",
    "\n",
    "print(\"\\n✓ Upload complete:\")\n",
    "print(f\"  Successful: {results['successful']}\")\n",
    "print(f\"  Failed: {results['failed']}\")\n",
    "print(f\"  Total records: {results['total_records']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Trigger Lambda Processing\n",
    "\n",
    "If S3 event triggers are configured, Lambda will run automatically. Otherwise, manually invoke Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Lambda invocation (if S3 trigger not configured)\n",
    "def invoke_lambda_for_file(s3_key: str):\n",
    "    \"\"\"Manually invoke Lambda for a specific S3 file.\"\"\"\n",
    "    event = {\"Records\": [{\"s3\": {\"bucket\": {\"name\": S3_BUCKET}, \"object\": {\"key\": s3_key}}}]}\n",
    "\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=LAMBDA_FUNCTION, InvocationType=\"RequestResponse\", Payload=json.dumps(event)\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Payload\"].read())\n",
    "    return result\n",
    "\n",
    "\n",
    "# Invoke for each uploaded file\n",
    "print(\"Invoking Lambda for uploaded files...\\n\")\n",
    "for course_id in df[\"course_id\"].unique():\n",
    "    s3_key = f\"raw-data/student_data_{course_id}.csv\"\n",
    "    print(f\"Processing {s3_key}...\")\n",
    "    result = invoke_lambda_for_file(s3_key)\n",
    "\n",
    "    if result.get(\"statusCode\") == 200:\n",
    "        body = json.loads(result.get(\"body\", \"{}\"))\n",
    "        print(f\"  ✓ Success: {body.get('students_processed', 0)} students processed\")\n",
    "    else:\n",
    "        print(f\"  ✗ Error: {result}\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ Lambda processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Results from DynamoDB\n",
    "\n",
    "Retrieve student analytics from DynamoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for DynamoDB writes to complete\n",
    "import time\n",
    "\n",
    "print(\"Waiting 5 seconds for processing...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Scan DynamoDB table\n",
    "response = table.scan()\n",
    "items = response.get(\"Items\", [])\n",
    "\n",
    "# Handle pagination\n",
    "while \"LastEvaluatedKey\" in response:\n",
    "    response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n",
    "    items.extend(response.get(\"Items\", []))\n",
    "\n",
    "print(f\"✓ Retrieved {len(items)} student records from DynamoDB\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "students_df = pd.DataFrame(items)\n",
    "students_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-level statistics\n",
    "print(\"CLASS STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Students:      {len(students_df)}\")\n",
    "print(\n",
    "    f\"Average Grade:       {students_df['avg_grade'].mean():.2f} ± {students_df['avg_grade'].std():.2f}\"\n",
    ")\n",
    "print(f\"Median Grade:        {students_df['avg_grade'].median():.2f}\")\n",
    "print(f\"Completion Rate:     {students_df['completion_rate'].mean():.2f}%\")\n",
    "print(f\"Engagement Score:    {students_df['engagement_score'].mean():.2f}\")\n",
    "print(f\"Mastery Level:       {students_df['mastery_level'].mean():.2f}%\")\n",
    "\n",
    "# Risk distribution\n",
    "print(\"\\nRisk Distribution:\")\n",
    "risk_counts = students_df[\"risk_level\"].value_counts()\n",
    "for level in [\"high\", \"medium\", \"low\", \"none\"]:\n",
    "    count = risk_counts.get(level, 0)\n",
    "    pct = count / len(students_df) * 100 if len(students_df) > 0 else 0\n",
    "    print(f\"  {level:10s}: {count:3d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Learning Analytics\n",
    "\n",
    "Create dashboards and visualizations for educational insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Grade distribution histogram\n",
    "axes[0, 0].hist(students_df[\"avg_grade\"], bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0, 0].axvline(\n",
    "    students_df[\"avg_grade\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {students_df['avg_grade'].mean():.1f}\",\n",
    ")\n",
    "axes[0, 0].axvline(\n",
    "    students_df[\"avg_grade\"].median(),\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Median: {students_df['avg_grade'].median():.1f}\",\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Average Grade\")\n",
    "axes[0, 0].set_ylabel(\"Number of Students\")\n",
    "axes[0, 0].set_title(\"Grade Distribution\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Risk level distribution\n",
    "risk_order = [\"none\", \"low\", \"medium\", \"high\"]\n",
    "risk_colors = {\"none\": \"green\", \"low\": \"yellow\", \"medium\": \"orange\", \"high\": \"red\"}\n",
    "risk_data = students_df[\"risk_level\"].value_counts().reindex(risk_order, fill_value=0)\n",
    "axes[0, 1].bar(\n",
    "    risk_data.index, risk_data.values, color=[risk_colors.get(x, \"gray\") for x in risk_data.index]\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Risk Level\")\n",
    "axes[0, 1].set_ylabel(\"Number of Students\")\n",
    "axes[0, 1].set_title(\"Student Risk Levels\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Engagement vs Grade scatter\n",
    "scatter = axes[1, 0].scatter(\n",
    "    students_df[\"engagement_score\"],\n",
    "    students_df[\"avg_grade\"],\n",
    "    c=students_df[\"completion_rate\"],\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"Engagement Score\")\n",
    "axes[1, 0].set_ylabel(\"Average Grade\")\n",
    "axes[1, 0].set_title(\"Engagement vs Performance\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label=\"Completion Rate\")\n",
    "\n",
    "# 4. Mastery level distribution\n",
    "axes[1, 1].hist(students_df[\"mastery_level\"], bins=20, edgecolor=\"black\", alpha=0.7)\n",
    "axes[1, 1].axvline(\n",
    "    students_df[\"mastery_level\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {students_df['mastery_level'].mean():.1f}%\",\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Mastery Level (%)\")\n",
    "axes[1, 1].set_ylabel(\"Number of Students\")\n",
    "axes[1, 1].set_title(\"Mastery Learning Distribution\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"learning_analytics_dashboard.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Dashboard saved as 'learning_analytics_dashboard.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = [\n",
    "    \"avg_grade\",\n",
    "    \"completion_rate\",\n",
    "    \"engagement_score\",\n",
    "    \"mastery_level\",\n",
    "    \"grade_trend\",\n",
    "    \"total_time_minutes\",\n",
    "]\n",
    "correlation_matrix = students_df[numeric_cols].corr()\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, square=True, linewidths=1\n",
    ")\n",
    "plt.title(\"Learning Metrics Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"correlation_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Correlation heatmap saved as 'correlation_heatmap.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Identify At-Risk Students\n",
    "\n",
    "Generate actionable reports for educators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-risk students\n",
    "high_risk = students_df[students_df[\"risk_level\"] == \"high\"].sort_values(\"avg_grade\")\n",
    "\n",
    "print(\"HIGH-RISK STUDENTS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total high-risk students: {len(high_risk)}\\n\")\n",
    "\n",
    "if len(high_risk) > 0:\n",
    "    display_cols = [\n",
    "        \"student_id\",\n",
    "        \"course_id\",\n",
    "        \"avg_grade\",\n",
    "        \"completion_rate\",\n",
    "        \"engagement_score\",\n",
    "        \"risk_factors\",\n",
    "    ]\n",
    "    print(high_risk[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "    # Common risk factors\n",
    "    all_risk_factors = []\n",
    "    for factors in high_risk[\"risk_factors\"]:\n",
    "        if isinstance(factors, str):\n",
    "            all_risk_factors.extend(\n",
    "                json.loads(factors) if factors.startswith(\"[\") else factors.split(\",\")\n",
    "            )\n",
    "\n",
    "    if all_risk_factors:\n",
    "        print(\"\\nMost Common Risk Factors:\")\n",
    "        from collections import Counter\n",
    "\n",
    "        factor_counts = Counter(all_risk_factors)\n",
    "        for factor, count in factor_counts.most_common(5):\n",
    "            print(f\"  {factor}: {count} students ({count / len(high_risk) * 100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No high-risk students identified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervention recommendations\n",
    "def recommend_intervention(row):\n",
    "    \"\"\"Generate intervention recommendations based on student profile.\"\"\"\n",
    "    recommendations = []\n",
    "\n",
    "    if row[\"avg_grade\"] < 60:\n",
    "        recommendations.append(\"Urgent academic support needed\")\n",
    "    elif row[\"avg_grade\"] < 70:\n",
    "        recommendations.append(\"Schedule tutoring session\")\n",
    "\n",
    "    if row[\"completion_rate\"] < 70:\n",
    "        recommendations.append(\"Address assignment completion issues\")\n",
    "\n",
    "    if row[\"engagement_score\"] < 50:\n",
    "        recommendations.append(\"Increase student engagement\")\n",
    "\n",
    "    if row[\"grade_trend\"] < -2:\n",
    "        recommendations.append(\"Monitor declining performance\")\n",
    "\n",
    "    return \"; \".join(recommendations) if recommendations else \"Continue current support\"\n",
    "\n",
    "\n",
    "# Apply to at-risk students\n",
    "at_risk = students_df[students_df[\"risk_level\"].isin([\"high\", \"medium\"])].copy()\n",
    "at_risk[\"recommendations\"] = at_risk.apply(recommend_intervention, axis=1)\n",
    "\n",
    "print(\"\\nINTERVENTION RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Students needing intervention: {len(at_risk)}\\n\")\n",
    "\n",
    "for _idx, row in at_risk.head(10).iterrows():\n",
    "    print(f\"Student: {row['student_id'][:12]}... | Course: {row['course_id']}\")\n",
    "    print(\n",
    "        f\"  Grade: {row['avg_grade']:.1f} | Completion: {row['completion_rate']:.1f}% | Engagement: {row['engagement_score']:.1f}\"\n",
    "    )\n",
    "    print(f\"  → {row['recommendations']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Query with Athena (Optional)\n",
    "\n",
    "Run SQL queries on processed data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Athena query\n",
    "ATHENA_DATABASE = \"learning_analytics\"\n",
    "ATHENA_OUTPUT = f\"s3://{S3_BUCKET}/athena-results/\"\n",
    "\n",
    "\n",
    "def run_athena_query(query: str, max_wait: int = 30):\n",
    "    \"\"\"Execute Athena query and return results.\"\"\"\n",
    "    # Start query execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\"Database\": ATHENA_DATABASE},\n",
    "        ResultConfiguration={\"OutputLocation\": ATHENA_OUTPUT},\n",
    "    )\n",
    "\n",
    "    query_id = response[\"QueryExecutionId\"]\n",
    "\n",
    "    # Wait for completion\n",
    "    import time\n",
    "\n",
    "    for _ in range(max_wait):\n",
    "        status = athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "        state = status[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "\n",
    "        if state == \"SUCCEEDED\":\n",
    "            # Get results\n",
    "            results = athena_client.get_query_results(QueryExecutionId=query_id)\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            rows = results[\"ResultSet\"][\"Rows\"]\n",
    "            if len(rows) > 1:\n",
    "                headers = [col[\"VarCharValue\"] for col in rows[0][\"Data\"]]\n",
    "                data = [[col.get(\"VarCharValue\", \"\") for col in row[\"Data\"]] for row in rows[1:]]\n",
    "                return pd.DataFrame(data, columns=headers)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        elif state in [\"FAILED\", \"CANCELLED\"]:\n",
    "            raise Exception(\n",
    "                f\"Query {state}: {status['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')}\"\n",
    "            )\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    raise TimeoutError(\"Query timed out\")\n",
    "\n",
    "\n",
    "# Example query: Average grade by course\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    course_id,\n",
    "    COUNT(*) as student_count,\n",
    "    AVG(avg_grade) as avg_grade,\n",
    "    AVG(completion_rate) as avg_completion,\n",
    "    AVG(engagement_score) as avg_engagement\n",
    "FROM student_metrics\n",
    "GROUP BY course_id\n",
    "ORDER BY avg_grade DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Running Athena query...\")\n",
    "    results = run_athena_query(query)\n",
    "    print(\"\\nCOURSE-LEVEL STATISTICS (from Athena)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Note: Athena query skipped or failed: {e}\")\n",
    "    print(\"This is optional - you may need to configure Athena first (see setup_guide.md)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Export Reports\n",
    "\n",
    "Save analysis results for sharing with educators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export at-risk students report\n",
    "at_risk_report = at_risk[\n",
    "    [\n",
    "        \"student_id\",\n",
    "        \"course_id\",\n",
    "        \"avg_grade\",\n",
    "        \"completion_rate\",\n",
    "        \"engagement_score\",\n",
    "        \"risk_level\",\n",
    "        \"recommendations\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "at_risk_report.to_csv(\"at_risk_students_report.csv\", index=False)\n",
    "print(\"✓ At-risk students report saved: 'at_risk_students_report.csv'\")\n",
    "\n",
    "# Export class statistics\n",
    "class_stats = (\n",
    "    students_df.groupby(\"course_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"student_id\": \"count\",\n",
    "            \"avg_grade\": [\"mean\", \"median\", \"std\"],\n",
    "            \"completion_rate\": \"mean\",\n",
    "            \"engagement_score\": \"mean\",\n",
    "            \"mastery_level\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "class_stats.to_csv(\"class_statistics.csv\")\n",
    "print(\"✓ Class statistics saved: 'class_statistics.csv'\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete! Reports ready for review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Cost Estimate\n",
    "\n",
    "Review what we accomplished and estimate AWS costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LEARNING ANALYTICS PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Data Generation:\")\n",
    "print(f\"   - Students: {NUM_STUDENTS}\")\n",
    "print(f\"   - Courses: {NUM_COURSES}\")\n",
    "print(f\"   - Activity records: {len(df):,}\")\n",
    "\n",
    "print(\"\\n2. AWS Processing:\")\n",
    "print(f\"   - Files uploaded to S3: {results['successful']}\")\n",
    "print(f\"   - Lambda invocations: {NUM_COURSES}\")\n",
    "print(f\"   - DynamoDB records: {len(students_df)}\")\n",
    "\n",
    "print(\"\\n3. Analytics Results:\")\n",
    "print(f\"   - Average grade: {students_df['avg_grade'].mean():.2f}\")\n",
    "print(f\"   - Completion rate: {students_df['completion_rate'].mean():.2f}%\")\n",
    "print(f\"   - High-risk students: {len(high_risk)}\")\n",
    "print(f\"   - Students needing intervention: {len(at_risk)}\")\n",
    "\n",
    "print(\"\\n4. Estimated AWS Costs:\")\n",
    "data_size_mb = len(df) * 0.001  # Rough estimate\n",
    "s3_cost = data_size_mb * 0.023 / 1024  # $0.023 per GB per month\n",
    "lambda_cost = NUM_COURSES * 0.0000002 * 20  # 20 seconds per invocation\n",
    "dynamodb_cost = len(students_df) * 0.00000125  # On-demand write cost\n",
    "total_cost = s3_cost + lambda_cost + dynamodb_cost\n",
    "\n",
    "print(f\"   - S3 storage: ${s3_cost:.4f}\")\n",
    "print(f\"   - Lambda compute: ${lambda_cost:.4f}\")\n",
    "print(f\"   - DynamoDB writes: ${dynamodb_cost:.4f}\")\n",
    "print(f\"   - Total (for this run): ${total_cost:.4f}\")\n",
    "print(\"\\n   Note: Actual costs may vary. Monitor AWS Cost Explorer.\")\n",
    "\n",
    "print(\"\\n5. Next Steps:\")\n",
    "print(\"   - Review at-risk students report\")\n",
    "print(\"   - Implement recommended interventions\")\n",
    "print(\"   - Run cleanup script to delete AWS resources\")\n",
    "print(\"   - See cleanup_guide.md for instructions\")\n",
    "\n",
    "print(\"\\n✓ Learning analytics pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
