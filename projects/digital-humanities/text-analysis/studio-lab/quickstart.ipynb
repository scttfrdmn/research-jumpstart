{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Text Analysis & Corpus Linguistics\n",
    "\n",
    "This notebook demonstrates computational methods for analyzing historical texts, including lexical analysis, stylometry, and topic modeling.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Load and preprocess historical texts\n",
    "2. Perform lexical analysis (word frequencies, vocabulary richness)\n",
    "3. Conduct stylometric analysis for authorship attribution\n",
    "4. Apply topic modeling to identify themes\n",
    "5. Extract and visualize named entities\n",
    "6. Compare texts and measure similarity\n",
    "7. Create visualizations (word clouds, frequency distributions)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Public domain texts:\n",
    "- Shakespeare: Sonnet 18\n",
    "- Lincoln: Gettysburg Address\n",
    "- Austen: Pride and Prejudice (excerpt)\n",
    "- King: I Have a Dream speech (excerpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"maxent_ne_chunker\", quiet=True)\n",
    "nltk.download(\"words\", quiet=True)\n",
    "\n",
    "print(\"✓ NLTK data downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts from files\n",
    "def load_text(filename):\n",
    "    \"\"\"Load text from file.\"\"\"\n",
    "    with open(os.path.join(\"texts\", filename), encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "texts = {\n",
    "    \"Shakespeare\": load_text(\"shakespeare_sonnet18.txt\"),\n",
    "    \"Lincoln\": load_text(\"lincoln_gettysburg.txt\"),\n",
    "    \"Austen\": load_text(\"austen_pride_excerpt.txt\"),\n",
    "    \"King\": load_text(\"king_dream_excerpt.txt\"),\n",
    "}\n",
    "\n",
    "print(\"Loaded texts:\")\n",
    "for author, text in texts.items():\n",
    "    print(f\"  {author}: {len(text)} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample from Shakespeare:\")\n",
    "print(texts[\"Shakespeare\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \"\"\"Tokenize and preprocess text.\"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Preprocess all texts\n",
    "tokens_all = {author: preprocess_text(text) for author, text in texts.items()}\n",
    "tokens_nostop = {\n",
    "    author: preprocess_text(text, remove_stopwords=True) for author, text in texts.items()\n",
    "}\n",
    "\n",
    "print(\"Token counts (with stopwords):\")\n",
    "for author, tokens in tokens_all.items():\n",
    "    print(f\"  {author}: {len(tokens)} tokens\")\n",
    "\n",
    "print(\"\\nToken counts (without stopwords):\")\n",
    "for author, tokens in tokens_nostop.items():\n",
    "    print(f\"  {author}: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexical_stats(tokens):\n",
    "    \"\"\"Calculate lexical diversity metrics.\"\"\"\n",
    "    total_tokens = len(tokens)\n",
    "    unique_tokens = len(set(tokens))\n",
    "\n",
    "    # Type-Token Ratio (TTR)\n",
    "    ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # Hapax legomena (words appearing once)\n",
    "    freq_dist = Counter(tokens)\n",
    "    hapax = sum(1 for word, count in freq_dist.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"unique_tokens\": unique_tokens,\n",
    "        \"ttr\": ttr,\n",
    "        \"hapax_legomena\": hapax,\n",
    "        \"hapax_percentage\": (hapax / unique_tokens * 100) if unique_tokens > 0 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate stats for all texts\n",
    "lexical_stats = {}\n",
    "for author, tokens in tokens_all.items():\n",
    "    lexical_stats[author] = calculate_lexical_stats(tokens)\n",
    "\n",
    "# Display as DataFrame\n",
    "stats_df = pd.DataFrame(lexical_stats).T\n",
    "print(\"Lexical Statistics:\")\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize lexical diversity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Type-Token Ratio\n",
    "stats_df[\"ttr\"].plot(kind=\"bar\", ax=axes[0], color=\"steelblue\", alpha=0.7)\n",
    "axes[0].set_title(\"Type-Token Ratio (Vocabulary Richness)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Author\")\n",
    "axes[0].set_ylabel(\"TTR (higher = more diverse)\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Hapax Legomena\n",
    "stats_df[\"hapax_legomena\"].plot(kind=\"bar\", ax=axes[1], color=\"coral\", alpha=0.7)\n",
    "axes[1].set_title(\"Hapax Legomena (Words Used Once)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Author\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 words for each text (without stopwords)\n",
    "print(\"Top 10 Most Frequent Words (excluding stopwords):\\n\")\n",
    "\n",
    "for author, tokens in tokens_nostop.items():\n",
    "    freq_dist = Counter(tokens)\n",
    "    top_10 = freq_dist.most_common(10)\n",
    "\n",
    "    print(f\"{author}:\")\n",
    "    for word, count in top_10:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frequency distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (author, tokens) in enumerate(tokens_nostop.items()):\n",
    "    freq_dist = Counter(tokens)\n",
    "    top_15 = dict(freq_dist.most_common(15))\n",
    "\n",
    "    axes[idx].barh(list(top_15.keys()), list(top_15.values()), color=\"skyblue\", alpha=0.7)\n",
    "    axes[idx].set_title(f\"{author}: Top 15 Words\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Frequency\")\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for each text\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (author, text) in enumerate(texts.items()):\n",
    "    wordcloud = WordCloud(\n",
    "        width=400, height=300, background_color=\"white\", colormap=\"viridis\", max_words=50\n",
    "    ).generate(text)\n",
    "\n",
    "    axes[idx].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    axes[idx].axis(\"off\")\n",
    "    axes[idx].set_title(f\"{author}\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stylometric Analysis\n",
    "\n",
    "Analyze writing style through sentence structure and function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stylometric_features(text):\n",
    "    \"\"\"Calculate stylometric features for authorship attribution.\"\"\"\n",
    "    # Sentence statistics\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "\n",
    "    # Word length statistics\n",
    "    word_lengths = [len(word) for word in words if word.isalpha()]\n",
    "    avg_word_length = np.mean(word_lengths) if word_lengths else 0\n",
    "\n",
    "    # Function words (common in English)\n",
    "    function_words = [\"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"it\", \"for\"]\n",
    "    tokens_lower = [w.lower() for w in words if w.isalpha()]\n",
    "    function_word_freq = (\n",
    "        sum(tokens_lower.count(fw) for fw in function_words) / len(tokens_lower)\n",
    "        if tokens_lower\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"num_sentences\": len(sentences),\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"function_word_freq\": function_word_freq * 100,  # as percentage\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate stylometric features\n",
    "stylometric_stats = {}\n",
    "for author, text in texts.items():\n",
    "    stylometric_stats[author] = calculate_stylometric_features(text)\n",
    "\n",
    "style_df = pd.DataFrame(stylometric_stats).T\n",
    "print(\"Stylometric Features:\")\n",
    "print(style_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stylometric features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Average sentence length\n",
    "style_df[\"avg_sentence_length\"].plot(kind=\"bar\", ax=axes[0], color=\"purple\", alpha=0.7)\n",
    "axes[0].set_title(\"Average Sentence Length\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Author\")\n",
    "axes[0].set_ylabel(\"Words per Sentence\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Average word length\n",
    "style_df[\"avg_word_length\"].plot(kind=\"bar\", ax=axes[1], color=\"teal\", alpha=0.7)\n",
    "axes[1].set_title(\"Average Word Length\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Author\")\n",
    "axes[1].set_ylabel(\"Characters per Word\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Function word frequency\n",
    "style_df[\"function_word_freq\"].plot(kind=\"bar\", ax=axes[2], color=\"salmon\", alpha=0.7)\n",
    "axes[2].set_title(\"Function Word Frequency\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].set_xlabel(\"Author\")\n",
    "axes[2].set_ylabel(\"Percentage (%)\")\n",
    "axes[2].tick_params(axis=\"x\", rotation=45)\n",
    "axes[2].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words=\"english\")\n",
    "corpus = list(texts.values())\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=texts.keys(), columns=texts.keys())\n",
    "\n",
    "print(\"Text Similarity Matrix (Cosine Similarity):\")\n",
    "print(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    similarity_df,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cbar_kws={\"label\": \"Cosine Similarity\"},\n",
    ")\n",
    "ax.set_title(\"Text Similarity Heatmap\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Topic Modeling (LDA)\n",
    "\n",
    "Extract themes from the corpus using Latent Dirichlet Allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LDA\n",
    "count_vectorizer = CountVectorizer(max_features=50, stop_words=\"english\", max_df=0.95)\n",
    "doc_term_matrix = count_vectorizer.fit_transform(corpus)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Fit LDA model\n",
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)\n",
    "lda_output = lda.fit_transform(doc_term_matrix)\n",
    "\n",
    "\n",
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        topics[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "\n",
    "topics = display_topics(lda, feature_names, n_top_words=10)\n",
    "\n",
    "print(\"Discovered Topics:\\n\")\n",
    "for topic_name, words in topics.items():\n",
    "    print(f\"{topic_name}: {', '.join(words)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-topic distribution\n",
    "topic_distribution = pd.DataFrame(\n",
    "    lda_output, index=texts.keys(), columns=[f\"Topic {i + 1}\" for i in range(n_topics)]\n",
    ")\n",
    "\n",
    "print(\"Document-Topic Distribution:\")\n",
    "print(topic_distribution)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "topic_distribution.plot(kind=\"bar\", stacked=True, ax=ax, alpha=0.7)\n",
    "ax.set_title(\"Topic Distribution Across Texts\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Author\")\n",
    "ax.set_ylabel(\"Topic Proportion\")\n",
    "ax.legend(title=\"Topics\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    \"\"\"Extract named entities using NLTK.\"\"\"\n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Simple NER: extract proper nouns (NNP, NNPS)\n",
    "    entities = [word for word, pos in pos_tags if pos in [\"NNP\", \"NNPS\"]]\n",
    "\n",
    "    return Counter(entities)\n",
    "\n",
    "\n",
    "# Extract entities from all texts\n",
    "print(\"Named Entities (Proper Nouns):\\n\")\n",
    "\n",
    "for author, text in texts.items():\n",
    "    entities = extract_named_entities(text)\n",
    "    print(f\"{author}:\")\n",
    "    if entities:\n",
    "        for entity, count in entities.most_common(10):\n",
    "            print(f\"  {entity}: {count}\")\n",
    "    else:\n",
    "        print(\"  No named entities found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Part-of-Speech Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pos(text):\n",
    "    \"\"\"Analyze part-of-speech distribution.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Count POS tags\n",
    "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "\n",
    "    # Group into major categories\n",
    "    categories = {\n",
    "        \"Nouns\": sum(count for tag, count in pos_counts.items() if tag.startswith(\"NN\")),\n",
    "        \"Verbs\": sum(count for tag, count in pos_counts.items() if tag.startswith(\"VB\")),\n",
    "        \"Adjectives\": sum(count for tag, count in pos_counts.items() if tag.startswith(\"JJ\")),\n",
    "        \"Adverbs\": sum(count for tag, count in pos_counts.items() if tag.startswith(\"RB\")),\n",
    "        \"Pronouns\": sum(count for tag, count in pos_counts.items() if tag.startswith(\"PR\")),\n",
    "    }\n",
    "\n",
    "    return categories\n",
    "\n",
    "\n",
    "# Analyze POS for all texts\n",
    "pos_analysis = {}\n",
    "for author, text in texts.items():\n",
    "    pos_analysis[author] = analyze_pos(text)\n",
    "\n",
    "pos_df = pd.DataFrame(pos_analysis).T\n",
    "print(\"Part-of-Speech Distribution:\")\n",
    "print(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize POS distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "pos_df.plot(kind=\"bar\", ax=ax, alpha=0.7)\n",
    "ax.set_title(\"Part-of-Speech Distribution Across Texts\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Author\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend(title=\"POS Category\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_data = []\n",
    "\n",
    "for author in texts:\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Author\": author,\n",
    "            \"Characters\": len(texts[author]),\n",
    "            \"Tokens\": lexical_stats[author][\"total_tokens\"],\n",
    "            \"Unique Words\": lexical_stats[author][\"unique_tokens\"],\n",
    "            \"TTR\": f\"{lexical_stats[author]['ttr']:.3f}\",\n",
    "            \"Sentences\": stylometric_stats[author][\"num_sentences\"],\n",
    "            \"Avg Sent Length\": f\"{stylometric_stats[author]['avg_sentence_length']:.1f}\",\n",
    "            \"Avg Word Length\": f\"{stylometric_stats[author]['avg_word_length']:.2f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEXT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(\"text_analysis_summary.csv\", index=False)\n",
    "print(\"\\n✓ Summary saved to text_analysis_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Lexical Diversity\n",
    "- **Highest TTR**: [Author] shows greatest vocabulary diversity\n",
    "- **Hapax Legomena**: [Author] uses most unique words\n",
    "\n",
    "### Stylometric Patterns\n",
    "- **Longest Sentences**: [Author] uses most complex sentence structure\n",
    "- **Shortest Words**: [Author] favors simpler vocabulary\n",
    "- **Function Words**: [Author] has highest function word density\n",
    "\n",
    "### Text Similarity\n",
    "- Most similar texts: [Text A] and [Text B] (similarity: [value])\n",
    "- Least similar texts: [Text C] and [Text D] (similarity: [value])\n",
    "\n",
    "### Topics Identified\n",
    "1. **Topic 1**: [Theme description based on top words]\n",
    "2. **Topic 2**: [Theme description]\n",
    "3. **Topic 3**: [Theme description]\n",
    "\n",
    "## Research Applications\n",
    "\n",
    "### Authorship Attribution\n",
    "- Stylometric features distinguish different authors\n",
    "- Function word patterns reveal writing habits\n",
    "- Sentence structure complexity varies by author\n",
    "\n",
    "### Historical Analysis\n",
    "- Language evolution across time periods\n",
    "- Thematic shifts in discourse\n",
    "- Cultural context through vocabulary\n",
    "\n",
    "### Literary Studies\n",
    "- Genre classification\n",
    "- Influence detection\n",
    "- Stylistic comparison\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Expand Corpus**: Add more texts for robust analysis\n",
    "2. **Advanced NER**: Use spaCy for better entity recognition\n",
    "3. **Sentiment Analysis**: Track emotional tone\n",
    "4. **Network Analysis**: Build co-occurrence networks\n",
    "5. **Temporal Analysis**: Track language change over time\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Project Gutenberg](https://www.gutenberg.org/): Free public domain texts\n",
    "- [NLTK Book](https://www.nltk.org/book/): Natural Language Processing with Python\n",
    "- [Stylometry Guide](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python)\n",
    "- [HathiTrust](https://www.hathitrust.org/): Digital library of millions of texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
