{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Text Corpus Analysis with AWS\n",
    "\n",
    "This notebook demonstrates a complete digital humanities workflow using AWS services:\n",
    "- Download historical texts from Project Gutenberg\n",
    "- Upload corpus to S3 with metadata organization\n",
    "- Process texts with Lambda (NLP analysis)\n",
    "- Query results from DynamoDB\n",
    "- Visualize linguistic features and comparative analysis\n",
    "\n",
    "**Prerequisites:**\n",
    "- AWS credentials configured\n",
    "- S3 bucket created\n",
    "- Lambda function deployed\n",
    "- DynamoDB table created\n",
    "\n",
    "See `setup_guide.md` for AWS setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Add scripts directory to path\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "# Replace with your bucket name and region\n",
    "BUCKET_NAME = \"text-corpus-your-id\"  # Change this!\n",
    "DYNAMODB_TABLE = \"TextAnalysis\"\n",
    "LAMBDA_FUNCTION = \"process-text-document\"\n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=AWS_REGION)\n",
    "lambda_client = boto3.client(\"lambda\", region_name=AWS_REGION)\n",
    "\n",
    "# Verify access\n",
    "try:\n",
    "    s3.head_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"✓ Connected to S3 bucket: {BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error connecting to S3: {e}\")\n",
    "    print(\"  Please update BUCKET_NAME with your actual bucket name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Corpus to S3\n",
    "\n",
    "We'll use the upload script to download and upload texts from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import upload script\n",
    "from upload_to_s3 import SAMPLE_CORPUS, TextCorpusUploader\n",
    "\n",
    "# Initialize uploader\n",
    "uploader = TextCorpusUploader(bucket_name=BUCKET_NAME, local_dir=\"./corpus\")\n",
    "\n",
    "print(f\"Sample corpus contains {len(SAMPLE_CORPUS)} texts:\")\n",
    "for _gid, author, title, period, _genre in SAMPLE_CORPUS[:5]:\n",
    "    print(f\"  - {author}: {title} ({period})\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload corpus (this will take 5-10 minutes)\n",
    "# Uncomment to run (skip if already uploaded)\n",
    "\n",
    "# stats = uploader.upload_corpus(SAMPLE_CORPUS)\n",
    "# print(f\"\\nUpload complete!\")\n",
    "# print(f\"  Uploaded: {stats['uploaded']} texts\")\n",
    "# print(f\"  Failed: {stats['failed']} texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploads\n",
    "texts = uploader.list_uploaded_texts()\n",
    "print(f\"Found {len(texts)} texts in S3\")\n",
    "\n",
    "# Display sample\n",
    "if texts:\n",
    "    df_texts = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"key\": t[\"key\"],\n",
    "                \"author\": t[\"metadata\"].get(\"author\", \"Unknown\"),\n",
    "                \"title\": t[\"metadata\"].get(\"title\", \"Unknown\"),\n",
    "                \"period\": t[\"metadata\"].get(\"period\", \"Unknown\"),\n",
    "                \"size_kb\": t[\"size\"] / 1024,\n",
    "            }\n",
    "            for t in texts\n",
    "        ]\n",
    "    )\n",
    "    print(\"\\nSample texts:\")\n",
    "    print(df_texts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Texts with Lambda\n",
    "\n",
    "Lambda will automatically process texts if S3 trigger is configured.\n",
    "Otherwise, we can manually invoke Lambda for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if S3 trigger is configured\n",
    "try:\n",
    "    notification = s3.get_bucket_notification_configuration(Bucket=BUCKET_NAME)\n",
    "    if \"LambdaFunctionConfigurations\" in notification:\n",
    "        print(\"✓ S3 trigger configured - texts will be processed automatically\")\n",
    "    else:\n",
    "        print(\"⚠ S3 trigger not configured - manual processing required\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not check S3 notification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually invoke Lambda for a sample text (for testing)\n",
    "# Uncomment to process a specific text\n",
    "\n",
    "# sample_key = 'raw/austen/pride-and-prejudice.txt'\n",
    "# event = {\n",
    "#     'Records': [{\n",
    "#         's3': {\n",
    "#             'bucket': {'name': BUCKET_NAME},\n",
    "#             'object': {'key': sample_key}\n",
    "#         }\n",
    "#     }]\n",
    "# }\n",
    "\n",
    "# response = lambda_client.invoke(\n",
    "#     FunctionName=LAMBDA_FUNCTION,\n",
    "#     InvocationType='RequestResponse',\n",
    "#     Payload=json.dumps(event)\n",
    "# )\n",
    "\n",
    "# result = json.loads(response['Payload'].read())\n",
    "# print(f\"Lambda response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Results from DynamoDB\n",
    "\n",
    "Retrieve processed results and load into pandas for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import query script\n",
    "from query_results import TextAnalysisQuerier\n",
    "\n",
    "# Initialize querier\n",
    "querier = TextAnalysisQuerier(table_name=DYNAMODB_TABLE)\n",
    "\n",
    "# Get all documents\n",
    "documents = querier.get_all_documents()\n",
    "print(f\"Retrieved {len(documents)} analyzed documents from DynamoDB\")\n",
    "\n",
    "if not documents:\n",
    "    print(\"⚠ No documents found. Wait for Lambda processing to complete.\")\n",
    "    print(\"  Check CloudWatch logs: aws logs tail /aws/lambda/process-text-document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "if documents:\n",
    "    df = pd.DataFrame(documents)\n",
    "\n",
    "    # Display columns\n",
    "    print(\"Available columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Display sample\n",
    "    print(\"\\nSample documents:\")\n",
    "    display_cols = [\n",
    "        \"author\",\n",
    "        \"title\",\n",
    "        \"period\",\n",
    "        \"word_count\",\n",
    "        \"vocabulary_richness\",\n",
    "        \"avg_sentence_length\",\n",
    "    ]\n",
    "    display_cols = [col for col in display_cols if col in df.columns]\n",
    "    print(df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus statistics\n",
    "stats = querier.get_corpus_statistics()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Documents:              {stats.get('total_documents', 0):,}\")\n",
    "print(f\"Total Words:                  {stats.get('total_words', 0):,}\")\n",
    "print(f\"Average Vocabulary Richness:  {stats.get('avg_vocabulary_richness', 0):.4f}\")\n",
    "print(f\"Average Sentence Length:      {stats.get('avg_sentence_length', 0):.2f} words\")\n",
    "print(f\"Unique Authors:               {stats.get('authors', 0)}\")\n",
    "print(f\"Literary Periods:             {stats.get('periods', 0)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis\n",
    "\n",
    "Compare linguistic features across authors and periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Richness by Author\n",
    "if documents and len(documents) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    author_richness = (\n",
    "        df.groupby(\"author\")[\"vocabulary_richness\"].mean().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    author_richness.plot(kind=\"bar\", ax=ax, color=\"steelblue\")\n",
    "    ax.set_title(\"Vocabulary Richness by Author\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Author\", fontsize=12)\n",
    "    ax.set_ylabel(\"Average Type-Token Ratio\", fontsize=12)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nVocabulary Richness Rankings:\")\n",
    "    for i, (author, richness) in enumerate(author_richness.items(), 1):\n",
    "        print(f\"{i}. {author}: {richness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Length by Period\n",
    "if documents and len(documents) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    period_sentence = (\n",
    "        df.groupby(\"period\")[\"avg_sentence_length\"].mean().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    period_sentence.plot(kind=\"bar\", ax=ax, color=\"coral\")\n",
    "    ax.set_title(\"Average Sentence Length by Literary Period\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Period\", fontsize=12)\n",
    "    ax.set_ylabel(\"Average Sentence Length (words)\", fontsize=12)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nSentence Length by Period:\")\n",
    "    for period, length in period_sentence.items():\n",
    "        print(f\"{period}: {length:.2f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count Distribution\n",
    "if documents and len(documents) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Histogram\n",
    "    df[\"word_count\"].hist(bins=20, ax=ax1, color=\"skyblue\", edgecolor=\"black\")\n",
    "    ax1.set_title(\"Distribution of Word Counts\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.set_xlabel(\"Word Count\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Number of Documents\", fontsize=12)\n",
    "    ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Box plot by author\n",
    "    df.boxplot(column=\"word_count\", by=\"author\", ax=ax2)\n",
    "    ax2.set_title(\"Word Count by Author\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.set_xlabel(\"Author\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Word Count\", fontsize=12)\n",
    "    plt.suptitle(\"\")  # Remove default title\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Richness vs Word Count Scatter\n",
    "if documents and len(documents) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Color by author\n",
    "    for author in df[\"author\"].unique():\n",
    "        author_df = df[df[\"author\"] == author]\n",
    "        ax.scatter(\n",
    "            author_df[\"word_count\"],\n",
    "            author_df[\"vocabulary_richness\"],\n",
    "            label=author,\n",
    "            s=100,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Vocabulary Richness vs Document Length\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Word Count\", fontsize=12)\n",
    "    ax.set_ylabel(\"Vocabulary Richness (Type-Token Ratio)\", fontsize=12)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation\n",
    "    corr = df[\"word_count\"].corr(df[\"vocabulary_richness\"])\n",
    "    print(f\"\\nCorrelation between word count and vocabulary richness: {corr:.3f}\")\n",
    "    print(\"Note: Negative correlation is typical - longer texts tend to have lower TTR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis by Author\n",
    "\n",
    "Deep dive into specific authors' linguistic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query specific author\n",
    "author_name = \"Jane Austen\"  # Change to analyze different author\n",
    "\n",
    "author_docs = querier.query_by_author(author_name)\n",
    "print(f\"Found {len(author_docs)} documents by {author_name}\")\n",
    "\n",
    "if author_docs:\n",
    "    author_df = pd.DataFrame(author_docs)\n",
    "    print(\"\\nDocuments:\")\n",
    "    display_cols = [\"title\", \"word_count\", \"vocabulary_richness\", \"avg_sentence_length\"]\n",
    "    display_cols = [col for col in display_cols if col in author_df.columns]\n",
    "    print(author_df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Austen vs Dickens\n",
    "if documents and len(documents) > 0:\n",
    "    austen = querier.query_by_author(\"Jane Austen\")\n",
    "    dickens = querier.query_by_author(\"Charles Dickens\")\n",
    "\n",
    "    if austen and dickens:\n",
    "        austen_df = pd.DataFrame(austen)\n",
    "        dickens_df = pd.DataFrame(dickens)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        # Vocabulary Richness\n",
    "        data = [austen_df[\"vocabulary_richness\"], dickens_df[\"vocabulary_richness\"]]\n",
    "        axes[0, 0].boxplot(data, labels=[\"Austen\", \"Dickens\"])\n",
    "        axes[0, 0].set_title(\"Vocabulary Richness\", fontweight=\"bold\")\n",
    "        axes[0, 0].set_ylabel(\"Type-Token Ratio\")\n",
    "        axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Sentence Length\n",
    "        data = [austen_df[\"avg_sentence_length\"], dickens_df[\"avg_sentence_length\"]]\n",
    "        axes[0, 1].boxplot(data, labels=[\"Austen\", \"Dickens\"])\n",
    "        axes[0, 1].set_title(\"Sentence Length\", fontweight=\"bold\")\n",
    "        axes[0, 1].set_ylabel(\"Average Words per Sentence\")\n",
    "        axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Word Count\n",
    "        data = [austen_df[\"word_count\"], dickens_df[\"word_count\"]]\n",
    "        axes[1, 0].boxplot(data, labels=[\"Austen\", \"Dickens\"])\n",
    "        axes[1, 0].set_title(\"Document Length\", fontweight=\"bold\")\n",
    "        axes[1, 0].set_ylabel(\"Total Words\")\n",
    "        axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Unique Words\n",
    "        data = [austen_df[\"unique_words\"], dickens_df[\"unique_words\"]]\n",
    "        axes[1, 1].boxplot(data, labels=[\"Austen\", \"Dickens\"])\n",
    "        axes[1, 1].set_title(\"Vocabulary Size\", fontweight=\"bold\")\n",
    "        axes[1, 1].set_ylabel(\"Unique Words\")\n",
    "        axes[1, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Jane Austen vs Charles Dickens: Linguistic Comparison\", fontsize=16, fontweight=\"bold\"\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Period-Based Analysis\n",
    "\n",
    "Compare Romantic vs Victorian literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by period\n",
    "romantic_docs = querier.query_by_period(\"Romantic\")\n",
    "victorian_docs = querier.query_by_period(\"Victorian\")\n",
    "\n",
    "print(f\"Romantic period: {len(romantic_docs)} documents\")\n",
    "print(f\"Victorian period: {len(victorian_docs)} documents\")\n",
    "\n",
    "if romantic_docs and victorian_docs:\n",
    "    romantic_df = pd.DataFrame(romantic_docs)\n",
    "    victorian_df = pd.DataFrame(victorian_docs)\n",
    "\n",
    "    print(\"\\nRomantic Period Statistics:\")\n",
    "    print(f\"  Avg Vocabulary Richness: {romantic_df['vocabulary_richness'].mean():.4f}\")\n",
    "    print(f\"  Avg Sentence Length: {romantic_df['avg_sentence_length'].mean():.2f}\")\n",
    "    print(f\"  Avg Word Count: {romantic_df['word_count'].mean():.0f}\")\n",
    "\n",
    "    print(\"\\nVictorian Period Statistics:\")\n",
    "    print(f\"  Avg Vocabulary Richness: {victorian_df['vocabulary_richness'].mean():.4f}\")\n",
    "    print(f\"  Avg Sentence Length: {victorian_df['avg_sentence_length'].mean():.2f}\")\n",
    "    print(f\"  Avg Word Count: {victorian_df['word_count'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize period comparison\n",
    "if romantic_docs and victorian_docs:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    # Vocabulary Richness\n",
    "    data = [romantic_df[\"vocabulary_richness\"], victorian_df[\"vocabulary_richness\"]]\n",
    "    bp1 = axes[0].boxplot(data, labels=[\"Romantic\", \"Victorian\"], patch_artist=True)\n",
    "    for patch, color in zip(bp1[\"boxes\"], [\"lightblue\", \"lightcoral\"]):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[0].set_title(\"Vocabulary Richness\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Type-Token Ratio\", fontsize=11)\n",
    "    axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Sentence Length\n",
    "    data = [romantic_df[\"avg_sentence_length\"], victorian_df[\"avg_sentence_length\"]]\n",
    "    bp2 = axes[1].boxplot(data, labels=[\"Romantic\", \"Victorian\"], patch_artist=True)\n",
    "    for patch, color in zip(bp2[\"boxes\"], [\"lightblue\", \"lightcoral\"]):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[1].set_title(\"Sentence Length\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[1].set_ylabel(\"Words per Sentence\", fontsize=11)\n",
    "    axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Document Length\n",
    "    data = [romantic_df[\"word_count\"], victorian_df[\"word_count\"]]\n",
    "    bp3 = axes[2].boxplot(data, labels=[\"Romantic\", \"Victorian\"], patch_artist=True)\n",
    "    for patch, color in zip(bp3[\"boxes\"], [\"lightblue\", \"lightcoral\"]):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[2].set_title(\"Document Length\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[2].set_ylabel(\"Total Words\", fontsize=11)\n",
    "    axes[2].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Romantic vs Victorian Literature: Linguistic Features\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "Save analysis results for further research or publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "if documents:\n",
    "    output_file = \"../results/corpus_analysis.csv\"\n",
    "    Path(\"../results\").mkdir(exist_ok=True)\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Exported {len(df)} documents to {output_file}\")\n",
    "\n",
    "    # Export summary statistics\n",
    "    summary_file = \"../results/summary_statistics.txt\"\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        f.write(\"CORPUS SUMMARY STATISTICS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Total Documents: {len(df)}\\n\")\n",
    "        f.write(f\"Total Words: {df['word_count'].sum():,}\\n\")\n",
    "        f.write(f\"Avg Vocabulary Richness: {df['vocabulary_richness'].mean():.4f}\\n\")\n",
    "        f.write(f\"Avg Sentence Length: {df['avg_sentence_length'].mean():.2f}\\n\\n\")\n",
    "\n",
    "        f.write(\"By Author:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for author in sorted(df[\"author\"].unique()):\n",
    "            author_data = df[df[\"author\"] == author]\n",
    "            f.write(f\"\\n{author}:\\n\")\n",
    "            f.write(f\"  Documents: {len(author_data)}\\n\")\n",
    "            f.write(f\"  Avg Vocabulary Richness: {author_data['vocabulary_richness'].mean():.4f}\\n\")\n",
    "            f.write(f\"  Avg Sentence Length: {author_data['avg_sentence_length'].mean():.2f}\\n\")\n",
    "\n",
    "    print(f\"✓ Exported summary statistics to {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Further Analysis\n",
    "- Download detailed JSON results from S3 for word frequency analysis\n",
    "- Examine named entities (characters, places) from processed texts\n",
    "- Create word clouds for individual authors or periods\n",
    "- Perform topic modeling across the corpus\n",
    "- Build character co-occurrence networks\n",
    "\n",
    "### Expand Corpus\n",
    "- Add more authors and texts\n",
    "- Include different genres (poetry, drama)\n",
    "- Analyze translations\n",
    "- Compare different time periods\n",
    "\n",
    "### Advanced AWS Features\n",
    "- Use Athena for complex SQL queries\n",
    "- Set up automated processing with S3 triggers\n",
    "- Create dashboards with QuickSight\n",
    "- Move to Tier 3 for production infrastructure\n",
    "\n",
    "### Cleanup\n",
    "When finished, follow `cleanup_guide.md` to delete AWS resources and avoid charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"\\nProcessed {len(documents) if documents else 0} documents from {len(df['author'].unique()) if documents else 0} authors\"\n",
    ")\n",
    "print(\"\\nKey findings:\")\n",
    "if documents and len(documents) > 0:\n",
    "    top_author = df.groupby(\"author\")[\"vocabulary_richness\"].mean().idxmax()\n",
    "    top_richness = df.groupby(\"author\")[\"vocabulary_richness\"].mean().max()\n",
    "    print(f\"  - Highest vocabulary richness: {top_author} ({top_richness:.4f})\")\n",
    "\n",
    "    longest_author = df.groupby(\"author\")[\"avg_sentence_length\"].mean().idxmax()\n",
    "    longest_sentences = df.groupby(\"author\")[\"avg_sentence_length\"].mean().max()\n",
    "    print(f\"  - Longest sentences: {longest_author} ({longest_sentences:.2f} words)\")\n",
    "\n",
    "    most_words = df.groupby(\"author\")[\"word_count\"].sum().idxmax()\n",
    "    total_words = df.groupby(\"author\")[\"word_count\"].sum().max()\n",
    "    print(f\"  - Most prolific: {most_words} ({total_words:,} words)\")\n",
    "\n",
    "print(\"\\nResults exported to ../results/\")\n",
    "print(\"\\nRemember to run cleanup_guide.md when finished to delete AWS resources!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
