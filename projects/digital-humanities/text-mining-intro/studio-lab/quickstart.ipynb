{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Introduction: Historical Document Analysis\n",
    "\n",
    "Learn digital humanities fundamentals through 19th century text analysis.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "20 historical documents (1850-1900) about industrialization:\n",
    "- Topics: Labor, economy, technology, society\n",
    "- Authors and publication years\n",
    "- Industrial revolution period\n",
    "\n",
    "## Methods\n",
    "- Text preprocessing\n",
    "- Word frequency analysis\n",
    "- TF-IDF scoring\n",
    "- Temporal trends\n",
    "- Topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "df = pd.read_csv(\"sample_texts.csv\")\n",
    "\n",
    "print(f\"Number of documents: {len(df)}\")\n",
    "print(f\"Time period: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"\\nAuthors: {df['author'].nunique()}\")\n",
    "print(f\"Average text length: {df['text'].str.len().mean():.0f} characters\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    # Remove short words\n",
    "    tokens = [w for w in tokens if len(w) > 3]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"tokens\"] = df[\"text\"].apply(preprocess_text)\n",
    "df[\"word_count\"] = df[\"tokens\"].apply(len)\n",
    "\n",
    "print(\"Example preprocessing:\")\n",
    "print(f\"Original: {df['text'].iloc[0][:100]}...\")\n",
    "print(f\"Tokens: {df['tokens'].iloc[0][:15]}\")\n",
    "print(f\"\\nAverage words per document: {df['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all words\n",
    "all_words = []\n",
    "for tokens in df[\"tokens\"]:\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "most_common = word_freq.most_common(20)\n",
    "\n",
    "# Plot most frequent words\n",
    "words = [w[0] for w in most_common]\n",
    "counts = [w[1] for w in most_common]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(words, counts, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Frequency\", fontsize=12)\n",
    "ax.set_title(\"Top 20 Most Frequent Words\", fontsize=14, fontweight=\"bold\")\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 Most Common Words:\")\n",
    "for word, count in most_common:\n",
    "    print(f\"  {word:.<25} {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words=\"english\", min_df=2)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"text\"])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get top terms for each document\n",
    "print(\"Top TF-IDF Terms by Document:\\n\")\n",
    "for idx in [0, 5, 10, 15]:  # Sample documents\n",
    "    doc_tfidf = tfidf_matrix[idx].toarray()[0]\n",
    "    top_indices = doc_tfidf.argsort()[-5:][::-1]\n",
    "    top_terms = [(feature_names[i], doc_tfidf[i]) for i in top_indices]\n",
    "\n",
    "    print(f\"{df['title'].iloc[idx]} ({df['year'].iloc[idx]}):\")\n",
    "    for term, score in top_terms:\n",
    "        print(f\"  {term}: {score:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Overall most important terms\n",
    "mean_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "top_terms_overall = sorted(zip(feature_names, mean_tfidf), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "# Plot\n",
    "terms = [t[0] for t in top_terms_overall]\n",
    "scores = [t[1] for t in top_terms_overall]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(terms, scores, color=\"green\", alpha=0.7, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Average TF-IDF Score\", fontsize=12)\n",
    "ax.set_title(\"Most Distinctive Terms (TF-IDF)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"TF-IDF identifies words that are distinctive to documents,\")\n",
    "print(\"not just frequent overall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track key terms over time\n",
    "key_terms = [\"workers\", \"industrial\", \"society\", \"economic\", \"technology\"]\n",
    "\n",
    "# Count occurrences by year\n",
    "term_by_year = {term: [] for term in key_terms}\n",
    "years = sorted(df[\"year\"].unique())\n",
    "\n",
    "for year in years:\n",
    "    year_docs = df[df[\"year\"] == year]\n",
    "    year_text = \" \".join(year_docs[\"text\"].values).lower()\n",
    "\n",
    "    for term in key_terms:\n",
    "        count = year_text.count(term)\n",
    "        term_by_year[term].append(count)\n",
    "\n",
    "# Plot trends\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for term in key_terms:\n",
    "    ax.plot(years, term_by_year[term], marker=\"o\", linewidth=2, label=term.capitalize())\n",
    "\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax.set_title(\"Key Terms Over Time (1850-1900)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Temporal analysis reveals how vocabulary changes over time.\")\n",
    "print(\"Different themes gain or lose prominence across decades.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document similarity using TF-IDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Create DataFrame with document titles\n",
    "sim_df = pd.DataFrame(similarity_matrix, index=df[\"title\"], columns=df[\"title\"])\n",
    "\n",
    "# Plot heatmap (sample)\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    sim_df.iloc[:10, :10],\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cbar_kws={\"label\": \"Cosine Similarity\"},\n",
    ")\n",
    "ax.set_title(\"Document Similarity Matrix (First 10 Documents)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most similar document pairs\n",
    "print(\"\\nMost Similar Document Pairs:\")\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        if similarity_matrix[i, j] > 0.6:  # Threshold\n",
    "            print(\n",
    "                f\"  {df['title'].iloc[i]} <-> {df['title'].iloc[j]}: {similarity_matrix[i, j]:.3f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Topic Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple topic identification by clustering key terms\n",
    "# Group documents by their most distinctive terms\n",
    "\n",
    "topic_groups = {}\n",
    "for idx, row in df.iterrows():\n",
    "    doc_tfidf = tfidf_matrix[idx].toarray()[0]\n",
    "    top_term_idx = doc_tfidf.argmax()\n",
    "    top_term = feature_names[top_term_idx]\n",
    "\n",
    "    if top_term not in topic_groups:\n",
    "        topic_groups[top_term] = []\n",
    "    topic_groups[top_term].append(row[\"title\"])\n",
    "\n",
    "print(\"Document Groupings by Dominant Theme:\\n\")\n",
    "for theme, docs in sorted(topic_groups.items(), key=lambda x: len(x[1]), reverse=True)[:5]:\n",
    "    print(f\"Theme: {theme.upper()}\")\n",
    "    for doc in docs:\n",
    "        print(f\"  - {doc}\")\n",
    "    print()\n",
    "\n",
    "# Count documents per decade\n",
    "df[\"decade\"] = (df[\"year\"] // 10) * 10\n",
    "decade_counts = df[\"decade\"].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nDocuments by Decade:\")\n",
    "for decade, count in decade_counts.items():\n",
    "    print(f\"  {decade}s: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"Total Documents\": len(df),\n",
    "    \"Time Span\": f\"{df['year'].min()}-{df['year'].max()}\",\n",
    "    \"Unique Authors\": df[\"author\"].nunique(),\n",
    "    \"Total Words\": sum(all_words for all_words in [len(tokens) for tokens in df[\"tokens\"]]),\n",
    "    \"Unique Words\": len(word_freq),\n",
    "    \"Avg Words/Doc\": f\"{df['word_count'].mean():.1f}\",\n",
    "    \"Most Common Word\": most_common[0][0],\n",
    "    \"Top TF-IDF Term\": top_terms_overall[0][0],\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT MINING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:.<40} {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  1. Industrial and economic themes dominate the corpus\")\n",
    "print(\"  2. Vocabulary shifts over the 50-year period\")\n",
    "print(\"  3. Documents cluster by thematic similarity\")\n",
    "print(\"  4. TF-IDF reveals distinctive terminology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Learned\n",
    "\n",
    "### Text Preprocessing\n",
    "- **Tokenization**: Splitting text into words\n",
    "- **Stop words**: Common words removed (the, and, is)\n",
    "- **Normalization**: Lowercasing, punctuation removal\n",
    "\n",
    "### Frequency Analysis\n",
    "- **Term frequency**: How often words appear\n",
    "- **Word counts**: Basic frequency statistics\n",
    "- **Common terms**: Most frequent vocabulary\n",
    "\n",
    "### TF-IDF\n",
    "- **Term Frequency**: Word frequency in document\n",
    "- **Inverse Document Frequency**: Rarity across corpus\n",
    "- **Distinctive terms**: Words unique to documents\n",
    "\n",
    "### Temporal Analysis\n",
    "- **Trends over time**: Vocabulary changes\n",
    "- **Historical periods**: Era-specific terms\n",
    "- **Thematic evolution**: Shifting topics\n",
    "\n",
    "### Document Similarity\n",
    "- **Cosine similarity**: Vector-based comparison\n",
    "- **Thematic clustering**: Grouping similar texts\n",
    "- **Relationships**: Connections between documents\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Extend the Analysis\n",
    "- Named entity recognition (people, places)\n",
    "- Sentiment analysis\n",
    "- Topic modeling (LDA)\n",
    "- Network analysis of concepts\n",
    "\n",
    "### Real Historical Corpora\n",
    "- **[Project Gutenberg](https://www.gutenberg.org/)**: 70,000+ free books\n",
    "- **[HathiTrust](https://www.hathitrust.org/)**: Millions of digitized texts\n",
    "- **[Internet Archive](https://archive.org/)**: Historical documents\n",
    "- **[Chronicling America](https://chroniclingamerica.loc.gov/)**: Historical newspapers\n",
    "\n",
    "### Advanced Methods\n",
    "- Word embeddings (Word2Vec, GloVe)\n",
    "- Topic modeling at scale\n",
    "- Neural language models\n",
    "- Stylometry and authorship attribution\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **[NLTK Book](https://www.nltk.org/book/)**: Natural Language Processing with Python\n",
    "- **[Voyant Tools](https://voyant-tools.org/)**: Web-based text analysis\n",
    "- **[Programming Historian](https://programminghistorian.org/)**: Digital humanities tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
