{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORDEX Regional Climate Data Loading and Exploration\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **Coordinated Regional Climate Downscaling Experiment (CORDEX)** is an international effort to generate high-resolution regional climate projections using Regional Climate Models (RCMs) driven by Global Climate Models (GCMs). This notebook provides a comprehensive guide to loading, exploring, and understanding CORDEX data structures.\n",
    "\n",
    "### What is CORDEX?\n",
    "\n",
    "CORDEX produces regional climate projections at resolutions of ~12-50 km (compared to GCM resolutions of ~100-300 km), making them suitable for regional impact studies. The framework consists of:\n",
    "\n",
    "- **GCMs (Global Climate Models)**: Provide boundary conditions (e.g., CNRM-CM5, MPI-ESM-LR)\n",
    "- **RCMs (Regional Climate Models)**: Dynamically downscale GCM outputs (e.g., RCA4, RegCM4, WRF)\n",
    "- **CORDEX Domains**: Geographic regions (e.g., EUR-44, NAM-44, AFR-44)\n",
    "- **Scenarios**: RCP2.6, RCP4.5, RCP8.5 (Representative Concentration Pathways)\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand CORDEX data structure and naming conventions\n",
    "2. Load and inspect CORDEX NetCDF files using xarray\n",
    "3. Visualize regional climate data on geographic projections\n",
    "4. Extract time series for specific locations or regions\n",
    "5. Perform data quality checks and validation\n",
    "6. Handle multiple files and large datasets efficiently using dask\n",
    "7. Apply best practices for memory management with climate data\n",
    "8. Export processed data for downstream analysis\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "This notebook follows a production-ready workflow for climate data analysis, demonstrating:\n",
    "- Proper handling of CF-compliant NetCDF files\n",
    "- Efficient memory management for large datasets\n",
    "- Professional visualization using cartopy\n",
    "- Reproducible data processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required libraries for climate data analysis. This notebook uses:\n",
    "\n",
    "- **xarray**: For labeled multi-dimensional arrays (CF-compliant NetCDF)\n",
    "- **numpy/pandas**: For numerical operations and tabular data\n",
    "- **matplotlib**: For visualization\n",
    "- **cartopy**: For geographic projections and mapping\n",
    "- **dask**: For lazy loading and parallel computation on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing libraries\n",
    "# Standard library\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Geographic projections and mapping\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Parallel computing and lazy loading\n",
    "import dask\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Configure display options\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "xr.set_options(display_style=\"html\")\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Matplotlib configuration\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "print(f\"xarray version: {xr.__version__}\")\n",
    "print(f\"dask version: {dask.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding CORDEX Data Structure\n",
    "\n",
    "### CORDEX Naming Convention\n",
    "\n",
    "CORDEX files follow a standardized naming convention:\n",
    "\n",
    "```\n",
    "<variable>_<domain>_<driving_model>_<experiment>_<ensemble>_<rcm_model>_<rcm_version>_<frequency>_<start_time>-<end_time>.nc\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "tas_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\n",
    "```\n",
    "\n",
    "Breaking this down:\n",
    "- **tas**: Near-surface air temperature (variable)\n",
    "- **EUR-44**: European domain at ~44 km resolution\n",
    "- **MPI-ESM-LR**: Driving GCM (Max Planck Institute Earth System Model)\n",
    "- **rcp85**: Representative Concentration Pathway 8.5 W/m²\n",
    "- **r1i1p1**: Realization, initialization, physics ensemble member\n",
    "- **SMHI-RCA4**: Regional model (Rossby Centre RCA4)\n",
    "- **v1**: Model version\n",
    "- **day**: Daily temporal frequency\n",
    "- **20060101-20101231**: Time range\n",
    "\n",
    "### Model Hierarchy\n",
    "\n",
    "```\n",
    "GCM (Global Climate Model)\n",
    "  ↓ boundary conditions\n",
    "RCM (Regional Climate Model)\n",
    "  ↓ high-resolution output\n",
    "CORDEX Data (50km, 25km, or 12km)\n",
    "```\n",
    "\n",
    "### Common CORDEX Variables\n",
    "\n",
    "| Variable | Standard Name | Units | Description |\n",
    "|----------|---------------|-------|-------------|\n",
    "| tas | air_temperature | K | Near-surface air temperature |\n",
    "| pr | precipitation_flux | kg m⁻² s⁻¹ | Precipitation |\n",
    "| tasmax | air_temperature | K | Daily maximum temperature |\n",
    "| tasmin | air_temperature | K | Daily minimum temperature |\n",
    "| hurs | relative_humidity | % | Near-surface relative humidity |\n",
    "| sfcWind | wind_speed | m s⁻¹ | Near-surface wind speed |\n",
    "| psl | air_pressure_at_sea_level | Pa | Sea level pressure |\n",
    "\n",
    "### Emission Scenarios (RCPs)\n",
    "\n",
    "- **RCP2.6**: Strong mitigation, peak ~3 W/m² by 2100, decline to 2.6 W/m²\n",
    "- **RCP4.5**: Intermediate stabilization, ~4.5 W/m² by 2100\n",
    "- **RCP8.5**: High emissions, business-as-usual, ~8.5 W/m² by 2100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample CORDEX Data\n",
    "\n",
    "For demonstration purposes, we'll create synthetic CORDEX-like data with realistic structure, dimensions, and metadata. This allows us to work through the analysis workflow even without access to actual CORDEX files.\n",
    "\n",
    "The synthetic data will represent:\n",
    "- **Domain**: EUR-44 (European domain, 0.44° resolution)\n",
    "- **Variables**: tas (temperature), pr (precipitation)\n",
    "- **Time period**: 2006-2010 (5 years, daily data)\n",
    "- **Spatial extent**: Central Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_cordex_data(output_dir=\"./data\", create_file=True):\n",
    "    \"\"\"\n",
    "    Create synthetic CORDEX-like NetCDF data with realistic structure.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_dir : str\n",
    "        Directory to save the sample NetCDF file\n",
    "    create_file : bool\n",
    "        If True, save to disk; if False, return xarray Dataset only\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds : xarray.Dataset\n",
    "        Sample CORDEX dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Define spatial domain (Central Europe)\n",
    "    lat = np.arange(42.0, 58.0, 0.44)  # ~44 km resolution\n",
    "    lon = np.arange(2.0, 18.0, 0.44)\n",
    "\n",
    "    # Define temporal dimension (5 years, daily)\n",
    "    start_date = \"2006-01-01\"\n",
    "    end_date = \"2010-12-31\"\n",
    "    time = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # Create meshgrid for spatial coordinates\n",
    "    _lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
    "\n",
    "    # Generate synthetic temperature data (tas)\n",
    "    # Base temperature with seasonal cycle and spatial gradient\n",
    "    days_of_year = time.dayofyear.values\n",
    "    years = time.year.values\n",
    "\n",
    "    # Seasonal cycle (warmer in summer, colder in winter)\n",
    "    seasonal_cycle = 15 * np.sin(2 * np.pi * (days_of_year - 80) / 365.25)\n",
    "\n",
    "    # Warming trend (0.03 K/year for RCP8.5)\n",
    "    trend = 0.03 * (years - 2006)\n",
    "\n",
    "    # Spatial pattern (cooler in north, warmer in south)\n",
    "    base_temp = 273.15 + 10 + 0.5 * (lat_grid - lat.mean())\n",
    "\n",
    "    # Combine components with random variability\n",
    "    tas_data = np.zeros((len(time), len(lat), len(lon)))\n",
    "    for i in range(len(time)):\n",
    "        tas_data[i] = (\n",
    "            base_temp + seasonal_cycle[i] + trend[i] + np.random.normal(0, 2, (len(lat), len(lon)))\n",
    "        )\n",
    "\n",
    "    # Generate synthetic precipitation data (pr)\n",
    "    # Higher precipitation in winter, spatial variability\n",
    "    seasonal_pr = 2.0e-5 * (1 - 0.4 * np.sin(2 * np.pi * (days_of_year - 80) / 365.25))\n",
    "\n",
    "    pr_data = np.zeros((len(time), len(lat), len(lon)))\n",
    "    for i in range(len(time)):\n",
    "        # Precipitation with spatial pattern and random events\n",
    "        base_pr = seasonal_pr[i] * (1 + 0.3 * np.sin(lat_grid / 10))\n",
    "        random_pr = np.random.gamma(2, seasonal_pr[i] / 2, (len(lat), len(lon)))\n",
    "        pr_data[i] = np.maximum(0, base_pr + random_pr)\n",
    "\n",
    "    # Create xarray Dataset with proper metadata\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"tas\": (\n",
    "                [\"time\", \"lat\", \"lon\"],\n",
    "                tas_data,\n",
    "                {\n",
    "                    \"standard_name\": \"air_temperature\",\n",
    "                    \"long_name\": \"Near-Surface Air Temperature\",\n",
    "                    \"units\": \"K\",\n",
    "                    \"cell_methods\": \"time: mean\",\n",
    "                    \"missing_value\": 1.0e20,\n",
    "                },\n",
    "            ),\n",
    "            \"pr\": (\n",
    "                [\"time\", \"lat\", \"lon\"],\n",
    "                pr_data,\n",
    "                {\n",
    "                    \"standard_name\": \"precipitation_flux\",\n",
    "                    \"long_name\": \"Precipitation\",\n",
    "                    \"units\": \"kg m-2 s-1\",\n",
    "                    \"cell_methods\": \"time: mean\",\n",
    "                    \"missing_value\": 1.0e20,\n",
    "                },\n",
    "            ),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": (\n",
    "                [\"time\"],\n",
    "                time,\n",
    "                {\"standard_name\": \"time\", \"long_name\": \"time\", \"axis\": \"T\", \"calendar\": \"standard\"},\n",
    "            ),\n",
    "            \"lat\": (\n",
    "                [\"lat\"],\n",
    "                lat,\n",
    "                {\n",
    "                    \"standard_name\": \"latitude\",\n",
    "                    \"long_name\": \"latitude\",\n",
    "                    \"units\": \"degrees_north\",\n",
    "                    \"axis\": \"Y\",\n",
    "                },\n",
    "            ),\n",
    "            \"lon\": (\n",
    "                [\"lon\"],\n",
    "                lon,\n",
    "                {\n",
    "                    \"standard_name\": \"longitude\",\n",
    "                    \"long_name\": \"longitude\",\n",
    "                    \"units\": \"degrees_east\",\n",
    "                    \"axis\": \"X\",\n",
    "                },\n",
    "            ),\n",
    "        },\n",
    "        attrs={\n",
    "            \"Conventions\": \"CF-1.6\",\n",
    "            \"title\": \"Sample CORDEX EUR-44 Regional Climate Model Data\",\n",
    "            \"institution\": \"Climate Research Lab (Synthetic Data)\",\n",
    "            \"source\": \"SMHI-RCA4 driven by MPI-ESM-LR\",\n",
    "            \"experiment\": \"RCP8.5\",\n",
    "            \"experiment_id\": \"rcp85\",\n",
    "            \"driving_model_id\": \"MPI-ESM-LR\",\n",
    "            \"driving_model_ensemble_member\": \"r1i1p1\",\n",
    "            \"model_id\": \"SMHI-RCA4\",\n",
    "            \"rcm_version_id\": \"v1\",\n",
    "            \"CORDEX_domain\": \"EUR-44\",\n",
    "            \"frequency\": \"day\",\n",
    "            \"contact\": \"cordex@climate.research\",\n",
    "            \"creation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"history\": \"Synthetic data created for demonstration purposes\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Optionally save to disk\n",
    "    if create_file:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        filename = \"tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Use compression for efficient storage\n",
    "        encoding = {\"tas\": {\"zlib\": True, \"complevel\": 5}, \"pr\": {\"zlib\": True, \"complevel\": 5}}\n",
    "\n",
    "        ds.to_netcdf(filepath, encoding=encoding)\n",
    "        print(f\"Sample data saved to: {filepath}\")\n",
    "        print(f\"File size: {os.path.getsize(filepath) / 1e6:.2f} MB\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Create sample dataset\n",
    "ds_sample = create_sample_cordex_data(output_dir=\"./data\", create_file=True)\n",
    "print(\"\\nSample CORDEX dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CORDEX Data\n",
    "\n",
    "Load the CORDEX NetCDF file using xarray. The `open_dataset` function reads the file metadata immediately but loads data arrays lazily (only when needed).\n",
    "\n",
    "### Loading Strategies:\n",
    "\n",
    "1. **`xr.open_dataset()`**: Single file, lazy loading\n",
    "2. **`xr.open_mfdataset()`**: Multiple files, concatenate along dimension\n",
    "3. **`chunks` parameter**: Control dask chunking for parallel operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "data_dir = \"./data\"\n",
    "filename = \"tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\"\n",
    "filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "# Load dataset with chunking for efficient memory usage\n",
    "ds = xr.open_dataset(\n",
    "    filepath,\n",
    "    chunks={\"time\": 365, \"lat\": 20, \"lon\": 20},  # Chunk by year and spatial blocks\n",
    ")\n",
    "\n",
    "print(\"Dataset loaded successfully!\\n\")\n",
    "print(f\"File: {filename}\")\n",
    "print(\"Memory: Lazy loading enabled via dask\")\n",
    "\n",
    "# Display dataset structure\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Data Structure\n",
    "\n",
    "Examine the dataset's dimensions, coordinates, variables, and metadata. Understanding the data structure is crucial for correct analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dimensions\n",
    "print(\"=\" * 60)\n",
    "print(\"DIMENSIONS\")\n",
    "print(\"=\" * 60)\n",
    "for dim, size in ds.dims.items():\n",
    "    print(f\"{dim:15s}: {size:6d}\")\n",
    "\n",
    "# Coordinates\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COORDINATES\")\n",
    "print(\"=\" * 60)\n",
    "for coord in ds.coords:\n",
    "    coord_data = ds.coords[coord]\n",
    "    print(f\"\\n{coord}:\")\n",
    "    print(f\"  Shape: {coord_data.shape}\")\n",
    "    print(f\"  Dtype: {coord_data.dtype}\")\n",
    "    if coord == \"time\":\n",
    "        print(f\"  Range: {coord_data.values[0]} to {coord_data.values[-1]}\")\n",
    "        print(f\"  Duration: {len(coord_data)} days ({len(coord_data) / 365.25:.1f} years)\")\n",
    "    else:\n",
    "        print(f\"  Range: [{coord_data.values.min():.2f}, {coord_data.values.max():.2f}]\")\n",
    "        print(f\"  Resolution: {np.diff(coord_data.values).mean():.4f}°\")\n",
    "\n",
    "# Data variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA VARIABLES\")\n",
    "print(\"=\" * 60)\n",
    "for var in ds.data_vars:\n",
    "    var_data = ds[var]\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Long name: {var_data.attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"  Standard name: {var_data.attrs.get('standard_name', 'N/A')}\")\n",
    "    print(f\"  Units: {var_data.attrs.get('units', 'N/A')}\")\n",
    "    print(f\"  Shape: {var_data.shape}\")\n",
    "    print(f\"  Dtype: {var_data.dtype}\")\n",
    "    print(f\"  Chunks: {var_data.chunks if hasattr(var_data.data, 'chunks') else 'Not chunked'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global attributes (metadata)\n",
    "print(\"=\" * 60)\n",
    "print(\"GLOBAL ATTRIBUTES (Metadata)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "important_attrs = [\n",
    "    \"title\",\n",
    "    \"institution\",\n",
    "    \"source\",\n",
    "    \"experiment\",\n",
    "    \"experiment_id\",\n",
    "    \"driving_model_id\",\n",
    "    \"model_id\",\n",
    "    \"CORDEX_domain\",\n",
    "    \"frequency\",\n",
    "]\n",
    "\n",
    "for attr in important_attrs:\n",
    "    if attr in ds.attrs:\n",
    "        print(f\"{attr:25s}: {ds.attrs[attr]}\")\n",
    "\n",
    "print(\"\\nAll attributes:\")\n",
    "for key, value in ds.attrs.items():\n",
    "    if key not in important_attrs:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time range analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL COVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "time_values = ds.time.values\n",
    "time_index = pd.DatetimeIndex(time_values)\n",
    "\n",
    "print(f\"Start date: {time_index[0]}\")\n",
    "print(f\"End date: {time_index[-1]}\")\n",
    "print(f\"Total days: {len(time_index)}\")\n",
    "print(f\"Total years: {len(time_index) / 365.25:.2f}\")\n",
    "print(f\"\\nYears covered: {sorted(time_index.year.unique().tolist())}\")\n",
    "\n",
    "# Check for temporal gaps\n",
    "time_diffs = np.diff(time_index)\n",
    "expected_diff = np.timedelta64(1, \"D\")\n",
    "gaps = np.where(time_diffs != expected_diff)[0]\n",
    "\n",
    "if len(gaps) > 0:\n",
    "    print(f\"\\nWARNING: Found {len(gaps)} temporal gaps!\")\n",
    "    for gap_idx in gaps[:5]:  # Show first 5 gaps\n",
    "        print(f\"  Gap at index {gap_idx}: {time_diffs[gap_idx]}\")\n",
    "else:\n",
    "    print(\"\\nTemporal continuity: ✓ No gaps detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial extent analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"SPATIAL COVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lat = ds.lat.values\n",
    "lon = ds.lon.values\n",
    "\n",
    "print(f\"Latitude range: [{lat.min():.2f}°N, {lat.max():.2f}°N]\")\n",
    "print(f\"Longitude range: [{lon.min():.2f}°E, {lon.max():.2f}°E]\")\n",
    "print(f\"\\nGrid dimensions: {len(lat)} × {len(lon)} points\")\n",
    "print(f\"Latitude resolution: ~{np.diff(lat).mean():.4f}° (~{np.diff(lat).mean() * 111:.1f} km)\")\n",
    "print(\n",
    "    f\"Longitude resolution: ~{np.diff(lon).mean():.4f}° (~{np.diff(lon).mean() * 111 * np.cos(np.radians(lat.mean())):.1f} km)\"\n",
    ")\n",
    "\n",
    "# Calculate approximate area\n",
    "lat_extent = lat.max() - lat.min()\n",
    "lon_extent = lon.max() - lon.min()\n",
    "area_approx = lat_extent * lon_extent * (111 * 111)  # Rough approximation in km²\n",
    "\n",
    "print(f\"\\nApproximate domain area: {area_approx:,.0f} km²\")\n",
    "print(f\"Domain extent: {lat_extent:.1f}° lat × {lon_extent:.1f}° lon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spatial Visualization\n",
    "\n",
    "Visualize the CORDEX regional domain and climate variables using cartopy for geographic projections. We'll create professional maps showing:\n",
    "\n",
    "1. Mean temperature field\n",
    "2. Mean precipitation field\n",
    "3. Seasonal composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal means for visualization\n",
    "tas_mean = ds.tas.mean(dim=\"time\").compute()  # Compute to load into memory\n",
    "pr_mean = ds.pr.mean(dim=\"time\").compute()\n",
    "\n",
    "# Convert units for better readability\n",
    "tas_mean_celsius = tas_mean - 273.15  # K to °C\n",
    "pr_mean_mmday = pr_mean * 86400  # kg m-2 s-1 to mm/day\n",
    "\n",
    "print(\"Temporal means calculated.\")\n",
    "print(f\"Mean temperature: {tas_mean_celsius.mean().values:.2f} °C\")\n",
    "print(f\"Mean precipitation: {pr_mean_mmday.mean().values:.2f} mm/day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean temperature field\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.LAND, facecolor=\"lightgray\", alpha=0.3)\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle=\":\")\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS, linewidth=0.5)\n",
    "\n",
    "# Plot temperature data\n",
    "im = ax.pcolormesh(\n",
    "    ds.lon,\n",
    "    ds.lat,\n",
    "    tas_mean_celsius,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"RdYlBu_r\",\n",
    "    vmin=tas_mean_celsius.quantile(0.05),\n",
    "    vmax=tas_mean_celsius.quantile(0.95),\n",
    "    shading=\"auto\",\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, orientation=\"horizontal\", pad=0.05, shrink=0.8)\n",
    "cbar.set_label(\"Mean Near-Surface Air Temperature (°C)\", fontsize=12)\n",
    "\n",
    "# Add gridlines\n",
    "gl = ax.gridlines(draw_labels=True, linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Set extent and title\n",
    "ax.set_extent([ds.lon.min(), ds.lon.max(), ds.lat.min(), ds.lat.max()], crs=ccrs.PlateCarree())\n",
    "plt.title(\n",
    "    f\"CORDEX EUR-44: Mean Temperature (2006-2010)\\n\"\n",
    "    f\"Model: {ds.attrs.get('model_id', 'N/A')} driven by {ds.attrs.get('driving_model_id', 'N/A')} ({ds.attrs.get('experiment_id', 'N/A')})\",\n",
    "    fontsize=14,\n",
    "    pad=20,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean precipitation field\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.LAND, facecolor=\"lightgray\", alpha=0.3)\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle=\":\")\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS, linewidth=0.5)\n",
    "\n",
    "# Plot precipitation data with logarithmic colormap for better visibility\n",
    "im = ax.pcolormesh(\n",
    "    ds.lon,\n",
    "    ds.lat,\n",
    "    pr_mean_mmday,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"YlGnBu\",\n",
    "    vmin=0,\n",
    "    vmax=pr_mean_mmday.quantile(0.95),\n",
    "    shading=\"auto\",\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, orientation=\"horizontal\", pad=0.05, shrink=0.8)\n",
    "cbar.set_label(\"Mean Precipitation (mm/day)\", fontsize=12)\n",
    "\n",
    "# Add gridlines\n",
    "gl = ax.gridlines(draw_labels=True, linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Set extent and title\n",
    "ax.set_extent([ds.lon.min(), ds.lon.max(), ds.lat.min(), ds.lat.max()], crs=ccrs.PlateCarree())\n",
    "plt.title(\n",
    "    f\"CORDEX EUR-44: Mean Precipitation (2006-2010)\\n\"\n",
    "    f\"Model: {ds.attrs.get('model_id', 'N/A')} driven by {ds.attrs.get('driving_model_id', 'N/A')} ({ds.attrs.get('experiment_id', 'N/A')})\",\n",
    "    fontsize=14,\n",
    "    pad=20,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal composites\n",
    "# Group data by season and calculate means\n",
    "ds_seasonal = ds.groupby(\"time.season\").mean(dim=\"time\")\n",
    "\n",
    "# Define season order\n",
    "season_order = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "season_names = {\"DJF\": \"Winter\", \"MAM\": \"Spring\", \"JJA\": \"Summer\", \"SON\": \"Autumn\"}\n",
    "\n",
    "# Create subplot figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, season in enumerate(season_order):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.LAND, facecolor=\"lightgray\", alpha=0.3)\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.3, linestyle=\":\")\n",
    "\n",
    "    # Get seasonal data\n",
    "    tas_season = ds_seasonal.tas.sel(season=season) - 273.15  # Convert to °C\n",
    "\n",
    "    # Plot\n",
    "    im = ax.pcolormesh(\n",
    "        ds.lon,\n",
    "        ds.lat,\n",
    "        tas_season,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=\"RdYlBu_r\",\n",
    "        vmin=-5,\n",
    "        vmax=25,\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation=\"horizontal\", pad=0.05, shrink=0.9)\n",
    "    cbar.set_label(\"Temperature (°C)\", fontsize=10)\n",
    "\n",
    "    # Gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.3, alpha=0.5, linestyle=\"--\")\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    if i < 2:\n",
    "        gl.bottom_labels = False\n",
    "    if i % 2 == 1:\n",
    "        gl.left_labels = False\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(f\"{season_names[season]} ({season})\", fontsize=12, pad=10)\n",
    "    ax.set_extent([ds.lon.min(), ds.lon.max(), ds.lat.min(), ds.lat.max()], crs=ccrs.PlateCarree())\n",
    "\n",
    "plt.suptitle(\"CORDEX EUR-44: Seasonal Mean Temperature (2006-2010)\", fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Extraction\n",
    "\n",
    "Extract and analyze time series data for specific locations or regional averages. This is essential for understanding temporal variability and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define points of interest (major European cities)\n",
    "cities = {\n",
    "    \"Paris\": {\"lat\": 48.85, \"lon\": 2.35},\n",
    "    \"Berlin\": {\"lat\": 52.52, \"lon\": 13.40},\n",
    "    \"Vienna\": {\"lat\": 48.20, \"lon\": 16.37},\n",
    "    \"Munich\": {\"lat\": 48.14, \"lon\": 11.58},\n",
    "}\n",
    "\n",
    "# Extract time series for each city (nearest grid point)\n",
    "city_data = {}\n",
    "for city_name, coords in cities.items():\n",
    "    # Select nearest grid point\n",
    "    point_data = ds.sel(lat=coords[\"lat\"], lon=coords[\"lon\"], method=\"nearest\")\n",
    "    city_data[city_name] = point_data\n",
    "    print(f\"{city_name}: lat={point_data.lat.values:.2f}, lon={point_data.lon.values:.2f}\")\n",
    "\n",
    "print(\"\\nTime series extracted for all cities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temperature time series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Temperature panel\n",
    "ax = axes[0]\n",
    "for city_name, data in city_data.items():\n",
    "    tas_celsius = data.tas - 273.15\n",
    "    tas_celsius.plot(ax=ax, label=city_name, linewidth=0.8, alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Temperature (°C)\", fontsize=12)\n",
    "ax.set_title(\"Daily Near-Surface Air Temperature\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\", ncol=4, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Precipitation panel\n",
    "ax = axes[1]\n",
    "for city_name, data in city_data.items():\n",
    "    pr_mmday = data.pr * 86400  # Convert to mm/day\n",
    "    pr_mmday.plot(ax=ax, label=city_name, linewidth=0.8, alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Precipitation (mm/day)\", fontsize=12)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_title(\"Daily Precipitation\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\", ncol=4, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"CORDEX EUR-44: Time Series for Selected Cities (2006-2010)\", fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot monthly climatology\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Temperature climatology\n",
    "ax = axes[0]\n",
    "for city_name, data in city_data.items():\n",
    "    tas_celsius = data.tas - 273.15\n",
    "    monthly_mean = tas_celsius.groupby(\"time.month\").mean()\n",
    "    monthly_std = tas_celsius.groupby(\"time.month\").std()\n",
    "\n",
    "    months = monthly_mean.month.values\n",
    "    ax.plot(months, monthly_mean.values, \"o-\", label=city_name, linewidth=2, markersize=6)\n",
    "    ax.fill_between(\n",
    "        months,\n",
    "        monthly_mean.values - monthly_std.values,\n",
    "        monthly_mean.values + monthly_std.values,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Temperature (°C)\", fontsize=12)\n",
    "ax.set_title(\"Monthly Mean Temperature (2006-2010)\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\", ncol=4, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(1, 13))\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Precipitation climatology\n",
    "ax = axes[1]\n",
    "for city_name, data in city_data.items():\n",
    "    pr_mmday = data.pr * 86400\n",
    "    monthly_mean = pr_mmday.groupby(\"time.month\").mean()\n",
    "    monthly_std = pr_mmday.groupby(\"time.month\").std()\n",
    "\n",
    "    months = monthly_mean.month.values\n",
    "    ax.plot(months, monthly_mean.values, \"o-\", label=city_name, linewidth=2, markersize=6)\n",
    "    ax.fill_between(\n",
    "        months,\n",
    "        monthly_mean.values - monthly_std.values,\n",
    "        monthly_mean.values + monthly_std.values,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Precipitation (mm/day)\", fontsize=12)\n",
    "ax.set_xlabel(\"Month\", fontsize=12)\n",
    "ax.set_title(\"Monthly Mean Precipitation (2006-2010)\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\", ncol=4, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(1, 13))\n",
    "ax.set_xticklabels(\n",
    "    [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    ")\n",
    "\n",
    "plt.suptitle(\"CORDEX EUR-44: Monthly Climatology (shaded area = ±1 std)\", fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regional average (spatial mean)\n",
    "regional_mean = ds.mean(dim=[\"lat\", \"lon\"])\n",
    "\n",
    "# Calculate annual means\n",
    "tas_annual = (regional_mean.tas - 273.15).groupby(\"time.year\").mean()\n",
    "pr_annual = (regional_mean.pr * 86400 * 365.25).groupby(\"time.year\").mean()  # mm/year\n",
    "\n",
    "# Plot annual means with trend\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Temperature\n",
    "ax = axes[0]\n",
    "years = tas_annual.year.values\n",
    "ax.plot(years, tas_annual.values, \"o-\", linewidth=2, markersize=8, label=\"Annual mean\")\n",
    "\n",
    "# Linear trend\n",
    "z = np.polyfit(years, tas_annual.values, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(years, p(years), \"--\", linewidth=2, alpha=0.7, label=f\"Trend: {z[0]:.3f} °C/year\")\n",
    "\n",
    "ax.set_ylabel(\"Temperature (°C)\", fontsize=12)\n",
    "ax.set_title(\"Regional Mean Annual Temperature\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\", framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Precipitation\n",
    "ax = axes[1]\n",
    "ax.bar(years, pr_annual.values, width=0.6, alpha=0.7, label=\"Annual total\")\n",
    "ax.axhline(\n",
    "    pr_annual.mean().values,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {pr_annual.mean().values:.1f} mm/year\",\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Precipitation (mm/year)\", fontsize=12)\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.set_title(\"Regional Mean Annual Precipitation\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\", framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"CORDEX EUR-44: Regional Average Annual Statistics\", fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Temperature trend: {z[0]:.4f} °C/year\")\n",
    "print(f\"Total warming (2006-2010): {z[0] * (years[-1] - years[0]):.3f} °C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks\n",
    "\n",
    "Perform comprehensive quality control checks on the CORDEX data:\n",
    "\n",
    "1. Missing values detection\n",
    "2. Physical range validation\n",
    "3. Temporal continuity\n",
    "4. Spatial consistency\n",
    "5. Statistical outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check_variable(data, var_name, valid_min, valid_max, units):\n",
    "    \"\"\"\n",
    "    Perform quality checks on a climate variable.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : xarray.DataArray\n",
    "        Variable data array\n",
    "    var_name : str\n",
    "        Variable name for reporting\n",
    "    valid_min : float\n",
    "        Minimum valid value\n",
    "    valid_max : float\n",
    "        Maximum valid value\n",
    "    units : str\n",
    "        Units for reporting\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"QUALITY CHECK: {var_name}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # 1. Missing values\n",
    "    n_total = data.size\n",
    "    n_missing = data.isnull().sum().values\n",
    "    pct_missing = (n_missing / n_total) * 100\n",
    "\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    print(f\"   Total points: {n_total:,}\")\n",
    "    print(f\"   Missing: {n_missing:,} ({pct_missing:.4f}%)\")\n",
    "    if pct_missing == 0:\n",
    "        print(\"   Status: ✓ PASS - No missing values\")\n",
    "    elif pct_missing < 1:\n",
    "        print(\"   Status: ⚠ WARNING - Few missing values\")\n",
    "    else:\n",
    "        print(\"   Status: ✗ FAIL - Significant missing data\")\n",
    "\n",
    "    # 2. Physical range validation\n",
    "    data_min = float(data.min().values)\n",
    "    data_max = float(data.max().values)\n",
    "    data_mean = float(data.mean().values)\n",
    "    data_std = float(data.std().values)\n",
    "\n",
    "    print(\"\\n2. Physical Range Validation:\")\n",
    "    print(f\"   Valid range: [{valid_min}, {valid_max}] {units}\")\n",
    "    print(f\"   Data range: [{data_min:.4f}, {data_max:.4f}] {units}\")\n",
    "    print(f\"   Mean: {data_mean:.4f} {units}\")\n",
    "    print(f\"   Std dev: {data_std:.4f} {units}\")\n",
    "\n",
    "    # Check for out-of-range values\n",
    "    out_of_range = ((data < valid_min) | (data > valid_max)).sum().values\n",
    "    pct_out_of_range = (out_of_range / n_total) * 100\n",
    "\n",
    "    if out_of_range == 0:\n",
    "        print(\"   Status: ✓ PASS - All values within valid range\")\n",
    "    else:\n",
    "        print(f\"   Out of range: {out_of_range:,} ({pct_out_of_range:.4f}%)\")\n",
    "        print(\"   Status: ✗ FAIL - Some values outside valid range\")\n",
    "\n",
    "    # 3. Statistical outliers (values beyond 5 sigma)\n",
    "    print(\"\\n3. Statistical Outliers (>5σ):\")\n",
    "    outliers = (np.abs(data - data_mean) > 5 * data_std).sum().values\n",
    "    pct_outliers = (outliers / n_total) * 100\n",
    "\n",
    "    print(f\"   Outliers: {outliers:,} ({pct_outliers:.4f}%)\")\n",
    "    if pct_outliers < 0.1:\n",
    "        print(\"   Status: ✓ PASS - Few statistical outliers\")\n",
    "    else:\n",
    "        print(\"   Status: ⚠ WARNING - Many statistical outliers\")\n",
    "\n",
    "    # 4. Spatial consistency\n",
    "    print(\"\\n4. Spatial Consistency:\")\n",
    "    spatial_mean = data.mean(dim=\"time\")\n",
    "    spatial_std = spatial_mean.std().values\n",
    "    spatial_range = spatial_mean.max().values - spatial_mean.min().values\n",
    "\n",
    "    print(f\"   Spatial std dev: {spatial_std:.4f} {units}\")\n",
    "    print(f\"   Spatial range: {spatial_range:.4f} {units}\")\n",
    "    print(\"   Status: ✓ INFO - Check values against expected patterns\")\n",
    "\n",
    "    # 5. Distribution summary\n",
    "    print(\"\\n5. Distribution Percentiles:\")\n",
    "    percentiles = [1, 5, 25, 50, 75, 95, 99]\n",
    "    for p in percentiles:\n",
    "        val = float(data.quantile(p / 100).values)\n",
    "        print(f\"   {p:2d}th: {val:.4f} {units}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# Run quality checks\n",
    "quality_check_variable(\n",
    "    ds.tas,\n",
    "    var_name=\"Near-Surface Air Temperature (tas)\",\n",
    "    valid_min=200.0,  # -73°C\n",
    "    valid_max=330.0,  # 57°C\n",
    "    units=\"K\",\n",
    ")\n",
    "\n",
    "quality_check_variable(\n",
    "    ds.pr,\n",
    "    var_name=\"Precipitation (pr)\",\n",
    "    valid_min=0.0,\n",
    "    valid_max=0.01,  # ~860 mm/day (extreme)\n",
    "    units=\"kg m-2 s-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Temperature histogram\n",
    "ax = axes[0, 0]\n",
    "tas_sample = ds.tas.isel(time=slice(0, 365)).values.flatten()  # Sample first year\n",
    "ax.hist(tas_sample - 273.15, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Temperature (°C)\", fontsize=11)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=11)\n",
    "ax.set_title(\"Temperature Distribution (Year 1)\", fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Temperature box plot by year\n",
    "ax = axes[0, 1]\n",
    "annual_data = []\n",
    "years = sorted(ds.time.dt.year.values.tolist())\n",
    "unique_years = sorted(set(years))\n",
    "for year in unique_years:\n",
    "    year_data = ds.tas.sel(time=str(year)) - 273.15\n",
    "    annual_data.append(year_data.values.flatten())\n",
    "\n",
    "bp = ax.boxplot(annual_data, labels=unique_years, patch_artist=True)\n",
    "for patch in bp[\"boxes\"]:\n",
    "    patch.set_facecolor(\"lightblue\")\n",
    "ax.set_xlabel(\"Year\", fontsize=11)\n",
    "ax.set_ylabel(\"Temperature (°C)\", fontsize=11)\n",
    "ax.set_title(\"Temperature Distribution by Year\", fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Precipitation histogram (log scale)\n",
    "ax = axes[1, 0]\n",
    "pr_sample = ds.pr.isel(time=slice(0, 365)).values.flatten() * 86400  # Sample first year\n",
    "pr_sample = pr_sample[pr_sample > 0]  # Remove zeros for log scale\n",
    "ax.hist(pr_sample, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Precipitation (mm/day)\", fontsize=11)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=11)\n",
    "ax.set_title(\"Precipitation Distribution (Year 1, excluding zeros)\", fontsize=12)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Precipitation box plot by season\n",
    "ax = axes[1, 1]\n",
    "seasonal_pr = []\n",
    "season_labels = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "for season in season_labels:\n",
    "    season_data = ds.pr.sel(time=ds.time.dt.season == season) * 86400\n",
    "    seasonal_pr.append(season_data.values.flatten())\n",
    "\n",
    "bp = ax.boxplot(seasonal_pr, labels=season_labels, patch_artist=True)\n",
    "for patch in bp[\"boxes\"]:\n",
    "    patch.set_facecolor(\"lightgreen\")\n",
    "ax.set_xlabel(\"Season\", fontsize=11)\n",
    "ax.set_ylabel(\"Precipitation (mm/day)\", fontsize=11)\n",
    "ax.set_title(\"Precipitation Distribution by Season\", fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"CORDEX EUR-44: Data Distribution Analysis\", fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-File Handling\n",
    "\n",
    "Demonstrate efficient handling of multiple CORDEX files using `xr.open_mfdataset()`. This is essential when working with:\n",
    "\n",
    "- Multiple time periods (concatenate along time)\n",
    "- Multiple ensemble members\n",
    "- Multiple scenarios or models\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. Use glob patterns to match file sets\n",
    "2. Specify chunking strategy for memory efficiency\n",
    "3. Use `combine='by_coords'` for automatic dimension alignment\n",
    "4. Enable parallel loading with `parallel=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple sample files for demonstration\n",
    "def create_multiple_cordex_files(output_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Create multiple CORDEX files representing different time periods.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define time periods\n",
    "    periods = [\n",
    "        (\"2006-01-01\", \"2007-12-31\"),\n",
    "        (\"2008-01-01\", \"2009-12-31\"),\n",
    "        (\"2010-01-01\", \"2011-12-31\"),\n",
    "    ]\n",
    "\n",
    "    files_created = []\n",
    "\n",
    "    for start_date, end_date in periods:\n",
    "        # Spatial domain\n",
    "        lat = np.arange(42.0, 58.0, 0.44)\n",
    "        lon = np.arange(2.0, 18.0, 0.44)\n",
    "        time = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "        # Create simple synthetic data\n",
    "        _lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
    "\n",
    "        # Temperature with seasonal cycle\n",
    "        days_of_year = time.dayofyear.values\n",
    "        years = time.year.values\n",
    "        seasonal_cycle = 15 * np.sin(2 * np.pi * (days_of_year - 80) / 365.25)\n",
    "        trend = 0.03 * (years - 2006)\n",
    "        base_temp = 273.15 + 10 + 0.5 * (lat_grid - lat.mean())\n",
    "\n",
    "        tas_data = np.zeros((len(time), len(lat), len(lon)))\n",
    "        for i in range(len(time)):\n",
    "            tas_data[i] = (\n",
    "                base_temp\n",
    "                + seasonal_cycle[i]\n",
    "                + trend[i]\n",
    "                + np.random.normal(0, 2, (len(lat), len(lon)))\n",
    "            )\n",
    "\n",
    "        # Precipitation\n",
    "        seasonal_pr = 2.0e-5 * (1 - 0.4 * np.sin(2 * np.pi * (days_of_year - 80) / 365.25))\n",
    "        pr_data = np.zeros((len(time), len(lat), len(lon)))\n",
    "        for i in range(len(time)):\n",
    "            base_pr = seasonal_pr[i] * (1 + 0.3 * np.sin(lat_grid / 10))\n",
    "            random_pr = np.random.gamma(2, seasonal_pr[i] / 2, (len(lat), len(lon)))\n",
    "            pr_data[i] = np.maximum(0, base_pr + random_pr)\n",
    "\n",
    "        # Create dataset\n",
    "        ds_period = xr.Dataset(\n",
    "            {\n",
    "                \"tas\": (\n",
    "                    [\"time\", \"lat\", \"lon\"],\n",
    "                    tas_data,\n",
    "                    {\n",
    "                        \"standard_name\": \"air_temperature\",\n",
    "                        \"long_name\": \"Near-Surface Air Temperature\",\n",
    "                        \"units\": \"K\",\n",
    "                    },\n",
    "                ),\n",
    "                \"pr\": (\n",
    "                    [\"time\", \"lat\", \"lon\"],\n",
    "                    pr_data,\n",
    "                    {\n",
    "                        \"standard_name\": \"precipitation_flux\",\n",
    "                        \"long_name\": \"Precipitation\",\n",
    "                        \"units\": \"kg m-2 s-1\",\n",
    "                    },\n",
    "                ),\n",
    "            },\n",
    "            coords={\"time\": time, \"lat\": lat, \"lon\": lon},\n",
    "            attrs={\n",
    "                \"Conventions\": \"CF-1.6\",\n",
    "                \"institution\": \"Climate Research Lab\",\n",
    "                \"source\": \"SMHI-RCA4 driven by MPI-ESM-LR\",\n",
    "                \"experiment_id\": \"rcp85\",\n",
    "                \"CORDEX_domain\": \"EUR-44\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Save file\n",
    "        start_str = start_date.replace(\"-\", \"\")\n",
    "        end_str = end_date.replace(\"-\", \"\")\n",
    "        filename = (\n",
    "            f\"tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_{start_str}-{end_str}.nc\"\n",
    "        )\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        encoding = {\"tas\": {\"zlib\": True, \"complevel\": 5}, \"pr\": {\"zlib\": True, \"complevel\": 5}}\n",
    "        ds_period.to_netcdf(filepath, encoding=encoding)\n",
    "\n",
    "        files_created.append(filepath)\n",
    "        print(f\"Created: {filename}\")\n",
    "\n",
    "    return files_created\n",
    "\n",
    "\n",
    "# Create multiple files\n",
    "print(\"Creating multiple CORDEX files...\\n\")\n",
    "files = create_multiple_cordex_files(output_dir=\"./data\")\n",
    "print(f\"\\nTotal files created: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple files using xr.open_mfdataset\n",
    "print(\"Loading multiple CORDEX files...\\n\")\n",
    "\n",
    "# Use glob pattern to match all files\n",
    "file_pattern = \"./data/tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_*.nc\"\n",
    "\n",
    "# Open multiple datasets with chunking\n",
    "ds_multi = xr.open_mfdataset(\n",
    "    file_pattern,\n",
    "    chunks={\"time\": 365, \"lat\": 20, \"lon\": 20},\n",
    "    combine=\"by_coords\",\n",
    "    parallel=True,\n",
    "    engine=\"netcdf4\",\n",
    ")\n",
    "\n",
    "print(\"Multi-file dataset loaded successfully!\\n\")\n",
    "print(f\"Time range: {ds_multi.time.values[0]} to {ds_multi.time.values[-1]}\")\n",
    "print(f\"Total time steps: {len(ds_multi.time)}\")\n",
    "print(f\"Duration: {len(ds_multi.time) / 365.25:.2f} years\\n\")\n",
    "\n",
    "# Display dataset\n",
    "ds_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs multi-file datasets\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Single File vs Multi-File Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nSingle file dataset:\")\n",
    "print(f\"  Time steps: {len(ds.time)}\")\n",
    "print(f\"  Duration: {len(ds.time) / 365.25:.2f} years\")\n",
    "print(f\"  Date range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "\n",
    "print(\"\\nMulti-file dataset:\")\n",
    "print(f\"  Time steps: {len(ds_multi.time)}\")\n",
    "print(f\"  Duration: {len(ds_multi.time) / 365.25:.2f} years\")\n",
    "print(f\"  Date range: {ds_multi.time.values[0]} to {ds_multi.time.values[-1]}\")\n",
    "\n",
    "print(f\"\\nDimensions match: {ds.dims == ds_multi.dims[next(iter(ds.dims.keys()))]}\")\n",
    "print(f\"Variables match: {set(ds.data_vars) == set(ds_multi.data_vars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate efficient computation on multi-file dataset\n",
    "print(\"Computing regional mean from multi-file dataset...\\n\")\n",
    "\n",
    "# Calculate regional mean with progress bar\n",
    "with ProgressBar():\n",
    "    regional_mean_multi = ds_multi[[\"tas\", \"pr\"]].mean(dim=[\"lat\", \"lon\"]).compute()\n",
    "\n",
    "print(\"Computation complete!\\n\")\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Temperature\n",
    "ax = axes[0]\n",
    "tas_celsius = regional_mean_multi.tas - 273.15\n",
    "tas_celsius.plot(ax=ax, linewidth=1, color=\"red\", alpha=0.7)\n",
    "\n",
    "# Add annual mean\n",
    "tas_annual = tas_celsius.resample(time=\"1Y\").mean()\n",
    "tas_annual.plot(ax=ax, linewidth=3, color=\"darkred\", label=\"Annual mean\")\n",
    "\n",
    "ax.set_ylabel(\"Temperature (°C)\", fontsize=12)\n",
    "ax.set_title(\"Regional Mean Temperature (Multi-File Dataset)\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Precipitation\n",
    "ax = axes[1]\n",
    "pr_mmday = regional_mean_multi.pr * 86400\n",
    "pr_mmday.plot(ax=ax, linewidth=1, color=\"blue\", alpha=0.7)\n",
    "\n",
    "# Add annual mean\n",
    "pr_annual = pr_mmday.resample(time=\"1Y\").mean()\n",
    "pr_annual.plot(ax=ax, linewidth=3, color=\"darkblue\", label=\"Annual mean\")\n",
    "\n",
    "ax.set_ylabel(\"Precipitation (mm/day)\", fontsize=12)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_title(\"Regional Mean Precipitation (Multi-File Dataset)\", fontsize=14, pad=10)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"CORDEX EUR-44: Multi-File Dataset Analysis\", fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Memory Management with Dask\n",
    "\n",
    "Efficient memory management is crucial when working with large climate datasets. This section demonstrates:\n",
    "\n",
    "1. Lazy loading with dask arrays\n",
    "2. Optimal chunking strategies\n",
    "3. Parallel computation\n",
    "4. Memory monitoring\n",
    "\n",
    "### Chunking Best Practices:\n",
    "\n",
    "- **Temporal chunks**: Typically 1 year (365 days) for daily data\n",
    "- **Spatial chunks**: 10-50 grid points per dimension\n",
    "- **Chunk size**: Aim for 100-200 MB per chunk\n",
    "- **Analysis dimension**: Larger chunks in dimensions you aggregate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dask chunking\n",
    "print(\"=\" * 70)\n",
    "print(\"DASK CHUNKING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for var in ds_multi.data_vars:\n",
    "    data = ds_multi[var]\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Dtype: {data.dtype}\")\n",
    "\n",
    "    if hasattr(data.data, \"chunks\"):\n",
    "        print(f\"  Chunks: {data.chunks}\")\n",
    "        print(f\"  Number of chunks: {data.data.npartitions}\")\n",
    "\n",
    "        # Estimate chunk size\n",
    "        chunk_elements = np.prod([c[0] if isinstance(c, tuple) else c for c in data.chunks])\n",
    "        chunk_bytes = chunk_elements * data.dtype.itemsize\n",
    "        chunk_mb = chunk_bytes / (1024 * 1024)\n",
    "\n",
    "        print(f\"  Approximate chunk size: {chunk_mb:.2f} MB\")\n",
    "\n",
    "        # Total memory if fully loaded\n",
    "        total_mb = data.nbytes / (1024 * 1024)\n",
    "        print(f\"  Total size if loaded: {total_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"  Not chunked (loaded in memory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different chunking strategies\n",
    "print(\"=\" * 70)\n",
    "print(\"CHUNKING STRATEGIES COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Strategy 1: Small temporal chunks (good for time series analysis)\n",
    "ds_chunk1 = xr.open_dataset(\n",
    "    \"./data/tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\",\n",
    "    chunks={\"time\": 30, \"lat\": -1, \"lon\": -1},  # Monthly chunks, full spatial domain\n",
    ")\n",
    "\n",
    "# Strategy 2: Large temporal chunks (good for spatial analysis)\n",
    "ds_chunk2 = xr.open_dataset(\n",
    "    \"./data/tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\",\n",
    "    chunks={\"time\": -1, \"lat\": 10, \"lon\": 10},  # Full time, small spatial blocks\n",
    ")\n",
    "\n",
    "# Strategy 3: Balanced chunks (good for general analysis)\n",
    "ds_chunk3 = xr.open_dataset(\n",
    "    \"./data/tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\",\n",
    "    chunks={\"time\": 365, \"lat\": 20, \"lon\": 20},  # Balanced\n",
    ")\n",
    "\n",
    "print(\"\\nStrategy 1: Time series optimized (monthly temporal chunks)\")\n",
    "print(f\"  tas chunks: {ds_chunk1.tas.chunks}\")\n",
    "print(f\"  Number of chunks: {ds_chunk1.tas.data.npartitions}\")\n",
    "\n",
    "print(\"\\nStrategy 2: Spatial analysis optimized (full time, small spatial blocks)\")\n",
    "print(f\"  tas chunks: {ds_chunk2.tas.chunks}\")\n",
    "print(f\"  Number of chunks: {ds_chunk2.tas.data.npartitions}\")\n",
    "\n",
    "print(\"\\nStrategy 3: Balanced (annual temporal, medium spatial chunks)\")\n",
    "print(f\"  tas chunks: {ds_chunk3.tas.chunks}\")\n",
    "print(f\"  Number of chunks: {ds_chunk3.tas.data.npartitions}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Use Strategy 1 for: Time series extraction, temporal statistics\")\n",
    "print(\"Use Strategy 2 for: Spatial maps, spatial statistics\")\n",
    "print(\"Use Strategy 3 for: General-purpose analysis, mixed operations\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate lazy vs eager evaluation\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LAZY vs EAGER EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Lazy evaluation (doesn't load data)\n",
    "print(\"\\n1. Lazy evaluation (setup only):\")\n",
    "start_time = time.time()\n",
    "result_lazy = ds_multi.tas.mean(dim=[\"lat\", \"lon\"])\n",
    "lazy_time = time.time() - start_time\n",
    "print(f\"   Time: {lazy_time:.6f} seconds\")\n",
    "print(f\"   Result type: {type(result_lazy.data)}\")\n",
    "print(\"   Memory loaded: No (dask array)\")\n",
    "\n",
    "# Eager evaluation (loads and computes)\n",
    "print(\"\\n2. Eager evaluation (actual computation):\")\n",
    "start_time = time.time()\n",
    "with ProgressBar():\n",
    "    result_eager = ds_multi.tas.mean(dim=[\"lat\", \"lon\"]).compute()\n",
    "eager_time = time.time() - start_time\n",
    "print(f\"   Time: {eager_time:.6f} seconds\")\n",
    "print(f\"   Result type: {type(result_eager.data)}\")\n",
    "print(\"   Memory loaded: Yes (numpy array)\")\n",
    "\n",
    "print(f\"\\nSpeedup factor: {eager_time / lazy_time:.0f}x\")\n",
    "print(\"\\nKey insight: Lazy evaluation allows you to build complex\")\n",
    "print(\"computation graphs before executing, optimizing memory usage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate parallel computation with dask\n",
    "print(\"=\" * 70)\n",
    "print(\"PARALLEL COMPUTATION DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Complex operation: Calculate anomalies relative to long-term mean\n",
    "print(\"\\nCalculating temperature anomalies...\")\n",
    "print(\"Operation: (T - T_mean) for each time step\\n\")\n",
    "\n",
    "# Calculate climatology (long-term mean)\n",
    "with ProgressBar():\n",
    "    print(\"Step 1: Computing long-term mean...\")\n",
    "    climatology = ds_multi.tas.mean(dim=\"time\").compute()\n",
    "\n",
    "# Calculate anomalies (lazy)\n",
    "print(\"\\nStep 2: Setting up anomaly calculation (lazy)...\")\n",
    "anomalies = ds_multi.tas - climatology\n",
    "print(f\"Anomalies type: {type(anomalies.data)}\")\n",
    "print(f\"Anomalies shape: {anomalies.shape}\")\n",
    "\n",
    "# Compute a subset to demonstrate\n",
    "print(\"\\nStep 3: Computing anomalies for first year...\")\n",
    "with ProgressBar():\n",
    "    anomalies_year1 = anomalies.isel(time=slice(0, 365)).compute()\n",
    "\n",
    "print(\"\\nAnomaly statistics (Year 1):\")\n",
    "print(f\"  Mean: {float(anomalies_year1.mean()):.6f} K\")\n",
    "print(f\"  Std: {float(anomalies_year1.std()):.6f} K\")\n",
    "print(f\"  Min: {float(anomalies_year1.min()):.6f} K\")\n",
    "print(f\"  Max: {float(anomalies_year1.max()):.6f} K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory-efficient workflow\n",
    "print(\"=\" * 70)\n",
    "print(\"MEMORY-EFFICIENT WORKFLOW EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nScenario: Calculate seasonal means for large multi-year dataset\")\n",
    "print(\"without loading entire dataset into memory.\\n\")\n",
    "\n",
    "# Build computation graph (lazy)\n",
    "print(\"Building computation graph...\")\n",
    "seasonal_means = ds_multi.groupby(\"time.season\").mean(dim=\"time\")\n",
    "print(f\"  Type: {type(seasonal_means.tas.data)}\")\n",
    "print(\"  Memory loaded: No\\n\")\n",
    "\n",
    "# Execute computation with progress bar\n",
    "print(\"Executing computation...\")\n",
    "with ProgressBar():\n",
    "    seasonal_means_computed = seasonal_means.compute()\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for season in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]:\n",
    "    tas_mean = float(seasonal_means_computed.tas.sel(season=season).mean()) - 273.15\n",
    "    pr_mean = float(seasonal_means_computed.pr.sel(season=season).mean()) * 86400\n",
    "    print(f\"  {season}: T = {tas_mean:.2f}°C, P = {pr_mean:.3f} mm/day\")\n",
    "\n",
    "# Close datasets\n",
    "ds_chunk1.close()\n",
    "ds_chunk2.close()\n",
    "ds_chunk3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data\n",
    "\n",
    "Export processed data for downstream analysis or sharing. Best practices include:\n",
    "\n",
    "1. Compress output files (zlib compression)\n",
    "2. Preserve metadata and attributes\n",
    "3. Use appropriate file formats (NetCDF4, Zarr)\n",
    "4. Document processing steps in file attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"./processed_data\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Save regional mean time series\n",
    "print(\"\\n1. Saving regional mean time series...\")\n",
    "\n",
    "# Calculate regional means\n",
    "regional_mean = ds.mean(dim=[\"lat\", \"lon\"])\n",
    "\n",
    "# Add processing metadata\n",
    "regional_mean.attrs.update(\n",
    "    {\n",
    "        \"title\": \"CORDEX EUR-44 Regional Mean Time Series\",\n",
    "        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"processing_steps\": \"Spatial mean over entire domain\",\n",
    "        \"source_file\": \"tas_pr_EUR-44_MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1_day_20060101-20101231.nc\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save with compression\n",
    "output_file = os.path.join(output_dir, \"regional_mean_timeseries.nc\")\n",
    "encoding = {\n",
    "    \"tas\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n",
    "    \"pr\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n",
    "}\n",
    "\n",
    "regional_mean.to_netcdf(output_file, encoding=encoding)\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "print(f\"   Saved: {output_file}\")\n",
    "print(f\"   Size: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Save seasonal climatology\n",
    "print(\"\\n2. Saving seasonal climatology...\")\n",
    "\n",
    "# Calculate seasonal means\n",
    "seasonal_clim = ds.groupby(\"time.season\").mean(dim=\"time\")\n",
    "\n",
    "# Add metadata\n",
    "seasonal_clim.attrs.update(\n",
    "    {\n",
    "        \"title\": \"CORDEX EUR-44 Seasonal Climatology (2006-2010)\",\n",
    "        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"processing_steps\": \"Seasonal means calculated from daily data\",\n",
    "        \"time_period\": \"2006-2010\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compute and save\n",
    "output_file = os.path.join(output_dir, \"seasonal_climatology.nc\")\n",
    "encoding = {\n",
    "    \"tas\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n",
    "    \"pr\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n",
    "}\n",
    "\n",
    "with ProgressBar():\n",
    "    seasonal_clim.to_netcdf(output_file, encoding=encoding)\n",
    "\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "print(f\"   Saved: {output_file}\")\n",
    "print(f\"   Size: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Save point time series for selected cities\n",
    "print(\"\\n3. Saving city time series...\")\n",
    "\n",
    "# Create dataset with all city time series\n",
    "city_datasets = []\n",
    "for city_name, coords in cities.items():\n",
    "    point_data = ds.sel(lat=coords[\"lat\"], lon=coords[\"lon\"], method=\"nearest\")\n",
    "\n",
    "    # Rename variables to include city name\n",
    "    point_data = point_data.rename({\"tas\": f\"tas_{city_name}\", \"pr\": f\"pr_{city_name}\"})\n",
    "\n",
    "    city_datasets.append(point_data)\n",
    "\n",
    "# Merge all city data\n",
    "cities_combined = xr.merge(city_datasets)\n",
    "\n",
    "# Add metadata\n",
    "cities_combined.attrs.update(\n",
    "    {\n",
    "        \"title\": \"CORDEX EUR-44 City Time Series\",\n",
    "        \"cities\": \", \".join(cities.keys()),\n",
    "        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"note\": \"Nearest grid point extraction for selected European cities\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_file = os.path.join(output_dir, \"city_timeseries.nc\")\n",
    "encoding = {\n",
    "    var: {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"} for var in cities_combined.data_vars\n",
    "}\n",
    "\n",
    "cities_combined.to_netcdf(output_file, encoding=encoding)\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "print(f\"   Saved: {output_file}\")\n",
    "print(f\"   Size: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Export to CSV for easy sharing/analysis\n",
    "print(\"\\n4. Exporting to CSV format...\")\n",
    "\n",
    "# Regional mean as CSV\n",
    "regional_df = regional_mean.to_dataframe()\n",
    "regional_df[\"tas_celsius\"] = regional_df[\"tas\"] - 273.15\n",
    "regional_df[\"pr_mmday\"] = regional_df[\"pr\"] * 86400\n",
    "regional_df = regional_df[[\"tas_celsius\", \"pr_mmday\"]]\n",
    "regional_df.columns = [\"Temperature_C\", \"Precipitation_mm_day\"]\n",
    "\n",
    "output_file = os.path.join(output_dir, \"regional_mean_timeseries.csv\")\n",
    "regional_df.to_csv(output_file)\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "print(f\"   Saved: {output_file}\")\n",
    "print(f\"   Size: {file_size:.2f} KB\")\n",
    "print(f\"   Rows: {len(regional_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of saved files\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY OF SAVED FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "saved_files = list(Path(output_dir).glob(\"*\"))\n",
    "total_size = 0\n",
    "\n",
    "for i, filepath in enumerate(sorted(saved_files), 1):\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    total_size += file_size\n",
    "    print(f\"{i}. {filepath.name}\")\n",
    "    print(f\"   Size: {file_size / 1024:.2f} KB\")\n",
    "    print(f\"   Type: {filepath.suffix[1:].upper()}\")\n",
    "\n",
    "print(f\"\\nTotal files: {len(saved_files)}\")\n",
    "print(f\"Total size: {total_size / 1024:.2f} KB ({total_size / (1024 * 1024):.2f} MB)\")\n",
    "print(f\"\\nOutput directory: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "This notebook covered the essential workflow for working with CORDEX regional climate data:\n",
    "\n",
    "1. **Data Structure**: Understanding CORDEX naming conventions, model hierarchy, and CF-compliant NetCDF format\n",
    "2. **Data Loading**: Using xarray for lazy loading and efficient memory management\n",
    "3. **Exploration**: Examining dimensions, coordinates, attributes, and temporal/spatial coverage\n",
    "4. **Visualization**: Creating professional maps with cartopy and analyzing spatial patterns\n",
    "5. **Time Series**: Extracting point and regional time series, calculating climatologies\n",
    "6. **Quality Control**: Checking for missing values, physical ranges, and statistical outliers\n",
    "7. **Multi-File Handling**: Loading and concatenating multiple files efficiently\n",
    "8. **Memory Management**: Leveraging dask for lazy evaluation and parallel computation\n",
    "9. **Data Export**: Saving processed data with proper metadata and compression\n",
    "\n",
    "### Best Practices Summary\n",
    "\n",
    "- Always use chunking for large datasets\n",
    "- Leverage lazy evaluation to build computation graphs\n",
    "- Preserve metadata in processed files\n",
    "- Document processing steps in file attributes\n",
    "- Use compression to reduce file sizes\n",
    "- Validate data quality before analysis\n",
    "- Choose appropriate chunking strategies for your analysis type\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue your CORDEX analysis with:\n",
    "\n",
    "1. **Bias Correction**: Compare RCM outputs with observations and apply bias correction methods\n",
    "2. **Climate Indices**: Calculate climate extremes indices (Rx1day, CDD, WSDI, etc.)\n",
    "3. **Ensemble Analysis**: Compare multiple RCM-GCM combinations and calculate ensemble statistics\n",
    "4. **Trend Analysis**: Detect and quantify long-term climate trends\n",
    "5. **Impact Studies**: Apply climate data to sector-specific impact models (hydrology, agriculture, etc.)\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- CORDEX website: https://cordex.org/\n",
    "- xarray documentation: https://xarray.pydata.org/\n",
    "- Dask documentation: https://dask.org/\n",
    "- CF Conventions: http://cfconventions.org/\n",
    "- Cartopy documentation: https://scitools.org.uk/cartopy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: Close all datasets\n",
    "ds.close()\n",
    "ds_multi.close()\n",
    "\n",
    "print(\"All datasets closed successfully.\")\n",
    "print(\"\\nNotebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
