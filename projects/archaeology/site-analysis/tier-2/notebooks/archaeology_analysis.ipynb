{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archaeological Site Analysis with AWS\n",
    "\n",
    "This notebook demonstrates a complete archaeological data analysis pipeline using AWS services:\n",
    "\n",
    "1. **Generate Sample Data** - Create realistic artifact dataset\n",
    "2. **Upload to S3** - Store artifact data in cloud\n",
    "3. **Lambda Processing** - Classify and analyze artifacts\n",
    "4. **Query DynamoDB** - Retrieve artifact catalog\n",
    "5. **Visualization** - Create publication-quality figures\n",
    "6. **Spatial Analysis** - Analyze artifact distribution\n",
    "7. **Chronological Analysis** - Examine temporal patterns\n",
    "8. **Typological Analysis** - Study artifact types\n",
    "\n",
    "**Duration:** 60-90 minutes  \n",
    "**Cost:** ~$5-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = 'archaeology-data-xxxx'  # Replace with your bucket name\n",
    "TABLE_NAME = 'ArtifactCatalog'\n",
    "LAMBDA_FUNCTION = 'classify-artifacts'\n",
    "SITE_ID = 'SITE_A'\n",
    "AWS_REGION = 'us-east-1'\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION)\n",
    "lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"  DynamoDB Table: {TABLE_NAME}\")\n",
    "print(f\"  Lambda Function: {LAMBDA_FUNCTION}\")\n",
    "print(f\"  Site ID: {SITE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Artifact Data\n",
    "\n",
    "Create a realistic archaeological dataset with multiple artifact types and periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_artifacts(num_artifacts=200, site_id='SITE_A'):\n",
    "    \"\"\"Generate sample archaeological artifacts.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    artifact_types = {\n",
    "        'pottery': ['ceramic', 'terracotta', 'glazed ceramic'],\n",
    "        'lithic': ['flint', 'obsidian', 'chert', 'quartzite'],\n",
    "        'bone': ['mammal bone', 'bird bone', 'fish bone'],\n",
    "        'coin': ['bronze', 'silver', 'gold'],\n",
    "        'architecture': ['brick', 'stone', 'tile']\n",
    "    }\n",
    "    \n",
    "    periods = ['Neolithic', 'Bronze Age', 'Iron Age', 'Classical', 'Medieval']\n",
    "    \n",
    "    artifacts = []\n",
    "    \n",
    "    for i in range(num_artifacts):\n",
    "        artifact_type = np.random.choice(list(artifact_types.keys()))\n",
    "        material = np.random.choice(artifact_types[artifact_type])\n",
    "        \n",
    "        # Generate measurements\n",
    "        if artifact_type == 'pottery':\n",
    "            length, width, thickness = 150, 120, 8\n",
    "            weight = 300\n",
    "        elif artifact_type == 'lithic':\n",
    "            length, width, thickness = 50, 35, 10\n",
    "            weight = 40\n",
    "        elif artifact_type == 'bone':\n",
    "            length, width, thickness = 80, 25, 15\n",
    "            weight = 50\n",
    "        elif artifact_type == 'coin':\n",
    "            length, width, thickness = 20, 20, 2\n",
    "            weight = 5\n",
    "        else:  # architecture\n",
    "            length, width, thickness = 250, 150, 50\n",
    "            weight = 2000\n",
    "        \n",
    "        # Add variation\n",
    "        length += np.random.normal(0, length * 0.2)\n",
    "        width += np.random.normal(0, width * 0.2)\n",
    "        thickness += np.random.normal(0, thickness * 0.2)\n",
    "        weight += np.random.normal(0, weight * 0.3)\n",
    "        \n",
    "        # GPS coordinates\n",
    "        gps_lat = 40.0 + np.random.normal(0, 0.05)\n",
    "        gps_lon = 20.0 + np.random.normal(0, 0.05)\n",
    "        \n",
    "        # Stratigraphy\n",
    "        layer = np.random.randint(1, 6)\n",
    "        strat_unit = f\"Layer_{layer}\"\n",
    "        period = periods[min(layer - 1, len(periods) - 1)]\n",
    "        \n",
    "        # Dating\n",
    "        period_dates = {\n",
    "            'Neolithic': (-8000, -4000),\n",
    "            'Bronze Age': (-3300, -1200),\n",
    "            'Iron Age': (-1200, -500),\n",
    "            'Classical': (-800, 300),\n",
    "            'Medieval': (500, 1500)\n",
    "        }\n",
    "        date_min, date_max = period_dates[period]\n",
    "        dating_value = np.random.randint(date_min, date_max)\n",
    "        \n",
    "        artifact = {\n",
    "            'artifact_id': f\"ART_{site_id}_{i+1:04d}\",\n",
    "            'site_id': site_id,\n",
    "            'artifact_type': artifact_type,\n",
    "            'material': material,\n",
    "            'length': round(max(1, length), 2),\n",
    "            'width': round(max(1, width), 2),\n",
    "            'thickness': round(max(0.5, thickness), 2),\n",
    "            'weight': round(max(0.1, weight), 2),\n",
    "            'gps_lat': round(gps_lat, 6),\n",
    "            'gps_lon': round(gps_lon, 6),\n",
    "            'stratigraphic_unit': strat_unit,\n",
    "            'period': period,\n",
    "            'dating_method': 'relative' if layer > 3 else 'radiocarbon',\n",
    "            'dating_value': dating_value,\n",
    "            'excavation_date': (datetime.now() - timedelta(days=np.random.randint(0, 180))).strftime('%Y-%m-%d'),\n",
    "            'notes': f'{artifact_type.capitalize()} fragment, {material}'\n",
    "        }\n",
    "        \n",
    "        artifacts.append(artifact)\n",
    "    \n",
    "    return pd.DataFrame(artifacts)\n",
    "\n",
    "# Generate dataset\n",
    "df_artifacts = generate_artifacts(200, SITE_ID)\n",
    "\n",
    "print(f\"Generated {len(df_artifacts)} artifacts\")\n",
    "print(f\"\\nArtifact types:\")\n",
    "print(df_artifacts['artifact_type'].value_counts())\n",
    "print(f\"\\nPeriods:\")\n",
    "print(df_artifacts['period'].value_counts())\n",
    "\n",
    "df_artifacts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Data to S3\n",
    "\n",
    "Upload the artifact dataset to S3 for Lambda processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally first\n",
    "csv_filename = f\"{SITE_ID}_artifacts.csv\"\n",
    "df_artifacts.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved locally: {csv_filename}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"raw/{csv_filename}\"\n",
    "s3.upload_file(csv_filename, BUCKET_NAME, s3_key)\n",
    "print(f\"✓ Uploaded to s3://{BUCKET_NAME}/{s3_key}\")\n",
    "\n",
    "# Verify upload\n",
    "response = s3.head_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "file_size = response['ContentLength'] / 1024\n",
    "print(f\"  File size: {file_size:.2f} KB\")\n",
    "print(f\"  Last modified: {response['LastModified']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Invoke Lambda for Processing\n",
    "\n",
    "Trigger Lambda function to classify and analyze artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Lambda payload\n",
    "payload = {\n",
    "    'bucket': BUCKET_NAME,\n",
    "    'key': s3_key\n",
    "}\n",
    "\n",
    "print(f\"Invoking Lambda function: {LAMBDA_FUNCTION}\")\n",
    "print(f\"Payload: {json.dumps(payload, indent=2)}\")\n",
    "print(\"\\nProcessing... (this may take 30-60 seconds)\\n\")\n",
    "\n",
    "# Invoke Lambda\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=LAMBDA_FUNCTION,\n",
    "    InvocationType='RequestResponse',\n",
    "    Payload=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Parse response\n",
    "response_payload = json.loads(response['Payload'].read())\n",
    "print(\"Lambda Response:\")\n",
    "print(json.dumps(response_payload, indent=2))\n",
    "\n",
    "if response_payload['statusCode'] == 200:\n",
    "    body = response_payload['body']\n",
    "    print(f\"\\n✓ Successfully processed {body['artifacts_processed']} artifacts\")\n",
    "    print(f\"  Summary file: {body['summary_file']}\")\n",
    "    print(f\"  Processed file: {body['processed_file']}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Processing failed: {response_payload.get('body', {}).get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Results from DynamoDB\n",
    "\n",
    "Retrieve classified artifacts from DynamoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query DynamoDB table\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "print(f\"Querying DynamoDB table: {TABLE_NAME}\")\n",
    "\n",
    "# Scan all items (for small datasets)\n",
    "response = table.scan()\n",
    "items = response['Items']\n",
    "\n",
    "# Handle pagination\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    items.extend(response['Items'])\n",
    "\n",
    "print(f\"✓ Retrieved {len(items)} artifacts from DynamoDB\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(items)\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['length', 'width', 'thickness', 'weight', 'gps_lat', 'gps_lon', \n",
    "                'dating_value', 'classification_confidence', 'l_w_ratio', \n",
    "                'thickness_index', 'shape_index']\n",
    "for col in numeric_cols:\n",
    "    if col in df_results.columns:\n",
    "        df_results[col] = pd.to_numeric(df_results[col], errors='coerce')\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Processing Summary\n",
    "\n",
    "Download and examine the comprehensive analysis summary from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download summary JSON\n",
    "summary_key = f\"processed/{SITE_ID}_summary.json\"\n",
    "\n",
    "try:\n",
    "    response = s3.get_object(Bucket=BUCKET_NAME, Key=summary_key)\n",
    "    summary = json.loads(response['Body'].read())\n",
    "    \n",
    "    print(\"Processing Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Site ID: {summary['site_id']}\")\n",
    "    print(f\"Processing Date: {summary['processing_date']}\")\n",
    "    print(f\"Artifacts Processed: {summary['artifacts_processed']}\")\n",
    "    print(f\"\\nSpatial Analysis:\")\n",
    "    print(f\"  Center: {summary['spatial_analysis']['center_of_mass']['latitude']:.6f}, \"\n",
    "          f\"{summary['spatial_analysis']['center_of_mass']['longitude']:.6f}\")\n",
    "    print(f\"\\nChronological Range:\")\n",
    "    print(f\"  {summary['chronological_analysis']['date_range']['earliest']} to \"\n",
    "          f\"{summary['chronological_analysis']['date_range']['latest']} \"\n",
    "          f\"(span: {summary['chronological_analysis']['date_range']['span_years']} years)\")\n",
    "    print(f\"\\nArtifact Counts:\")\n",
    "    for artifact_type, count in summary['artifact_counts']['by_type'].items():\n",
    "        print(f\"  {artifact_type}: {count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not download summary: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Artifact Distribution\n",
    "\n",
    "Create visualizations of artifact distributions by type and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Artifact type distribution\n",
    "type_counts = df_results['artifact_type'].value_counts()\n",
    "axes[0, 0].bar(type_counts.index, type_counts.values, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Artifact Type')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Artifact Type Distribution')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Period distribution\n",
    "period_counts = df_results['period'].value_counts().sort_index()\n",
    "axes[0, 1].bar(range(len(period_counts)), period_counts.values, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(period_counts)))\n",
    "axes[0, 1].set_xticklabels(period_counts.index, rotation=45)\n",
    "axes[0, 1].set_xlabel('Period')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Chronological Distribution')\n",
    "\n",
    "# 3. Material distribution\n",
    "material_counts = df_results['material'].value_counts().head(10)\n",
    "axes[1, 0].barh(material_counts.index, material_counts.values, color='seagreen')\n",
    "axes[1, 0].set_xlabel('Count')\n",
    "axes[1, 0].set_ylabel('Material')\n",
    "axes[1, 0].set_title('Material Distribution (Top 10)')\n",
    "\n",
    "# 4. Classification confidence\n",
    "axes[1, 1].hist(df_results['classification_confidence'], bins=20, color='mediumpurple', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Classification Confidence')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Classification Confidence Distribution')\n",
    "axes[1, 1].axvline(df_results['classification_confidence'].mean(), \n",
    "                   color='red', linestyle='--', label=f\"Mean: {df_results['classification_confidence'].mean():.2f}\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifact_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: artifact_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spatial Analysis: Geographic Distribution\n",
    "\n",
    "Visualize the spatial distribution of artifacts across the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. All artifacts spatial distribution\n",
    "scatter1 = axes[0].scatter(df_results['gps_lon'], df_results['gps_lat'], \n",
    "                          c=pd.Categorical(df_results['artifact_type']).codes,\n",
    "                          cmap='viridis', alpha=0.6, s=50)\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "axes[0].set_title('Spatial Distribution of Artifacts')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=plt.cm.viridis(i/5), markersize=8, label=atype)\n",
    "                  for i, atype in enumerate(df_results['artifact_type'].unique())]\n",
    "axes[0].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# 2. Density heatmap\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "x = df_results['gps_lon'].values\n",
    "y = df_results['gps_lat'].values\n",
    "\n",
    "# Calculate point density\n",
    "xy = np.vstack([x, y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "scatter2 = axes[1].scatter(x, y, c=z, s=50, cmap='hot', alpha=0.6)\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "axes[1].set_title('Artifact Density Heatmap')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spatial_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: spatial_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate spatial statistics\n",
    "print(\"\\nSpatial Statistics:\")\n",
    "print(f\"  Center of mass: {df_results['gps_lat'].mean():.6f}, {df_results['gps_lon'].mean():.6f}\")\n",
    "print(f\"  Latitude range: {df_results['gps_lat'].min():.6f} to {df_results['gps_lat'].max():.6f}\")\n",
    "print(f\"  Longitude range: {df_results['gps_lon'].min():.6f} to {df_results['gps_lon'].max():.6f}\")\n",
    "print(f\"  Spatial spread (std): Lat={df_results['gps_lat'].std():.6f}, Lon={df_results['gps_lon'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Morphometric Analysis\n",
    "\n",
    "Analyze artifact measurements and morphological indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create morphometric analysis plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Length vs Width scatter\n",
    "for artifact_type in df_results['artifact_type'].unique():\n",
    "    data = df_results[df_results['artifact_type'] == artifact_type]\n",
    "    axes[0, 0].scatter(data['length'], data['width'], label=artifact_type, alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Length (mm)')\n",
    "axes[0, 0].set_ylabel('Width (mm)')\n",
    "axes[0, 0].set_title('Length vs Width by Type')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Length/Width ratio distribution\n",
    "for artifact_type in df_results['artifact_type'].unique():\n",
    "    data = df_results[df_results['artifact_type'] == artifact_type]\n",
    "    axes[0, 1].hist(data['l_w_ratio'], alpha=0.5, label=artifact_type, bins=15)\n",
    "axes[0, 1].set_xlabel('Length/Width Ratio')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('L/W Ratio Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Thickness index\n",
    "axes[0, 2].boxplot([df_results[df_results['artifact_type'] == t]['thickness_index'].dropna() \n",
    "                    for t in df_results['artifact_type'].unique()],\n",
    "                   labels=df_results['artifact_type'].unique())\n",
    "axes[0, 2].set_xlabel('Artifact Type')\n",
    "axes[0, 2].set_ylabel('Thickness Index')\n",
    "axes[0, 2].set_title('Thickness Index by Type')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Weight distribution\n",
    "for artifact_type in df_results['artifact_type'].unique():\n",
    "    data = df_results[df_results['artifact_type'] == artifact_type]\n",
    "    axes[1, 0].hist(data['weight'], alpha=0.5, label=artifact_type, bins=15)\n",
    "axes[1, 0].set_xlabel('Weight (g)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Weight Distribution')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 5. Shape index\n",
    "axes[1, 1].boxplot([df_results[df_results['artifact_type'] == t]['shape_index'].dropna() \n",
    "                    for t in df_results['artifact_type'].unique()],\n",
    "                   labels=df_results['artifact_type'].unique())\n",
    "axes[1, 1].set_xlabel('Artifact Type')\n",
    "axes[1, 1].set_ylabel('Shape Index')\n",
    "axes[1, 1].set_title('Shape Index by Type')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. PCA of morphometric variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "morph_cols = ['length', 'width', 'thickness', 'weight', 'l_w_ratio', 'thickness_index']\n",
    "X = df_results[morph_cols].dropna()\n",
    "X_types = df_results.loc[X.index, 'artifact_type']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "for artifact_type in X_types.unique():\n",
    "    mask = X_types == artifact_type\n",
    "    axes[1, 2].scatter(X_pca[mask, 0], X_pca[mask, 1], label=artifact_type, alpha=0.6)\n",
    "axes[1, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[1, 2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[1, 2].set_title('PCA of Morphometric Variables')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('morphometric_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: morphometric_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Chronological Analysis\n",
    "\n",
    "Analyze temporal patterns in the artifact assemblage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chronological analysis plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Timeline of all artifacts\n",
    "for artifact_type in df_results['artifact_type'].unique():\n",
    "    data = df_results[df_results['artifact_type'] == artifact_type]\n",
    "    axes[0, 0].scatter(data['dating_value'], [artifact_type] * len(data), alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Date (years)')\n",
    "axes[0, 0].set_ylabel('Artifact Type')\n",
    "axes[0, 0].set_title('Chronological Distribution of Artifacts')\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.5, label='Present')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Artifact counts by period\n",
    "period_type = df_results.groupby(['period', 'artifact_type']).size().unstack(fill_value=0)\n",
    "period_type.plot(kind='bar', stacked=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Period')\n",
    "axes[0, 1].set_ylabel('Artifact Count')\n",
    "axes[0, 1].set_title('Artifact Types by Period')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. Stratigraphic sequence\n",
    "strat_period = df_results.groupby(['stratigraphic_unit', 'period']).size().unstack(fill_value=0)\n",
    "strat_period.plot(kind='bar', stacked=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Stratigraphic Unit')\n",
    "axes[1, 0].set_ylabel('Artifact Count')\n",
    "axes[1, 0].set_title('Periods by Stratigraphic Unit')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(title='Period')\n",
    "\n",
    "# 4. Dating value histogram\n",
    "axes[1, 1].hist(df_results['dating_value'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Date (years)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Temporal Distribution')\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', alpha=0.5, label='Present')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chronological_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: chronological_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print chronological summary\n",
    "print(\"\\nChronological Summary:\")\n",
    "print(f\"  Date range: {df_results['dating_value'].min()} to {df_results['dating_value'].max()}\")\n",
    "print(f\"  Time span: {df_results['dating_value'].max() - df_results['dating_value'].min()} years\")\n",
    "print(f\"\\nPeriod counts:\")\n",
    "for period, count in df_results['period'].value_counts().items():\n",
    "    print(f\"  {period}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics\n",
    "\n",
    "Generate comprehensive summary statistics for the artifact assemblage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARCHAEOLOGICAL SITE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSite ID: {SITE_ID}\")\n",
    "print(f\"Total Artifacts: {len(df_results)}\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TYPOLOGICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nArtifact Types:\")\n",
    "for atype, count in df_results['artifact_type'].value_counts().items():\n",
    "    pct = count / len(df_results) * 100\n",
    "    print(f\"  {atype:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nMaterials (top 5):\")\n",
    "for material, count in df_results['material'].value_counts().head(5).items():\n",
    "    pct = count / len(df_results) * 100\n",
    "    print(f\"  {material:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHRONOLOGICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDate Range: {df_results['dating_value'].min()} to {df_results['dating_value'].max()}\")\n",
    "print(f\"Time Span: {df_results['dating_value'].max() - df_results['dating_value'].min():,} years\")\n",
    "print(\"\\nPeriods:\")\n",
    "for period, count in df_results['period'].value_counts().sort_index().items():\n",
    "    pct = count / len(df_results) * 100\n",
    "    print(f\"  {period:15s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MORPHOMETRIC SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "morph_summary = df_results[['length', 'width', 'thickness', 'weight']].describe()\n",
    "print(\"\\n\", morph_summary.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SPATIAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCenter of Mass:\")\n",
    "print(f\"  Latitude:  {df_results['gps_lat'].mean():.6f} (std: {df_results['gps_lat'].std():.6f})\")\n",
    "print(f\"  Longitude: {df_results['gps_lon'].mean():.6f} (std: {df_results['gps_lon'].std():.6f})\")\n",
    "print(f\"\\nBounding Box:\")\n",
    "print(f\"  North: {df_results['gps_lat'].max():.6f}\")\n",
    "print(f\"  South: {df_results['gps_lat'].min():.6f}\")\n",
    "print(f\"  East:  {df_results['gps_lon'].max():.6f}\")\n",
    "print(f\"  West:  {df_results['gps_lon'].min():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASSIFICATION QUALITY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMean Confidence: {df_results['classification_confidence'].mean():.3f}\")\n",
    "print(f\"Std Confidence:  {df_results['classification_confidence'].std():.3f}\")\n",
    "print(f\"Min Confidence:  {df_results['classification_confidence'].min():.3f}\")\n",
    "print(f\"Max Confidence:  {df_results['classification_confidence'].max():.3f}\")\n",
    "\n",
    "high_conf = (df_results['classification_confidence'] > 0.8).sum()\n",
    "print(f\"\\nHigh confidence (>0.8): {high_conf} ({high_conf/len(df_results)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results\n",
    "\n",
    "Export analysis results for further use or publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to various formats\n",
    "\n",
    "# 1. Export full dataset to CSV\n",
    "output_csv = f\"{SITE_ID}_analysis_results.csv\"\n",
    "df_results.to_csv(output_csv, index=False)\n",
    "print(f\"✓ Exported to CSV: {output_csv}\")\n",
    "\n",
    "# 2. Export summary to JSON\n",
    "summary_dict = {\n",
    "    'site_id': SITE_ID,\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_artifacts': len(df_results),\n",
    "    'type_distribution': df_results['artifact_type'].value_counts().to_dict(),\n",
    "    'period_distribution': df_results['period'].value_counts().to_dict(),\n",
    "    'date_range': {\n",
    "        'min': int(df_results['dating_value'].min()),\n",
    "        'max': int(df_results['dating_value'].max())\n",
    "    },\n",
    "    'morphometric_summary': df_results[['length', 'width', 'thickness', 'weight']].describe().to_dict(),\n",
    "    'spatial_center': {\n",
    "        'latitude': float(df_results['gps_lat'].mean()),\n",
    "        'longitude': float(df_results['gps_lon'].mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "output_json = f\"{SITE_ID}_analysis_summary.json\"\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(summary_dict, f, indent=2)\n",
    "print(f\"✓ Exported to JSON: {output_json}\")\n",
    "\n",
    "# 3. Export by type to separate CSVs\n",
    "for artifact_type in df_results['artifact_type'].unique():\n",
    "    type_df = df_results[df_results['artifact_type'] == artifact_type]\n",
    "    output_file = f\"{SITE_ID}_{artifact_type}_artifacts.csv\"\n",
    "    type_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Exported {artifact_type}: {output_file}\")\n",
    "\n",
    "print(\"\\n✓ All exports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "Congratulations! You've completed a full archaeological data analysis pipeline using AWS.\n",
    "\n",
    "### What You've Accomplished\n",
    "- Generated and uploaded artifact data to S3\n",
    "- Processed data with Lambda serverless functions\n",
    "- Stored catalog in DynamoDB NoSQL database\n",
    "- Performed comprehensive archaeological analysis\n",
    "- Created publication-quality visualizations\n",
    "\n",
    "### Recommended Next Steps\n",
    "\n",
    "1. **Extend the analysis**:\n",
    "   - Add more sites for comparative analysis\n",
    "   - Include artifact images and computer vision\n",
    "   - Integrate with GIS software (QGIS, ArcGIS)\n",
    "   - Add radiocarbon dating calibration\n",
    "\n",
    "2. **Scale up**:\n",
    "   - Process 10,000+ artifacts from real excavations\n",
    "   - Use Step Functions for complex workflows\n",
    "   - Add real-time processing during excavation\n",
    "   - Create team collaboration features\n",
    "\n",
    "3. **Move to Tier 3**:\n",
    "   - Production infrastructure with CloudFormation\n",
    "   - High availability and disaster recovery\n",
    "   - API for external data access\n",
    "   - Advanced monitoring and alerting\n",
    "\n",
    "4. **Clean up resources**:\n",
    "   - Follow `cleanup_guide.md` to delete AWS resources\n",
    "   - Download all results before cleanup\n",
    "   - Verify costs have stopped\n",
    "\n",
    "### Resources\n",
    "- [Open Context](https://opencontext.org/) - Open archaeological data\n",
    "- [tDAR](https://www.tdar.org/) - The Digital Archaeological Record\n",
    "- [Archaeological Data Service](https://archaeologydataservice.ac.uk/)\n",
    "- AWS Documentation: [S3](https://docs.aws.amazon.com/s3/), [Lambda](https://docs.aws.amazon.com/lambda/), [DynamoDB](https://docs.aws.amazon.com/dynamodb/)\n",
    "\n",
    "### Questions or Issues?\n",
    "- GitHub Issues: https://github.com/research-jumpstart/research-jumpstart/issues\n",
    "- Tag: `archaeology`, `tier-2`, `aws`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
