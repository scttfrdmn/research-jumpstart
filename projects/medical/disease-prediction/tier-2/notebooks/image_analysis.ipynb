{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Processing Analysis\n",
    "\n",
    "## Tier 2: AWS Cloud Processing Pipeline\n",
    "\n",
    "This notebook analyzes the results of the medical image preprocessing pipeline using AWS services:\n",
    "- **S3**: Stores raw and processed images\n",
    "- **Lambda**: Preprocesses images (resize, normalize)\n",
    "- **DynamoDB**: Stores metadata about processing\n",
    "\n",
    "**Objectives:**\n",
    "1. Query prediction metadata from DynamoDB\n",
    "2. Analyze processing performance and statistics\n",
    "3. Visualize image statistics and distributions\n",
    "4. Understand AWS costs\n",
    "5. Download and inspect sample processed images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure AWS clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS clients\n",
    "region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=region)\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "print(f\"AWS Region: {region}\")\n",
    "print(f\"DynamoDB Region: {region}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Query Prediction Metadata from DynamoDB\n",
    "\n",
    "Retrieve all prediction records from DynamoDB table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "table_name = os.getenv(\"DYNAMODB_TABLE_NAME\", \"medical-predictions\")\n",
    "s3_bucket = os.getenv(\"S3_BUCKET_NAME\", \"\")\n",
    "\n",
    "print(f\"DynamoDB Table: {table_name}\")\n",
    "print(f\"S3 Bucket: {s3_bucket}\")\n",
    "\n",
    "# Connect to DynamoDB table\n",
    "table = dynamodb.Table(table_name)\n",
    "\n",
    "# Query all items\n",
    "try:\n",
    "    response = table.scan(Limit=200)\n",
    "    items = response.get(\"Items\", [])\n",
    "    print(f\"\\nRetrieved {len(items)} records from DynamoDB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying DynamoDB: {e}\")\n",
    "    items = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and organize results\n",
    "results = []\n",
    "\n",
    "for item in items:\n",
    "    record = {\"image_id\": item.get(\"image_id\"), \"timestamp\": item.get(\"timestamp\")}\n",
    "\n",
    "    # Parse metadata JSON string\n",
    "    metadata_str = item.get(\"metadata\", \"{}\")\n",
    "    if isinstance(metadata_str, str):\n",
    "        try:\n",
    "            metadata = json.loads(metadata_str)\n",
    "        except:\n",
    "            metadata = {}\n",
    "    else:\n",
    "        metadata = metadata_str\n",
    "\n",
    "    record.update(metadata)\n",
    "    results.append(record)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nResults DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few records:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing Performance Analysis\n",
    "\n",
    "Analyze Lambda function execution performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract processing time metrics\n",
    "if \"processing_time_ms\" in df.columns:\n",
    "    processing_times = df[\"processing_time_ms\"].dropna()\n",
    "\n",
    "    print(\"Processing Time Statistics (milliseconds):\")\n",
    "    print(f\"  Count:  {len(processing_times)}\")\n",
    "    print(f\"  Min:    {processing_times.min():.2f}\")\n",
    "    print(f\"  Max:    {processing_times.max():.2f}\")\n",
    "    print(f\"  Mean:   {processing_times.mean():.2f}\")\n",
    "    print(f\"  Median: {processing_times.median():.2f}\")\n",
    "    print(f\"  Std:    {processing_times.std():.2f}\")\n",
    "else:\n",
    "    print(\"No processing_time_ms data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize processing times\n",
    "if \"processing_time_ms\" in df.columns and len(df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "    # Histogram\n",
    "    axes[0].hist(df[\"processing_time_ms\"].dropna(), bins=20, edgecolor=\"black\", alpha=0.7)\n",
    "    axes[0].set_xlabel(\"Processing Time (ms)\")\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "    axes[0].set_title(\"Distribution of Image Processing Times\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Timeline\n",
    "    axes[1].plot(\n",
    "        range(len(df)), df[\"processing_time_ms\"].dropna(), marker=\"o\", linestyle=\"-\", alpha=0.7\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Image Index\")\n",
    "    axes[1].set_ylabel(\"Processing Time (ms)\")\n",
    "    axes[1].set_title(\"Processing Time Timeline\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Intensity Statistics\n",
    "\n",
    "Analyze pixel intensity distributions in processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract intensity statistics\n",
    "intensity_stats = {}\n",
    "\n",
    "for col in [\"min_value\", \"max_value\", \"mean_value\", \"std_value\"]:\n",
    "    if col in df.columns:\n",
    "        data = df[col].dropna()\n",
    "        intensity_stats[col] = {\n",
    "            \"count\": len(data),\n",
    "            \"mean\": data.mean(),\n",
    "            \"min\": data.min(),\n",
    "            \"max\": data.max(),\n",
    "            \"std\": data.std(),\n",
    "        }\n",
    "\n",
    "print(\"Image Intensity Statistics:\")\n",
    "print(\"(Normalized to [0, 1] range)\\n\")\n",
    "\n",
    "for metric, stats in intensity_stats.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  Count:  {stats['count']}\")\n",
    "    print(f\"  Mean:   {stats['mean']:.4f}\")\n",
    "    print(f\"  Min:    {stats['min']:.4f}\")\n",
    "    print(f\"  Max:    {stats['max']:.4f}\")\n",
    "    print(f\"  Std:    {stats['std']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intensity distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "intensity_cols = [\"min_value\", \"max_value\", \"mean_value\", \"std_value\"]\n",
    "titles = [\"Min Intensity\", \"Max Intensity\", \"Mean Intensity\", \"Std Intensity\"]\n",
    "\n",
    "for idx, (col, title) in enumerate(zip(intensity_cols, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    if col in df.columns:\n",
    "        data = df[col].dropna()\n",
    "        ax.hist(data, bins=20, edgecolor=\"black\", alpha=0.7, color=\"steelblue\")\n",
    "        ax.set_xlabel(title)\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"Distribution of {title}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add statistics text\n",
    "        stats_text = f\"Mean: {data.mean():.4f}\\nStd: {data.std():.4f}\"\n",
    "        ax.text(\n",
    "            0.98,\n",
    "            0.97,\n",
    "            stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment=\"top\",\n",
    "            horizontalalignment=\"right\",\n",
    "            bbox={\"boxstyle\": \"round\", \"facecolor\": \"wheat\", \"alpha\": 0.8},\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Size Analysis\n",
    "\n",
    "Analyze file sizes and data storage requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File size analysis\n",
    "if \"source_size\" in df.columns and \"output_size\" in df.columns:\n",
    "    source_sizes = df[\"source_size\"].dropna()\n",
    "    output_sizes = df[\"output_size\"].dropna()\n",
    "\n",
    "    print(\"Source Image Sizes:\")\n",
    "    print(f\"  Total files: {len(source_sizes)}\")\n",
    "    print(f\"  Total bytes: {source_sizes.sum():,.0f} ({source_sizes.sum() / 1e6:.2f} MB)\")\n",
    "    print(f\"  Mean size:   {source_sizes.mean():,.0f} bytes ({source_sizes.mean() / 1e6:.4f} MB)\")\n",
    "    print(f\"  Min size:    {source_sizes.min():,.0f} bytes\")\n",
    "    print(f\"  Max size:    {source_sizes.max():,.0f} bytes\")\n",
    "\n",
    "    print(\"\\nProcessed Image Sizes:\")\n",
    "    print(f\"  Total bytes: {output_sizes.sum():,.0f} ({output_sizes.sum() / 1e6:.2f} MB)\")\n",
    "    print(f\"  Mean size:   {output_sizes.mean():,.0f} bytes ({output_sizes.mean() / 1e6:.4f} MB)\")\n",
    "    print(f\"  Min size:    {output_sizes.min():,.0f} bytes\")\n",
    "    print(f\"  Max size:    {output_sizes.max():,.0f} bytes\")\n",
    "\n",
    "    # Compression ratio\n",
    "    if len(source_sizes) == len(output_sizes):\n",
    "        compression_ratio = (1 - output_sizes.sum() / source_sizes.sum()) * 100\n",
    "        print(f\"\\nCompression ratio: {compression_ratio:.1f}%\")\n",
    "else:\n",
    "    print(\"File size data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize file sizes\n",
    "if \"source_size\" in df.columns and \"output_size\" in df.columns and len(df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # File size comparison\n",
    "    size_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Source\": df[\"source_size\"].dropna() / 1e6,  # Convert to MB\n",
    "            \"Processed\": df[\"output_size\"].dropna() / 1e6,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    size_data.plot(kind=\"box\", ax=axes[0])\n",
    "    axes[0].set_ylabel(\"File Size (MB)\")\n",
    "    axes[0].set_title(\"File Size Comparison: Source vs Processed\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Storage requirements\n",
    "    if \"source_size\" in df.columns:\n",
    "        total_source = df[\"source_size\"].dropna().sum() / 1e9  # Convert to GB\n",
    "    else:\n",
    "        total_source = 0\n",
    "\n",
    "    if \"output_size\" in df.columns:\n",
    "        total_output = df[\"output_size\"].dropna().sum() / 1e9\n",
    "    else:\n",
    "        total_output = 0\n",
    "\n",
    "    storage_data = [total_source, total_output]\n",
    "    storage_labels = [\"Source Images\", \"Processed Images\"]\n",
    "\n",
    "    axes[1].bar(\n",
    "        storage_labels, storage_data, color=[\"steelblue\", \"coral\"], alpha=0.7, edgecolor=\"black\"\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Total Storage (GB)\")\n",
    "    axes[1].set_title(\"Total Storage Requirements\")\n",
    "    axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Analysis\n",
    "\n",
    "Estimate AWS costs based on actual usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculation\n",
    "print(\"AWS Cost Estimate (us-east-1 pricing)\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S3 Costs\n",
    "total_images = len(df)\n",
    "if \"source_size\" in df.columns:\n",
    "    total_source_gb = df[\"source_size\"].dropna().sum() / 1e9\n",
    "else:\n",
    "    total_source_gb = 0\n",
    "\n",
    "if \"output_size\" in df.columns:\n",
    "    total_output_gb = df[\"output_size\"].dropna().sum() / 1e9\n",
    "else:\n",
    "    total_output_gb = 0\n",
    "\n",
    "s3_storage_cost = (total_source_gb + total_output_gb) * 0.023 * 7 / 30  # 7 days storage\n",
    "s3_upload_cost = total_images * 0.005 / 1000  # $0.005 per 1000 PUT\n",
    "s3_download_cost = (total_images * 0.4) * 0.0004 / 1000  # Estimate 40% retrieval\n",
    "\n",
    "print(\"S3 Storage (7 days):\")\n",
    "print(f\"  Raw images: {total_source_gb:.3f} GB\")\n",
    "print(f\"  Processed:  {total_output_gb:.3f} GB\")\n",
    "print(f\"  Cost:       ${s3_storage_cost:.4f}\")\n",
    "print(\"\\nS3 Requests:\")\n",
    "print(f\"  Uploads:    {total_images} PUT requests = ${s3_upload_cost:.4f}\")\n",
    "print(f\"  Downloads:  {total_images * 0.4:.0f} GET requests = ${s3_download_cost:.4f}\")\n",
    "\n",
    "# Lambda Costs\n",
    "if \"processing_time_ms\" in df.columns:\n",
    "    total_lambda_ms = df[\"processing_time_ms\"].dropna().sum()\n",
    "    total_lambda_gb_seconds = (total_lambda_ms / 1000) * 0.25 / 1024  # 256 MB = 0.25 GB\n",
    "else:\n",
    "    total_lambda_gb_seconds = total_images * 0.030  # Estimate 30ms per image\n",
    "\n",
    "lambda_compute_cost = total_lambda_gb_seconds * 0.0000166667  # $0.0000166667 per GB-second\n",
    "lambda_request_cost = total_images * 0.0000002  # $0.0000002 per request\n",
    "\n",
    "print(\"\\nLambda:\")\n",
    "print(f\"  Invocations: {total_images} @ $0.0000002 = ${lambda_request_cost:.4f}\")\n",
    "print(f\"  Compute:     {total_lambda_gb_seconds:.2f} GB-seconds = ${lambda_compute_cost:.4f}\")\n",
    "\n",
    "# DynamoDB Costs\n",
    "dynamodb_write_units = total_images  # 1 write per image\n",
    "dynamodb_cost = dynamodb_write_units * 1.25 / 1e6  # $1.25 per 1M writes (on-demand)\n",
    "\n",
    "print(\"\\nDynamoDB (on-demand):\")\n",
    "print(f\"  Writes:      {dynamodb_write_units} items = ${dynamodb_cost:.4f}\")\n",
    "\n",
    "# Total\n",
    "total_cost = (\n",
    "    s3_storage_cost\n",
    "    + s3_upload_cost\n",
    "    + s3_download_cost\n",
    "    + lambda_compute_cost\n",
    "    + lambda_request_cost\n",
    "    + dynamodb_cost\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"TOTAL ESTIMATED COST: ${total_cost:.4f}\")\n",
    "print(f\"Cost per image:       ${total_cost / max(total_images, 1):.4f}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download and Inspect Sample Images\n",
    "\n",
    "Retrieve sample processed images from S3 for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample processed images\n",
    "sample_dir = Path(\"./sample_images\")\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "downloaded = 0\n",
    "max_samples = 3\n",
    "\n",
    "if s3_bucket:\n",
    "    for idx, row in df.head(max_samples).iterrows():\n",
    "        output_key = row.get(\"output_key\")\n",
    "\n",
    "        if output_key:\n",
    "            local_file = sample_dir / Path(output_key).name\n",
    "\n",
    "            try:\n",
    "                print(f\"Downloading: {output_key}\")\n",
    "                s3_client.download_file(s3_bucket, output_key, str(local_file))\n",
    "                downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "\n",
    "    print(f\"\\nDownloaded {downloaded} sample images\")\n",
    "else:\n",
    "    print(\"S3 bucket not configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "image_files = list(Path(sample_dir).glob(\"*.png\"))\n",
    "\n",
    "if image_files:\n",
    "    n_images = min(len(image_files), 3)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 5))\n",
    "\n",
    "    if n_images == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, img_file in zip(axes, image_files[:n_images]):\n",
    "        img = PILImage.open(img_file)\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(img_file.name)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sample images available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations\n",
    "\n",
    "Key findings and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MEDICAL IMAGE PROCESSING - TIER 2 PROJECT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nProject Completion Date: {datetime.utcnow().isoformat()}\")\n",
    "\n",
    "print(\"\\nKEY METRICS:\")\n",
    "print(f\"  Total images processed: {len(df)}\")\n",
    "\n",
    "if \"processing_time_ms\" in df.columns and len(df) > 0:\n",
    "    print(f\"  Average processing time: {df['processing_time_ms'].mean():.2f} ms\")\n",
    "\n",
    "if \"source_size\" in df.columns and len(df) > 0:\n",
    "    print(f\"  Total data processed: {df['source_size'].sum() / 1e6:.2f} MB\")\n",
    "\n",
    "print(f\"  Estimated cost: ${total_cost:.2f}\")\n",
    "\n",
    "print(\"\\nWHAT YOU LEARNED:\")\n",
    "print(\"  ✓ How to use AWS S3 for cloud storage\")\n",
    "print(\"  ✓ How to deploy and invoke AWS Lambda functions\")\n",
    "print(\"  ✓ How to store metadata in NoSQL databases (DynamoDB)\")\n",
    "print(\"  ✓ How to query cloud data with boto3\")\n",
    "print(\"  ✓ How AWS pricing works for serverless architectures\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  1. Clean up AWS resources using cleanup_guide.md\")\n",
    "print(\"  2. Review Lambda logs in CloudWatch\")\n",
    "print(\"  3. Check AWS Cost Explorer for actual costs\")\n",
    "print(\"  4. Consider Tier 3 for production infrastructure\")\n",
    "print(\"  5. Explore advanced features:\")\n",
    "print(\"     - Model inference in Lambda\")\n",
    "print(\"     - S3 event notifications for real-time processing\")\n",
    "print(\"     - CloudWatch monitoring and alerts\")\n",
    "\n",
    "print(\"\\nRESOURCES:\")\n",
    "print(\"  - setup_guide.md: AWS setup instructions\")\n",
    "print(\"  - cleanup_guide.md: How to delete resources\")\n",
    "print(\"  - scripts/: Python scripts for data pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
