{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astronomical Image Processing - Tier 2 Analysis\n",
    "\n",
    "This notebook demonstrates the complete workflow for astronomical source detection using AWS services.\n",
    "\n",
    "**Workflow:**\n",
    "1. Upload FITS images to S3\n",
    "2. Invoke Lambda for source detection\n",
    "3. Query results with Athena (SQL)\n",
    "4. Visualize and analyze findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# Set up plotting style\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "env_file = Path.home() / \".astronomy_env\"\n",
    "if env_file.exists():\n",
    "    with open(env_file) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"export \"):\n",
    "                key_value = line.replace(\"export \", \"\").strip()\n",
    "                if \"=\" in key_value:\n",
    "                    key, value = key_value.split(\"=\", 1)\n",
    "                    os.environ[key] = value\n",
    "\n",
    "# Configuration\n",
    "BUCKET_RAW = os.environ.get(\"BUCKET_RAW\", \"\")\n",
    "BUCKET_CATALOG = os.environ.get(\"BUCKET_CATALOG\", \"\")\n",
    "LAMBDA_FUNCTION = os.environ.get(\"LAMBDA_FUNCTION\", \"astronomy-source-detection\")\n",
    "ATHENA_WORKGROUP = os.environ.get(\"ATHENA_WORKGROUP\", \"astronomy-workgroup\")\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Raw Bucket: {BUCKET_RAW}\")\n",
    "print(f\"  Catalog Bucket: {BUCKET_CATALOG}\")\n",
    "print(f\"  Lambda Function: {LAMBDA_FUNCTION}\")\n",
    "print(f\"  AWS Region: {AWS_REGION}\")\n",
    "\n",
    "if not BUCKET_RAW or not BUCKET_CATALOG:\n",
    "    print(\"\\n⚠ Warning: Some environment variables not set!\")\n",
    "    print(\"Run setup_guide.md to configure AWS resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "try:\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "    lambda_client = boto3.client(\"lambda\", region_name=AWS_REGION)\n",
    "    athena = boto3.client(\"athena\", region_name=AWS_REGION)\n",
    "\n",
    "    # Test credentials\n",
    "    sts = boto3.client(\"sts\")\n",
    "    identity = sts.get_caller_identity()\n",
    "\n",
    "    print(\"✓ AWS Authentication Successful\")\n",
    "    print(f\"  Account: {identity['Account']}\")\n",
    "    print(f\"  User: {identity['Arn']}\")\n",
    "except NoCredentialsError:\n",
    "    print(\"✗ AWS credentials not configured\")\n",
    "    print(\"Run: aws configure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Check Uploaded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List uploaded FITS images\n",
    "def list_s3_objects(bucket, prefix):\n",
    "    \"\"\"List objects in S3 bucket.\"\"\"\n",
    "    objects = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    try:\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            if \"Contents\" in page:\n",
    "                for obj in page[\"Contents\"]:\n",
    "                    objects.append(\n",
    "                        {\n",
    "                            \"Key\": obj[\"Key\"],\n",
    "                            \"Size\": obj[\"Size\"],\n",
    "                            \"LastModified\": obj[\"LastModified\"],\n",
    "                        }\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "# Check images\n",
    "print(f\"Images in s3://{BUCKET_RAW}/images/:\\n\")\n",
    "images = list_s3_objects(BUCKET_RAW, \"images/\")\n",
    "\n",
    "if images:\n",
    "    df_images = pd.DataFrame(images)\n",
    "    df_images[\"Size_MB\"] = df_images[\"Size\"] / 1024 / 1024\n",
    "    print(df_images[[\"Key\", \"Size_MB\", \"LastModified\"]].to_string())\n",
    "    print(f\"\\nTotal: {len(images)} images, {df_images['Size_MB'].sum():.1f} MB\")\n",
    "else:\n",
    "    print(\"No images found. Run: python scripts/upload_to_s3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Source Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List source detection results\n",
    "print(f\"Source catalogs in s3://{BUCKET_CATALOG}/sources/:\\n\")\n",
    "catalogs = list_s3_objects(BUCKET_CATALOG, \"sources/\")\n",
    "\n",
    "if catalogs:\n",
    "    df_catalogs = pd.DataFrame(catalogs)\n",
    "    df_catalogs[\"Size_KB\"] = df_catalogs[\"Size\"] / 1024\n",
    "    print(df_catalogs[[\"Key\", \"Size_KB\", \"LastModified\"]].to_string())\n",
    "    print(f\"\\nTotal: {len(catalogs)} catalogs\")\n",
    "else:\n",
    "    print(\"No catalogs found. Run: python scripts/invoke_lambda.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Athena Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_athena_query(query_string, database=\"astronomy\"):\n",
    "    \"\"\"Execute Athena query and return results as DataFrame.\"\"\"\n",
    "\n",
    "    # Start query\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query_string,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": f\"s3://{BUCKET_CATALOG}/athena-results/\"},\n",
    "        WorkGroup=ATHENA_WORKGROUP,\n",
    "    )\n",
    "\n",
    "    query_id = response[\"QueryExecutionId\"]\n",
    "\n",
    "    # Wait for completion\n",
    "    for _i in range(60):\n",
    "        result = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "        status = result[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "\n",
    "        if status == \"SUCCEEDED\":\n",
    "            break\n",
    "        elif status == \"FAILED\":\n",
    "            print(f\"Query failed: {result['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"  Status: {status}...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Get results\n",
    "    results = athena.get_query_results(QueryExecutionId=query_id)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    rows = results[\"ResultSet\"][\"Rows\"]\n",
    "    headers = [col.get(\"VarCharValue\", \"\") for col in rows[0][\"Data\"]]\n",
    "    data = []\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        values = [col.get(\"VarCharValue\", \"\") for col in row[\"Data\"]]\n",
    "        data.append(values)\n",
    "\n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "\n",
    "print(\"Athena query function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Total sources\n",
    "print(\"\\n=== Total Sources ===\")\n",
    "try:\n",
    "    query = \"SELECT COUNT(*) as total_sources FROM astronomy.sources;\"\n",
    "    df = execute_athena_query(query)\n",
    "    print(df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Query error: {e}\")\n",
    "    print(\"Note: Athena table may not exist yet. Run: python scripts/invoke_lambda.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Source statistics\n",
    "print(\"\\n=== Source Statistics ===\")\n",
    "try:\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) as total_sources,\n",
    "      ROUND(AVG(CAST(flux AS DOUBLE)), 2) as mean_flux,\n",
    "      ROUND(MAX(CAST(flux AS DOUBLE)), 2) as max_flux,\n",
    "      ROUND(AVG(CAST(snr AS DOUBLE)), 2) as mean_snr,\n",
    "      ROUND(MAX(CAST(snr AS DOUBLE)), 2) as max_snr\n",
    "    FROM astronomy.sources;\n",
    "    \"\"\"\n",
    "    df = execute_athena_query(query)\n",
    "    print(df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Query error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Bright sources\n",
    "print(\"\\n=== Brightest 10 Sources ===\")\n",
    "try:\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      ROUND(CAST(ra AS DOUBLE), 4) as ra,\n",
    "      ROUND(CAST(dec AS DOUBLE), 4) as dec,\n",
    "      ROUND(CAST(flux AS DOUBLE), 1) as flux,\n",
    "      ROUND(CAST(snr AS DOUBLE), 1) as snr\n",
    "    FROM astronomy.sources\n",
    "    WHERE CAST(snr AS DOUBLE) > 20\n",
    "    ORDER BY CAST(flux AS DOUBLE) DESC\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    df = execute_athena_query(query)\n",
    "    print(df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Query error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all source data for visualization\n",
    "print(\"Loading source data...\")\n",
    "try:\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      image_id,\n",
    "      source_id,\n",
    "      CAST(ra AS DOUBLE) as ra,\n",
    "      CAST(dec AS DOUBLE) as dec,\n",
    "      CAST(x AS DOUBLE) as x,\n",
    "      CAST(y AS DOUBLE) as y,\n",
    "      CAST(flux AS DOUBLE) as flux,\n",
    "      CAST(peak AS DOUBLE) as peak,\n",
    "      CAST(fwhm AS DOUBLE) as fwhm,\n",
    "      CAST(snr AS DOUBLE) as snr\n",
    "    FROM astronomy.sources\n",
    "    ORDER BY flux DESC\n",
    "    LIMIT 1000;\n",
    "    \"\"\"\n",
    "    df_sources = execute_athena_query(query)\n",
    "    print(f\"✓ Loaded {len(df_sources)} sources\")\n",
    "    print(df_sources.head(10).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    df_sources = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flux distribution\n",
    "if df_sources is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Histogram of flux\n",
    "    axes[0, 0].hist(df_sources[\"flux\"], bins=50, edgecolor=\"black\")\n",
    "    axes[0, 0].set_xlabel(\"Flux (μJy)\")\n",
    "    axes[0, 0].set_ylabel(\"Count\")\n",
    "    axes[0, 0].set_title(\"Flux Distribution\")\n",
    "    axes[0, 0].set_yscale(\"log\")\n",
    "\n",
    "    # SNR distribution\n",
    "    axes[0, 1].hist(df_sources[\"snr\"], bins=50, edgecolor=\"black\", color=\"orange\")\n",
    "    axes[0, 1].set_xlabel(\"Signal-to-Noise Ratio\")\n",
    "    axes[0, 1].set_ylabel(\"Count\")\n",
    "    axes[0, 1].set_title(\"SNR Distribution\")\n",
    "\n",
    "    # Flux vs SNR\n",
    "    axes[1, 0].scatter(df_sources[\"flux\"], df_sources[\"snr\"], alpha=0.5, s=20)\n",
    "    axes[1, 0].set_xlabel(\"Flux (μJy)\")\n",
    "    axes[1, 0].set_ylabel(\"Signal-to-Noise Ratio\")\n",
    "    axes[1, 0].set_title(\"Flux vs SNR\")\n",
    "    axes[1, 0].set_xscale(\"log\")\n",
    "\n",
    "    # FWHM distribution\n",
    "    axes[1, 1].hist(df_sources[\"fwhm\"], bins=40, edgecolor=\"black\", color=\"green\")\n",
    "    axes[1, 1].set_xlabel(\"FWHM (pixels)\")\n",
    "    axes[1, 1].set_ylabel(\"Count\")\n",
    "    axes[1, 1].set_title(\"FWHM Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"source_distributions.png\", dpi=100, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: source_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sky map - spatial distribution\n",
    "if df_sources is not None and len(df_sources) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        df_sources[\"ra\"],\n",
    "        df_sources[\"dec\"],\n",
    "        c=np.log10(df_sources[\"flux\"]),\n",
    "        s=df_sources[\"snr\"] * 2,\n",
    "        alpha=0.6,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"RA (degrees)\")\n",
    "    ax.set_ylabel(\"Dec (degrees)\")\n",
    "    ax.set_title(\"Astronomical Source Distribution\")\n",
    "\n",
    "    cbar = plt.colorbar(scatter, ax=ax, label=\"log10(Flux)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"sky_distribution.png\", dpi=100, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: sky_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "if df_sources is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal sources analyzed: {len(df_sources)}\")\n",
    "    print(\"\\nFlux Statistics:\")\n",
    "    print(f\"  Mean: {df_sources['flux'].astype(float).mean():.1f} μJy\")\n",
    "    print(f\"  Median: {df_sources['flux'].astype(float).median():.1f} μJy\")\n",
    "    print(f\"  Min: {df_sources['flux'].astype(float).min():.1f} μJy\")\n",
    "    print(f\"  Max: {df_sources['flux'].astype(float).max():.1f} μJy\")\n",
    "\n",
    "    print(\"\\nSNR Statistics:\")\n",
    "    print(f\"  Mean: {df_sources['snr'].astype(float).mean():.1f}\")\n",
    "    print(f\"  Median: {df_sources['snr'].astype(float).median():.1f}\")\n",
    "    print(f\"  Min: {df_sources['snr'].astype(float).min():.1f}\")\n",
    "    print(f\"  Max: {df_sources['snr'].astype(float).max():.1f}\")\n",
    "\n",
    "    print(\"\\nSpatial Statistics:\")\n",
    "    print(\n",
    "        f\"  RA range: {df_sources['ra'].astype(float).min():.2f} - {df_sources['ra'].astype(float).max():.2f}°\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Dec range: {df_sources['dec'].astype(float).min():.2f} - {df_sources['dec'].astype(float).max():.2f}°\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Project Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Export results: df_sources.to_csv('sources.csv')\")\n",
    "print(\"  2. When finished, run cleanup_guide.md to delete AWS resources\")\n",
    "print(\"  3. Check costs in AWS Cost Explorer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
